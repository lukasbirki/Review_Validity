"Zeitstempel","Nutzername","This is the title of the Study","These are the Authors","Please scan the text. 
Does the study fulfil the eligibility criteria?
- Empirical Study
- Field of Political Communication
- Using Text as Data Method to operationalise a construct of interest 

You may come back to this question if you have started to fill in some fields as well","If no: Why is the study out of scope?","What is the data source? ","What is the data language?","If others: Name all languages in the data? If there is more than one language assessed, please separate languages by a comma (,)","Please name the target construct as described by the authors)
Note: Please use the exact wording of the authors, such as ""affective polarization"", ""semantic complexity"".
If there is more than one construct studied, please separate constructs by a comma (,)","What Type(s) of method are initially applied? ","What is the name of the method? If there is more than one method applied, please separate methods by a comma (,)","Do you have any comments on the study design? ","If yes: What are your comments on the study design?","Are steps of validation reported? ","In what paragraphs is the step of validation / robustness checks / […] reported? 
Please copy the paragraph/description into the open field, so that the whole validation procedure is copied! ","What step of validation exercises is reported? 
Please report the brief description by the authors (e.g., in the abstract, in the introduction, or at the beginning of the method/validation section). This can also include just a term (e.g. face validity), but should not be longer than one sentence.
","What type of Validation exercise does this refers to? 
For a detailed discussion on the different categories, please refer to the Coding Manual","Is there another validation step reported in the paper?","In what paragraphs is the step of validation / robustness checks / […] reported? 
Please copy the paragraph/description into the open field!","What step of validation exercises is reported? 
(As described by the authors)","What type of Validation exercise does this refers to? 
For a detailed discussion on the different categories, please refer to the Coding Manual","Is there another validation step reported in the paper?","In what paragraphs is the step of validation / robustness checks / […] reported? 
Please copy the paragraph/description into the open field!","What step of validation exercises is reported? 
(As described by the authors)","What type of Validation exercise does this refers to? 
For a detailed discussion on the different categories, please refer to the Coding Manual","Is there another validation step reported in the paper?","In what paragraphs is the step of validation / robustness checks / […] reported? 
Please copy the paragraph/description into the open field!","What step of validation exercises is reported? 
(As described by the authors)","What type of Validation exercise does this refers to? 
For a detailed discussion on the different categories, please refer to the Coding Manual","Is there another validation step reported in the paper?","In what paragraphs is the step of validation / robustness checks / […] reported? 
Please copy the paragraph/description into the open field!","What step of validation exercises is reported? 
(As described by the authors)","What type of Validation exercise does this refers to? 
For a detailed discussion on the different categories, please refer to the Coding Manual","Is there another validation step reported in the paper?","In what paragraphs are issues of measurement validity / robustness checks / […] reported? Please copy them into the open field! ","What step of validation exercises is reported? 
(As described by the authors)","What type of Validation exercise does this refers to? 
For a detailed discussion on the different categories, please refer to the Coding Manual","Is there another validation step reported in the paper?","In what paragraphs are issues of measurement validity / robustness checks / […] reported? Please copy them into the open field! ","What step of validation exercises is reported? 
(As described by the authors)","What type of Validation exercise does this refers to? 
For a detailed discussion on the different categories, please refer to the Coding Manual","Is there another validation step reported in the paper?","In what paragraphs are issues of measurement validity / robustness checks / […] reported? Please copy them into the open field! ","What step of validation exercises is reported? 
(As described by the authors)","What type of Validation exercise does this refers to? 
For a detailed discussion on the different categories, please refer to the Coding Manual","Is there another validation step reported in the paper?","Is there a link to an Appendix/additional Materials/GitHub repository? ","
Is the data for the analysis accessible? ","Is the code for the analysis accessible?","Any Comments on Open Science Standards?",""
"2022/09/05 2:47:55 PM OEZ","lukas.birkenmaier@outlook.de","Mapping Political Communities: A Statistical Analysis of Lobbying Networks in Legislative Politics","Kim, In Song; Kunisky, Dmitriy","No","only network analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2022/09/05 3:26:00 PM OEZ","lukas.birkenmaier@outlook.de","Estimating Spatial Preferences from Votes and Text","Kim, In Song; Londregan, John; Ratkovic, Marc","Yes, Continue with Coding","","Politics: Parliamentary Records","English","","Speech Dimension","New statistical model, most close to text scaling","Sparse Factor Analysis","Yes","Combining text and votes for estimating spatial locations of MPs","Yes","Before applying the method, let’s consider the applicability of SFA in this case. First, voting is not always sincere in the US Senate, as there are always motions to recommit, etc. We note, though, that ideal point estimates from the US Congress have been used extensively in other studies and possess high face validity. To be particularly careful, if a bill is voted on several times due to different motions, we only include the final vote in our analysis. Second, we consider the sincerity when speaking. Previous work has shown, albeit in the US House, that floor speeches are expressive rather than deliberative (Hill and Hurley, 2002; Maltzman and Sigelman, 1996). Many floor speeches aren’t even read verbally, but simply entered into the record, also suggesting that floor speeches are vehicles of expression rather than persuasion. For that reason, we feel more comfortable applying the method to floor speeches rather than, say, conference committee meetings","Considerations Applicability Data and Method","Content Validation","Yes","The left figure contains results from the 108th Senate, a Republican-led session during President George W. Bush’s tenure. The right figure contains results from the 112th Senate, a Democratic-led session during President Barack Obama’s time as President. We find a consistent pattern: for the majority party, the most extreme terms relate to parliamentary control words (consent committee, author meet, meet session). For the minority party, the first dimension identifies ideologically relevant terms. For the Democrats during the 108th Senate, these terms included administr, as the Democrats soured on the current Presidential administration, and health, a centerpiece of the Democratic policy agenda. In the 112th Senate, with the Democrats in the majority, parliamentary control terms switched their ideological polarity, aligning with the Democrats ( meet session, consent committee, author meet). The Republican end of this first dimension reflects that party’s programmatic fiscal concerns (budget, stimulus, debt, trillion).","Inspection of most relevant words","Unsure","Yes","Next, we look at the preferred outcomes of legislators from the 112th. Points in Figure 4 are shaded in proportion to their first dimensional DW-NOMINATE score, showing the agreement between SFA and DW-NOMINATE on the first dimension (p ρ « 0.93). The left plot labels party leaders, whips, and top chairmen, showing the close relationship between locations on the second dimension and leadership. The first dimension captures the political battle lines, reflecting legislators left vs right policy differences, while the second, vertical, dimension reflects differences in the terms selected by leaders versus the rank and file members.","Comparison with DW-Nominate Scores for vote choice (https://en.wikipedia.org/wiki/NOMINATE_(scaling_method))","External: Criterion data / Predictive validation","Yes","Scaling results informed by only words (α “ 1). We also apply SFA using only information from words. This is not our preferred model, as it ignores vote data, yet SFA still uncovers structure in the text data. [...] Figure 6 contains the top ten words at each of the first six dimensions of the 112th Senate. We note that the positive and negative level distinction along the y-axis is wholly arbitrary, as we only identify term levels up to a sign. Looking at the first column, we find that the first dimension starts with a set of non-controversial terms. These include parliamentary procedural terms (as opposed to parliamentary control terms) such as today wish, madam rise, and colleague support. Also on the non-controversial side are martial terms with universally positive affect during this Congress such as army, air forc, and deploy. On the other side are words that will be used in to differentiate issues in other dimensions, such as tax, vote, and peopl. The other dimensions have at their extremes words connoting some underlying dimension of policy. For example, the second dimension ranges from judiciary and women’s issues at one end to fiscal concerns at the other; the fourth goes from a broad set of social welfare concerns to the consideration of judicial nominees. These lower dimensions adapt to the issues of the day. Tobacco, for example is present in the 105th Senate; Iraq comes and goes as an issue, and health care goes from dealing with seniors and Medicare in the 107th Senate to dealing with students and families in the 112th. Even without including votes in","Visual inspection of word wheights in a scaling model","Content Validation","No","","","","","","","","","","","","","","","","","Yes","Yes","Yes","","Content Validation"
"2022/09/05 3:30:47 PM OEZ","lukas.birkenmaier@outlook.de","Campaigning in the fourth age of political communication. A multi-method study on the use of Facebook by German and Austrian parties in the 2013 national election campaigns","Magin, Melanie; Podschuweit, Nicole; Haßler, Jörg; Russmann, Uta","No","only descriptive statistics; texts are hand-coded and not analysed computationally","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2022/09/05 4:00:16 PM OEZ","lukas.birkenmaier@outlook.de","Policy Diffusion: The Issue‐Definition Stage","Gilardi, Fabrizio; Shipan, Charles R.; Wüest, Bruno","Yes, Continue with Coding","","Newspaper","English","","Policy Framing","Unsupervised: Topic Modeling","Structural Topic Model","No","","Yes","Figure 1 shows how topic prevalence is distributed over time across all states; a detailed validation is discussed in SI Appendix C.3.","Visual Inspection of Topic prevalende (supplementary materials can not be found)","Content Validation","No","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Yes","Yes","Yes","they claim that they provide data and code in the appendix, but this appendix can not be found on the internet",""
"2022/09/06 10:45:25 AM OEZ","lukas.birkenmaier@outlook.de","Right-Wing, Populist, Controlled by Foreign Powers? Topic Diversification and Partisanship in the Content Structures of German-Language Alternative Media","Müller, Philipp; Freudenthaler, Rainer","Yes, Continue with Coding","","Newspaper","Others","German","Topic Diversification","Unsupervised: Topic Modeling","LDA","No","","Yes","In the last step, two researchers independently validated and labeled the topics of the 70-topic solution based on an in-depth reading of the documents, using both lists of the most common words in each topic (at k ¼ .6, see Sievert and Shirley 2014, 67) and full articles with the highest k under each topic and coherence indicator to guide their decisions; they discussed their independent assessments to arrive at the final set of topics. During this step, boilerplate topics (which occur in many articles and are not internally coherent, see DiMaggio, Nag, and Blei 2013, 568; Maier et al. 2018, 108) were removed and similar topics were grouped together, resulting in 14 groups of topics encompassing the 66 remaining topics, which are summarized in Table 1","Human Evaluation of Topics by two independent researchers","Content Validation","No","","","","","","","","","","","","","","","","","","","","","","","","","","","","","No","","","",""
"2022/09/06 11:18:08 AM OEZ","lukas.birkenmaier@outlook.de","Connecting the (Far-)Right Dots: A Topic Modeling and Hyperlink Analysis of (Far-)Right Media Coverage during the US Elections 2016","Kaiser, Jonas; Rauchfleisch, Adrian; Bourassa, Nikki","Yes, Continue with Coding","","Newspaper","English","","convergence of topics","Unsupervised: Topic Modeling","structural topic model analysis (STA)","No","","Yes","We randomly selected 100 articles and automatically assigned the topic with a probability above 0.49 to the article. Two coders without specific training coded the topics. The intercoder-reliability score between the two coders and the topic model was with a Krippendorff’s alpha of 0.67 acceptable for our case. When the more generic Republican topics are merged the alpha is even over 0.7. This is surprisingly high as most documents have a mix of topics (e.g., economy and republican primaries)","Comparison with 100 hand coded articles","External: Human Annotated Scores","No","","","","","","","","","","","","","","","","","","","","","","","","","","","","","No","","","",""
"2022/09/06 11:44:32 AM OEZ","lukas.birkenmaier@outlook.de","Testing the Validity of Automatic Speech Recognition for Political Text Analysis","Proksch, Sven-Oliver; Wratil, Christopher; Wäckerle, Jens","No","The study assesses the accuracy of automatic speech transcription. However, no latent constructs are measured using CATM, so the study is out of scope for our analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2022/09/06 3:04:06 PM OEZ","lukas.birkenmaier@outlook.de","Measuring Agenda Setting in Interactive Political Communication","Rossiter, Erin L.","Yes, Continue with Coding","","Politics: Others;Presidential Debates, online discussion (mturk), In-Person Deliberations","English","","Shift in Agenda Setting","Unsupervised: Topic Modeling;Topic Segmentation","LDA, parametric Speaker Identity for Topic Segmentation (SITS)","Yes","Topic Segmentation model builds up on LDA","Yes","To assess if SITS can accurately identify where latent shifts in topic occur, I classify a speaking turn as shifting topic if the posterior probability of a shift is greater than or equal to 0.50. I then compare where shifts were inferred by SITS to where topic changes were prompted by the researchers. SITS identifies 81.40% of the locations that begin a new prompted topic segment by the researcher.7 I check for an SITS-inferred topic shift within two speaking turns after a researcher-prompted shift because often after reading the prompt, the first few speaking turns would simply answer the prompt’s question by saying “yes,” “no,” or “do you want to go first?” Instead of classifying this as a topic shift, SITS would classify a subsequent speaking turn that actually began discussing the topic at hand as a shift. This exercise demonstrates that SITS can accurately identify the speaking turns that should be attributed as shifting the agenda.8 Moreover, SITS provides a more nuanced view of how topics ebbed and flowed in these discussions than if we considered the locations of researcher-prompted topic changes as ground truth.","Comparison latent shifts with gold-standard coding","External: Human Annotated Scores","Yes","Influential work in computer science proposes that crowdsourced tasks are more useful than traditional metrics to assess if a topic model returns semantically meaningful and distinct topics (Chang et al. 2009). Therefore, I use the “topic intrusion” task proposed by Chang et al. (2009) to validate the topics from a SITS model estimated on 20 U.S. general election presidential debates held between 1992–2016.9
The topic intrusion task presents the human judge with a document, in this case a segment inferred by SITS. The judge is also presented with four word sets. Three of these word sets represent the three highest probability topics for the segment. The fourth word set is the intruder, drawn randomly from the segment’s low probability topics. Each word set contains the top eight frequent and exclusive (FREX) topwords for the topic (Roberts et al. 2014). I set up the topic intrusion task for 200 randomly drawn segments from the debates. Then, Amazon Mechanical Turk (MTurk) workers were asked to choose which word set was most unrelated to the passage. In line with recent work on validation procedures for topic models by Ying, Montgomery, and Stewart (2019), I ran two trials of the same 200 tasks. Figure 2 plots the results for each trial separately as well as the pooled result Workers competed 62% and 68% of the tasks correctly in Trial 1 and Trial 2, respectively. A difference of proportions test indicates that Trial 1 and Trial 2 are not significantly different (p = 0.25).Thisresultiscomparable to one, and better than three, of four models assessed by Ying, Montgomery, and Stewart (2019) using the topic intrusion task. In all, human coders and SITS largely agree about which topics are and are not associated with the inferred segments of the debates.
","topic intrusion from model topics","Content Validation","Yes","To do so, I follow a procedure proposed by Grimmer and King (2011) to evaluate “cluster quality,” which is the similarity of the documents (here, segments of the debates) estimated to belong to the same cluster (here, having similar topic distributions). Importantly, I evaluate SITS segments against segments derived from a hand-coding approach. Hand-coded data are from Boydstun, Glazier, and Pietryka (2013) for the 1992, 2004, and 2008 U.S. general election presidential debates. Boydstun, Glazier, and Pietryka hand code several variables from the debate transcripts, including the topic of each question posed to the candidates and the topic of each phrase in the candidates’ responses. Then, they deem a candidate as going “off-topic” and thus, engaging in agenda-setting behavior, if the phrase’s topic does not correspond to the question’s topic. Comparing the segments inferred by SITS to those derived from hand coding required five steps. First, I determined where topic changes occurred (and thus, formed segments of the debates) according to each method. Second, I determined the similarity of these segments according to each method’s topic assignments.10 Third, I set up the exercise outlined by Grimmer and King (2011). Separately with the segments from the SITS and the hand-coding approaches, I drew 25 random pairs of segments with the same most-assigned topic and 25 random pairs of segments with a different most-assigned topic.11 Fourth, four unique MTurk workers rated the similarity of the segments within each pair on a 3-point scale: (1) unrelated, (2) loosely related, or (3) closely related.12 Of interest is each method’s “cluster quality,” which is “the average similarity of pairs of documents from the same cluster minus the average similarity of pairs of documents from different clusters, as judged by human coders one pair at a time” (Grimmer and King 2011, p. 5). The fourth and final step is to use difference in means to compare the two methods. Figure 3 plots this point estimate along with the 80% (thick line) and 95% (thin line) confidence interval. The estimated difference between approaches is positive and significant. Therefore, the Grimmer and King (2011) evaluation suggests SITS can infer segments that are even more coherent than those derived from a hand-coding approach.","assessing the interrelated nature of where shifts in topic occur and the topics themselves by examining the resulting segments of an interaction. Using crowdsourced human judgments, I find that SITS segments are viewed as more coherent than segments derived from a hand-coding approach.","External: Human Annotated Scores","Yes","In particular, the results for 1992, 2004, and 2008 are in line with results from the hand-coded content analysis. Boydstun, Glazier, and Pietryka (2013) note that in 1992 and 2008, the economy was a salient issue, but in 2004, defense was uniquely more important to the public than the economy. These patterns hold in Figure 4, as we see the 2004 election discussed the economy less than any other election","Face Validation by drawing on knowledge on the salient political issues","Content Validation","No","","","","","","","","","","","","","","","","","Yes","Yes","Yes","",""
"2022/09/06 3:43:19 PM OEZ","lukas.birkenmaier@outlook.de","A Pairwise Comparison Framework for Fast, Flexible, and Reliable Human Coding of Political Texts","Carlson, David; Montgomery, Jacob M.","No","Introduction of a platform to conduct pairwise comparison","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2022/09/06 3:51:04 PM OEZ","lukas.birkenmaier@outlook.de","Twitter and News Gatekeeping: Interactivity, reciprocity, and promotion in news organizations’ tweets","Russell, Frank Michael","No","only hand coding for variables, quantitative analysis only includes descriptive statistcis, such as likes and retweets","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2022/09/06 5:04:49 PM OEZ","lukas.birkenmaier@outlook.de","Machine Translation Vs. Multilingual Dictionaries Assessing Two Strategies for the Topic Modeling of Multilingual Text Collections","Maier, Daniel; Baden, Christian; Stoltenberg, Daniela; De Vries-Kedem, Maya; Waldherr, Annie","Yes, Continue with Coding","","Newspaper;Social Media: Twitter","Others","Hebrew, Arabic, English","Topics","Unsupervised: Topic Modeling;Rule-based: Adjusted dictionary","Structural Topic Models (STM)","Yes","Two ""Preprocessing"" steps are compared to deal with multilingual texts for LDA","Yes","The dictionary was repeatedly validated and improved until it achieved precision and recall scores well in excess of .75 in each language (see Baden & Stalpouskaya, 2015; Tenenboim-Weinblatt & Baden, 2021 for more detail). As the dictionary uses structural features of the documents (e.g., word distances, syntactic structure) for concept identification and disambiguation,8 the dictionary was applied to the unprocessed text","reference to previous validation excercise","Content Validation","Yes","Following the modeling stage, the obtained models were validated and compared using both quantitative and qualitative procedures. For the quantitative comparison, we calculated the extent to which different topic models obtained from the respective methodological approaches break down the variation included in each corpus in similar ways. To do so, we use the Correlation Matrix Distance (CMD), a distance measure designed to determine the (dis)similarity between two quadratic correlation matrices (Herdin et al., 2005; Motta & Baden, 2013). Theoretically, the CMD ranges from one (no association between both matrices) to zero (both correlation matrices are identical, up to a scale factor). For this estimation, we depart from the compared topic models’ n � k-sized document-topic (θÞ matrices, wherein each of the n ¼ 1; . . . ; N rows correspond to a document in the respective corpus, and each of the k ¼ 1; . . . ; K columns to a topic in the respective topic model. In these matrices, the cell entries θnk represent the probability of document n to contain a given topic k – usually interpreted as the topic proportion that the respective document contains. Since the identity of the compared topics is uninformative for the comparison (i.e., one can permutate the order of topics (columns) in each matrix without changing the structural similarity), we transform the raw θ matrices into quadratic correlation matrices by calculating the dot product of θ with its transpose θT. As a result, we obtain, for each topic model, an n � n matrix C that reflects the extent to which different documents are composed of the same topics. The CMD then computes the distance between each pair of matrices C, which provides a direct measure of the extent to which the topics obtained by two compared topic models are distributed in (dis)similar ways over the same set of documents. Since the dimensionality of C depends solely on the number of the n documents, this transformation allows us to compare the θ’s of different topic models independently of their number of topics K. Formula 1 defines the CMD for the two correlation matrices CMTand CMD that result from the two methods under investigation, as denoted by their subscripts","calculated the extent to which different topic models obtained from the respective methodological approaches break down the variation included in each corpus in similar ways","Unsure","Yes","For the qualitative comparison, we first selected one best-fitting model per corpus and approach. Among all estimated topic models, we focused on those five models that offered the best fit based on the evaluation metrics offered by the STM package (K ¼ 10; 15; 20; 25; 30 f g for both corpora and both methods). Two of the authors jointly judged these models’ interpretability, inspecting their top associated words (MT approach) and concepts (MD approach). Based on this information, we selected for the News corpus those topic models obtained for K = 25 (independently for both MT and MD), and for the Twitter corpus those models obtained for K = 30, for further validation. For these selected models, all topics were then carefully labeled and validated by the same two researchers. To ascertain the validity of topics and their labels, we selected for every topic a random sample of ten documents with a topic probability above a threshold of t ¼ 0:3. Given that only few topics are prevalent in a single document, the threshold can be considered sufficiently high for a topic to be the most prevalent in the respective documents (see also Maier et al., 2018). Topics were confirmed as interpretable and labeled if, through a discursive process, both judges agreed on the same interpretation, which had to be supported both by the identified top words/concepts and by the inspected documents, read against our familiarity with the conflict and the referenced events.","Jugdement of models’ interpretability, inspecting their top associated words (MT approach) and concepts (MD approach)","Content Validation","No","","","","","","","","","","","","","","","","","","","","","Yes","No","No","",""
"2022/09/06 5:41:21 PM OEZ","lukas.birkenmaier@outlook.de","Information Credibility under Authoritarian Rule: Evidence from China","Chang, Charles","No","only hand coding of texts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2022/09/07 9:20:10 AM OEZ","lukas.birkenmaier@outlook.de","Predicting Partisan Responsiveness: A Probabilistic Text Mining Time-Series Approach","Bustikova, Lenka; Siroky, David S.; Alashri, Saud; Alzahrani, Sultan","Yes, Continue with Coding","","Party Websites","Others","Slovak (slovakia)","Partisan Responsiveness","Supervised: Machine Learning;Unsupervised: Topic Modeling","LDA Topic Model, sparse-learning classifier (SLEP)","Yes","two step approach; First LDA, then Classification","Yes","Determining the number of topics can be done using various methods (e.g., elbow curves, AIC, BIC, etc.). Among these approaches, LDA tends to be most resilient when the number of topics, k , increases (Blei et al. 2010). However, larger k imposes additional computational costs and makes convergence of the posterior probability estimate more difficult. Finding the right k also requires qualitative validation by experts. Aer multiple trials, we determined that the most applicable k was 100. Later, we determined that the results are robust to minor changes to k (","Topic Model: ""qualitative evaluation""","Content Validation","Yes"," Table 1, in the Supplementary materials, shows the accuracy for predicting ethnic party spikes that lead to radical right party responses. The accuracy varies between 81% and 89% for different issues (F-measure). Table 2 shows the accuracy for predicting ethnic spikes that the radical right parties ignore, which varies between 78% and 84% (F-measure). The average F-measure for predicting outcomes of ethnic spikes is therefore 82.9%. Similarly, Table 3 shows the accuracy for predicting ethnic party responsiveness to radical right party spikes, which varies between 80% and 86% depending on the issue (F-measure). Table 4 shows the accuracy for predicting radical spikes that are ignored by the ethnic parties, which varies between 78% and 86% (F-measure). The average F-measure for predicting outcomes of radical spikes is about the same: 82.7%.","Accuracy/F1 Score of ML classifier on topic escalation on the basis of LDA topic (changes)","External: Scores from other CATM","No","","","","","","","","","","","","","","","","","","","","","","","","","Yes","Yes","Yes","",""
"2022/09/07 11:39:29 AM OEZ","lukas.birkenmaier@outlook.de","Computational Identification of Media Frames: Strengths, Weaknesses, and Opportunities","Nicholls, Tom; Culpepper, Pepper D.","Yes, Continue with Coding","","Newspaper","English","","Frame Identification","Supervised: Machine Learning;Unsupervised: Topic Modeling;Unsupervised: K-Means Clustering, Unsupervised: Evolutionary Factor Analysis","K-Means, Evolutionary Factor Analysis, Structural Topic Model (STM)","Yes","comparison of different methods to identify media frames","Yes","We analyze two separate versions of this USA Today dataset. The first is the full collection of 4,653 news articles, covering all aspects of banking (and, indeed, containing some articles about non-bank related entities such as Red Bank (New Jersey), Tyra Banks, and the overflowing banks of various rivers). The second is a smaller 2,394 article subset produced by aggressively filtering out articles from the full dataset that match keywords associated with non-bank entities and subjects6. Thus, we have one narrow dataset (Australia Royal Commission) and one broad dataset (banking in USA Today), with the latter having both a filtered and unfiltered version. The value of using two contrasting kinds of data in this context is that they reflect two ends of a range of possibilities for the frame analyst. On the one hand, the Australian corpus is a fairly conventional tightly demarcated group of articles reflecting a single (extended) story. On the other, the USA Today corpus is much broader. It is the kind of dataset that would","Using two datasets to evaluate model performance","Content Validation","Yes","We also carried out a manual analysis on the Australian dataset, for comparison with the computational methods. The basic structure of the analysis was based upon the procedure recommended by Chong & Druckman (2007, pp. 106–108). In the first stage, the authors specified the event for which frames would be sought; in this case, the Australian banking Royal Commission. In the second stage, the frame-defining attitudes were chosen to be attitudes to the banking scandals. For the third stage, two researchers (the first author and a research assistant well-versed in the Australian banking scandals) worked separately after being briefed on the event and attitudes, and also the Chong & Druckman definition of emphasis frames. Each was issued an identical 300-article randomly ordered random sample of the Australian data, and instructed to inductively identify a framing schema. We do not claim that the results of this manual analysis are “the” gold standard set of frames against which to measure the computational approaches. Given the challenges of operationalizing framing and the low reliability of human coding as a result of researcher effects (Matthes and Kohring, 2008), we offer them as one plausible set of frames, not as the only one. However, they do stand up as reasonably distinct statements of problem definition and causal interpretation of Australian banking scandal of 2018.","manual identification of overarching frames","External: Human Annotated Scores","Yes","We therefore evaluate the different frame identification methods subjectively, according to the quality of the frames they have selected. There are no quantitative measures of each of these criteria; they are abstract theoretical qualities about which we make judgments. We use three criteria for quality, with the latter two drawing on Gerring (2001) and Roberts et al. (2014). First, the extent to which the frames identified by the methods fit with our understanding of what a media frame is – an interpretive lens that includes the definition of a problem and a diagnosis of its causes (Entman, 1993, p. 52). Second, the extent to which each identified frame is internally coherent. Third, the extent to which each identified frame is externally differentiable from other potential frames.","human evaluation of frames identified by different methods","Content Validation","No","","","","","","","","","","","","","","","","","","","","","Yes","Yes","Yes","",""
"2022/09/07 1:36:44 PM OEZ","lukas.birkenmaier@outlook.de","Latent Semantic Scaling: A Semisupervised Text Analysis Technique for New Domains and Languages","Watanabe, Kohei","Yes, Continue with Coding","","Newspaper","Others","English, Japanese","sentiment","Unsupervised: Text Scaling","New semisupervised document scaling technique Latent Semantic Scaling (LSS","Yes","development and application of a new method","Yes","Following Young and Soroka (2012), I computed mean sentiment scores for articles in the sample to compare the results of manual and machine coding. In Figure 4, the mean scores correspond almost linearly with the manual coding in LSD, while the mean scores for moderately positive (+1) articles are lower than they should in LSS; the standard errors for very positive articles are also smaller in LSD than in LSS; both LSD and LSSscaled negative articles accurately. The overlapped 95% confidence intervals indicate that some of the mean differences are not statistically significant, but their standard errors will be much smaller when the sample is larger. Figure 5 shows the correlation between machine and manual coding in a longitudinal setting. Although the mean sentiment scores can be less accurate in both manual and machine coding as the articles are spread over 30 years, I can observe the strong correlation between them, especially humans and LSS until 1995. However, neither LSD nor LSS replicate negative overall shifts in manual coding from 1993; one of the largest discrepancies between humans and machines can be found in 2006 when the number of articles is the smallest. In this setting, the correlation between","comparison with hand-coded scores","External: Human Annotated Scores","Yes","However, neither LSD nor LSS replicate negative overall shifts in manual coding from 1993; one of the largest discrepancies between humans and machines can be found in 2006 when the number of articles is the smallest. In this setting, the correlation between humans and machines is r = 0.62 in LSD and r = 0.70 in LSS (Figure 6), although the correlation between LSD and LSS is only r = 0.34.","comparing model estimates (LSS) with dictionary (LSD)","External: Scores from other CATM","Yes","In order to demonstrate how LSS can be used in real research projects, I applied the fitted model to the entire corpus of the English articles (Figure 7). I can confirm that the news articles were largely positive in the period of Reaganomics (1982–1989) but they became very negative after the savings and loan crisis; they became very positive after 1995, when America’s service sector enjoyed prosperity, but very negative after the occurrence of the economic crisis in Asia; the bursting of the dot-com bubble made their sentiment even more negative; the subprime mortgage crisis that triggered the global economic crisis also changed the sentiment dramatically. Figure 8 shows that mean sentiment scores by LSD and LSS are very strongly correlated with each other (r = 0.77) and with the changes in gross domestic product (GDP) of the United States (r = 0.65). Large discrepancies between the mean sentiment scores and the economic indicator are found in 1984 and 1997–1999, but the fall in sentiment in the latter period was caused mainly by the financial crisis in Asia","Face validation of scores on a long time frame","Content Validation","No","","","","","","","","","","","","","","","","","","","","","Yes","Yes","Yes","",""
"2022/09/07 2:12:20 PM OEZ","lukas.birkenmaier@outlook.de","The Automatic Analysis of Emotion in Political Speech Based on Transcripts","Cochrane, Christopher; Rheault, Ludovic; Godbout, Jean-François; Whyte, Tanya; Wong, Michael W.-C.; Borwein, Sophie","Yes, Continue with Coding","","Politics: Parliamentary Records","English","","emotional content","Supervised: Machine Learning;Rule-based: Off-the-shelf dictionary;Rule-based: Development dictionary","Lexicoder 3.0, Sentiwordnet 3.0, Hu-Lui Lexicon, Jockers-Ringers Lexicon, VADER","Yes","comparison of different methods to analyse parliamentary data on the sentence level","Yes","Figure 3 compares tools that we tested for predicting the sentiment scores of our human text coders, including the dictionary induced using word2vec embeddings described above. For each measure, the confusion matrix overlays a jitterplot to show the proportional reduction in error alongside the classification accuracy of the different tools. For the measure of accuracy, we consider a classification successful if the method produces a score in the positive range – past the midway point – while the human coders also coded a text as positive on average; the opposite must be true for negative predictions. This corresponds to the percent correctly predicted commonly used in binary classification tasks. Some of the dictionary tools performed well overall, but results were invariably mixed. Lexicoder was attuned very effectively to negative sentiment, but not as well to positive sentiment. Notably, many of the video transcripts contained no words in the Lexicoder, HuLiu, and Vader dictionaries. In total, 33% of the sentences were unclassified by Hu-Liu, 29% by Lexicoder, and 19% by Vader. Among the dictionaries, Sentiwordnet classified the largest proportion of the sentences, but it was also the least accurate.9 As we discussed earlier, there is no pre-classified or human annotated Hansard on which to train supervised learners, and manually annotating Hansard would be time-consuming, costly, and potentially predetermine the results to the same extent as human curated dictionaries. Nonetheless, it might be possible to leverage models trained on large annotated corpora from other domains. Thus, we also tested a number of supervised learners trained","comparison with hand-coded date","External: Human Annotated Scores","No","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Yes","Yes","Yes","",""
"2022/09/12 3:17:35 PM OEZ","lukas.birkenmaier@outlook.de","Role-based Association of Verbs, Actions, and Sentiments with Entities in Political Discourse","Fogel-Dror, Yair; Shenhav, Shaul R.; Sheafer, Tamir; Van Atteveldt, Wouter","Yes, Continue with Coding","","Newspaper","English","","sentiment oriented sentiment analysis","Supervised: Part of Speech Tagging","Lexicoder Sentiment Dictionary, role-based association method","No","","Yes","We initially reviewed the results on a timeline by aggregating the sentiment scores (per entity, per article) by summing them per day. While reviewing the entity-oriented sentiment timeline separately for Israel and the PA, observations with high levels of negative sentiment scores for both entities were correlated with the two main violent events in this period —“Operation Protective Edge” in the Gaza strip (July-August 2014), and the Temple Mount/Al-Aqsa Mosque clashes (October 2015). Please refer to Figure 1 (the graph for the 10-word proximity method is practically identical to the graph created by the sentence proximity method). These results might seem promising at first, as negative sentiment is expected to be attributed to rival entities in violent events. However, when placed side by side at the daily level (Figure 1), the sentiment–entity timeline of Israel and the PA looked fairly symmetrical. Assigning a negative sentiment to both entities may be a reasonable finding, as both participated in the violent events; yet, the level of symmetry still raises a concern. Is it really the case that the media is so balanced in terms of negative sentiment toward both actors, or is this a problem of discriminant validity (Adcock & Collier, 2001)? If the latter is true, this means that instead of measuring the specific sentiment that was supposed to be associated with each entity, we measured the volume of sentiment expressed in the text in general. If this is true, we actually measured the textual sentiment surrounding the entities but not the sentiment attributed to them.","face validation","Content Validation","Yes","to test our concern, we measured the correlation between sentiment scores that were assigned to the two entities in each sentiment category—positive and negative. The results pointed at a strong correlation for negative sentiment (Pearson’s r > .74) and moderate correlations for positive sentiment (r > .50). To further test whether the scores measured the number of sentiment expressions instead of the sentiment assigned to each entity, we measured the correlations between the volume of sentiment expressions (positive/negative) for each entity and the total sentiment score (positive/negative). The results here pointed to the same validity-related concerns, as we found moderate-to-strong correlations for negative sentiments and positive sentiments for both entities (r > .76 and r > .42, respectively). All correlations were significant (p < .05). This means that these proximity methods of association do not clearly differentiate among the general sentiments expressed in the text and the sentiments intended to be associated with each entity.","comparison with Model estimates with two unassociated sentiment scores","External: Scores from other CATM","Yes","Two trained human coders classified each sentiment verb expression as associated with Israel, the PA, neither, or both (Krippendorf’s α = .8). Differences between coders were discussed to form a gold standard that was used to train and test the algorithm. At this point, for every verb, the dataset contained the gold standard and the four features. We split the dataset into a training set (60%), to be used by the learning version, and a test set (40%), and created two versions of the RBA method: rule-based and learning. The rule-based version was easier to implement and did not need the addition of manually coded examples to train the algorithm. It had two rules: (1) If the verb is active, associate the sentiment measured in the verb with entities found in the subject, and otherwise with entities found in the predicate. (2) If the verb is a direct referencing verb, reverse the association. To improve the accuracy of the method by fine-tuning the weights of the features inductively and allowing for further expansion of the method, we created a version of the method that replaces the two a priori rules with a learning algorithm. To this end, we used a Python implementation of a support vector machine with a linear kernel (Pedregosa et al., 2011). The algorithm was trained on the training set (N = 615 verbs).","Human annotated test set","External: Human Annotated Scores","Yes","As a remedy, we focused the manual validation on measuring negative sentiment only, following researchers who have found negativity to be clearer to measure (Haselmayer & Jenny, 2017; Soroka et al., 2015). We also instructed human coders to follow the perspective and assumptions of the lexicon. For example, as the sentiment lexicon did not assign different values to different expressions, we only measured the quantity of sentiments, and not their quality (e.g., murdering a person and injuring a person had similar negative weights). Two human coders were trained to code articles for sentiments expressed toward each entity—Israel and the PA on a three-level scale: 0 (no negative sentiment), 1 (low level of negative sentiment), and 2 (high level of negative sentiment—see the full coding instructions in Section C in the online Appendix). As we were interested in the bias toward one entity compared with that toward the other, we calculated the value of delta between the negative sentiments associated with Israel and the PA. The result was a five-point scale of negativity measure that spanned from −2 (PA was represented more negatively) to + 2 (Israel was represented more negatively). Intercoder reliability was no lower than Krippendorf’s α = .71.

Moving to the validation of the association method, this analysis provides further support for our main concerns regarding proximity-based approaches and supports our alternative method. First, we found support for our claim regarding discriminant validation, as 39% of all articles were scored with a non-zero negativity score, which means that sentiments associated with both entities were asymmetrical. This finding was even more convincing when articles containing no negative sentiment toward any entity were eliminated. From the remaining articles, 72% received a non-zero negativity score. Second, results at the week level show that while the two proximity methods of association (sentence and 10-word) were not significantly correlated with the manual analysis (in fact, the correlation was negative), the RBA method was positively and significantly correlated with it (Spearman’s ρ = .26, p < .05). Although modest (maybe because of the RBA’s focus on associations of verbs), this correlation supports the validity of the RBA method compared with the other two proximity methods of association considered. These findings show that associations formed by the RBA method are considerably more accurate at the expression level and more valid at the document level than those made by proximity methods.","Manual Validation to test for discriminant validity (using hand-coded data, but the goal here is to evaluate discriminant validity between their method and other methods)","Content Validation","No","","","","","","","","","","","","","","","","","No","","","",""
"2022/09/12 3:47:26 PM OEZ","lukas.birkenmaier@outlook.de","A General Model of Author “Style” with Application to the UK House of Commons, 1935–2018","Huang, Leslie; Perry, Patrick O.; Spirling, Arthur","Yes, Continue with Coding","","Politics: Parliamentary Records","English","","distinctiveness in speech","Unsupervised: Statistical Model based on Word Frequencies","not named","No","","Yes","One basic requirement is that our method ought to label “intrusive” texts, i.e. ones that were not clearly produced by the parliamentary data generating process, as distinctive. To put this to the test, we took the set of backbenchers from a modern session (2015–2016) added to them the State of the Union speeches given by U.S. President Barack Obama. We randomly sampled n “speeches” of m sentences each from the SotU speeches, where n is the mean speeches per MP and m is the mean speech length in sentences for the 2015 session. Aer inserting Obama in the corpus, we select the vocabulary using our standard cross-validation procedure. We used Obama because although his works are approximately contemporaneous with our data, his style is distinctive relative to our MPs: they come from an American rather than British political system, and they are long oratories consumed by the general public rather than speeches directed primarily at other politicians. With that in mind, our model should identify Obama as easily the most distinctive author. As we see from Figure 1, this is indeed the case: Obama’s ƒt point estimate is by far the largest in the data and appears at the far top right. Its confidence interval does not overlap with any other MP in the distribution (note that we do not include every MP in the graphic due to limited space, but we do include the full range in terms of distinctiveness estimates). Of course, such a test might be cherry-picking, and there is no obvious baseline for performance (other than identifying the intruder).7 So we now turn to a domain-specific assessment.","Intrusion of outlier speeches","Content Validation","Yes","For more substantive performance evaluation, we look at the “most distinctive” and “least distinctive” backbench MPs for the parliamentary sessions on either side of Blair’s election landslide in 1997 (that is, 1995–1996 and 1998–1999). This has the advantage of being a period in which (a) control of the Commons switched (from Conservative to Labour), meaning we have variation in the party of the backbenchers, and (b) there are academic accounts which help ground our understanding of MPs at this time (Cowley 2002; Spirling and Quinn 2010; Kellermann 2012). In terms of measurement, we use a convergent validity approach: we compare our measure to another (computed independently) and show that they are related as expected.

To see how we proceed in practice, note that for each MP t , in each session, we have an estimate of their distinctiveness in log-odds terms: our ƒ, above. For current purposes, however, we focus on something related but more concrete and directly interpretable: the proportion of their speeches which are correctly predicted as being from them relative to all other MPs (proportion of speeches correctly predicted, or “PCP,” in the tables below). We use fivefold cross-validation to fit a model to texts from a given session, predict the speakers of held-out texts using this model, and calculate each speaker’s rate of correct predictions; we report each speaker’s mean PCP. To validate these estimates, we consider their extrema—their minimums and maximums. In the subsection tables below, we list the twenty names of the MPs who were most distinct and least distinct by this measure (subject to having made a minimum of twenty speeches). We do this for the two sessions in question: one in 1995–1996 and one in 1998–1999. We also list the number of mentions of each MP in the Times newspaper archives (via Gale Group Digital Archive) for the same period, specifically the “Politics and Parliament” subsection of the “News.”","criterion validity (even tough they claim it to be convergent validity, but the comparison is with MP mentions in the TIME newspaper and thus an external criterion)","External: Criterion data / Predictive validation","Yes","More substantively, we note the presence of several Labour “rebels” among the most distinct. These include Tony Benn, Diane Abbott, John McDonnell, Roger Berry, and Tam Dalyell, all of whom consistently voted against the Labour government’s plan to reform the welfare state.8 Peter Temple-Morris was a party switcher, and “interesting” for that reason—he was elected as a Tory MP in 1997, but then crossed the floor to Labour the same year. The most interesting MPs here include Stuart Bell, who was the Church Estates Commissioner, meaning that he was one of the managers of the Church of England’s property. David Hinchliffe, chairman of the Select Committee on Health, was subsequently extremely critical of the Blair government’s proposed reforms to the National","face validity of most extreme MPs","Content Validation","Yes","Finally, with respect to a broader understanding of validity, we ask what exactly we are capturing as “distinctiveness” in our measure? As regards our comments at the opening of Section 2, is it mere “phrasing” (different ways of saying the same thing) or “substance” (saying something different)? Put more directly with respect to the extant literature, does our model “improve” (in fit terms at least) over the original Mosteller and Wallace (1963) approach and capture something more than function word usage?

To assess this, we conducted a simple experiment. We ran a special case of the estimation using only the seventy function words (i.e. stop words) from the original Mosteller and Wallace (1963) study. Our contention is that if our model is simply capturing idiosyncratic stylistic differences (in the narrow sense meant in that earlier literature), the restricted version should perform approximately as well as the more general one that uses all words in the vocabulary. Studying Figure 2, we see this is clearly false: there, the bottom line with triangle points is the mean prediction rate (for each speaker, with fivefold cross-validation) from the stop word model. The top line is the mean prediction rate from our model, which has no restrictions on stop words (as in the rest of this paper). It performs about three to five times as well as the pure phrasing model, on average. This implies that there is certainly something more than expressive manner going on: we happily refer to that residual variation as “substance.” This does not mean, of course, that the Mosteller and Wallace (1963) approach vocabulary is “wrong” (it is just a special case of ours), but it does suggest our model is doing something statistically useful in terms of capturing practical variation between contemporaneous MPs. Why do we see this performance difference? From inspection, we note that the fit improvement comes mostly from the middle of the distribution (that is, both our approach and the more simple one perform similarly for the most and least distinctive MPs but not for the median and mean—at least for the sessions we looked at in detail). We suspect this is because while almost everyone will have non-zero use of all of the Mosteller and Wallace (1963) words, our richer vocabulary has much higher variance in use. At the top of the distribution—MPs who are distinctive whatever the vocabulary—this makes no difference. Conversely, at the bottom of the distribution—MPs who use neither vocabulary very much—this also makes no difference. But for MPs in the middle, our much larger vocabulary offers more opportunities to distinguish oneself (for a fixed amount of speaking), and, thus, our model does better for these people. Before moving to the results, we note that readers may be qualitatively interested in the underlying tokens that affect distinctiveness of individuals in the model: in Appendix C, we discuss how these might be obtained and examined.","Evaluation of Model Outputs with small subset of data (only stopwords) to test model features","Content Validation","No","","","","","","","","","","","","","","","","","Yes","No","No","",""
"2022/09/13 12:01:22 PM OEZ","lukas.birkenmaier@outlook.de","Strategy Framing in News Coverage and Electoral Success: An Analysis of Topic Model Networks Approach","Walter, Dror; Ophir, Yotam","Yes, Continue with Coding","","Newspaper","English","","Strategy Framing","Unsupervised: Topic Modeling","Analysis of Topic Model Network (ANTMN)","Yes","Strategy frames focus on candidates’ standing in polls (using horse race metaphors), their motivations, performance, character, and style. Issue frames focus on candidates’ record and opinions on policies. Media emphasis of strategy at the expense of issue frames (Aalberg et al., 2012; Patterson, 1994) was found to yield cynicism toward the political process (Cappella & Jamieson, 1997) and the media (Hopmann et al., 2015) and to decrease political knowledge (Cappella & Jamieson, 1997).","Yes","The strategy and issue estimation per document were further validated using manual content analysis of 150 articles by two independent trained coders (not the authors; Krippendorff’s α = .85). As coders used the common five-level strategy issue scale (Aalberg et al., 2012) and our estimation was a continues 0–100 scale (the percentage of the language that related to strategyoriented topics), we used Pearson’s correlation to examine the similarity between the two scales at the article level. Results showed high agreement between the manual coders and the auto mated estimations (r = 0.81, p < .001; see Appendix 2 for detail).","Human annotation","External: Human Annotated Scores","No","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Yes","No","No","",""