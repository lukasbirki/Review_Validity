Zeitstempel,title,These are the Authors,"Please scan the text. 
Does the study fulfil the eligibility criteria?
- Empirical Study
- Field of Political Communication
- Using Text as Data Method to operationalise a construct of interest 

You may come back to this question if you have started to fill in some fields as well",If no: Why is the study out of scope?,source,language,language_others,"Please name the target construct as described by the authors)
Note: Please use the exact wording of the authors, such as ""affective polarization"", ""semantic complexity"".
If there is more than one construct studied, please separate constructs by a comma (,)",construct_new,method_type,method_name,What is the outcome of the method?,Do you have any comments on the study design?,If yes: What are your comments on the study design?,Are steps of validation reported?,Is there a link to an Appendix/additional Materials/GitHub repository?,"
Is the data for the analysis accessible?",Is the code for the analysis accessible?,E-Mail-Adresse,id,doi,journal,year,n_validation,location,v1,v2,v3,v4
2022-11-29T16:51:20Z,Understanding Delegation Through Machine Learning: A Method and Application to the European Union,"Anastasopoulos, L. Jason; Bertelli, Anthony M.","Yes, Continue with Coding",NA,Party Politics: Legislative Documents,English,NA,authority and constraint,Authority,Supervised: Machine Learning,gradient-boosted decision trees,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Understanding_Anastasopoulos,",10.1017/S0003055419000522,American Political Science Review,2020,2,a,"Franchino (2007) tests a hypothesis regarding the relationship between the decision
rule for a law|specically, qualied majority (QMV) versus unanimity rules|and
discretion to national administrations. Proposition 2.2 from his formal theory states
that the discretion of national administrations under unanimity voting is greater than
or equal to that prevailing under QMV (54{55). He tests this claim in several models,
but our focus is on the \summary"" tobit specications for national administrationsin the right-hand column of Table 5.6 that control for other hypothesized variables
(172). The dependent variable is the discretion index (^), while the independent
variable of interest is Decision rule, which takes a value of one for QMV and zero for
unanimity rules (122), and is expected to have a negative eect in Table 5.6. 6 We
focus on this variable because of its measurement quality: QMV versus unanimity
rule is the precise distinction that Franchino's formal theory incorporates.
As a nal validity check, we seek to replicate the foregoing regression results while
treating 90% of Franchino's original observations as out-of-sample. This allows us
to show that even with a very limited random sample of hand-coded provisions, we
can achieve substantive eects that are similar in sign, signicance and magnitude
to Franchino's original inferences.",replicating Substantive Results Out-of-Sample from another study,Unsure,Yes
2022-11-29T16:51:20Z,Understanding Delegation Through Machine Learning: A Method and Application to the European Union,"Anastasopoulos, L. Jason; Bertelli, Anthony M.","Yes, Continue with Coding",NA,Party Politics: Legislative Documents,English,NA,authority and constraint,Authority,Supervised: Machine Learning,gradient-boosted decision trees,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Understanding_Anastasopoulos,",10.1017/S0003055419000522,American Political Science Review,2020,2,b,"Table 2 contains performance metrics for each of the classiers. Figures 2(a) and
2(b) contain information about term importance for the EC and national admin-
istration authority classiers. The overall quality of each classier is captured by
the F1 statistic, which combines precision and recall (sensitivity) in a single metric
ranging from 0{1. Recall refers to classier performance in detecting authority and
constraint, while precision refers to its ability to distinguish between correctly and
incorrectly labeled provisions. The precision and recall statistics in Table 2 suggest
that our trained classiers perform well for detecting authority and some categories
of constraint (high recall), but that they may tend to overestimate constraint (low
precision for some constraint categories). Figures 3a, 3b and 4a, 4b plot predicted
versus hand-coded delegation ratios averaged over each year in the training sam-
ple, 1958{1993 using 50% out of sample predictions. Both show that our classiers",hand-coding,External: Human Annotated Scores,No
2022-11-28T11:18:33Z,Enhancing Theory-Informed Dictionary Approaches with “Glass-box” Machine Learning: The Case of Integrative Complexity in Social Media Comments,"Dobbrick, Timo; Jakob, Julia; Chan, Chung-Hong; Wessler, Hartmut","Yes, Continue with Coding",NA,"Social Media: Twitter, Social Media: Facebook, Website Comments",Others,"English, German",integrative complexity,Complexity,"Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary",na,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Enhancing_Dobbrick,",10.1080/19312458.2021.1999913,Communication Methods and Measures,2021,2,a,"The gold standard was coded by three trained human coders based on the previously described scale
from 1 (lowest complexity) to 7 (highest complexity) (Baker-Brown et al., 1992). In a pretest with 100
stratified randomly selected user comments, the three raters reached a Krippendorff’s αordinal reliability
of .85. Each gold standard comment was then coded by two individuals, with disagreements being
resolved consensually. The coder pairs had reached Krippendorff’s αordinal reliability of .88 and .86 in
the pretest, respectively.",hand-coding,External: Human Annotated Scores,Yes
2022-11-28T11:18:33Z,Enhancing Theory-Informed Dictionary Approaches with “Glass-box” Machine Learning: The Case of Integrative Complexity in Social Media Comments,"Dobbrick, Timo; Jakob, Julia; Chan, Chung-Hong; Wessler, Hartmut","Yes, Continue with Coding",NA,"Social Media: Twitter, Social Media: Facebook, Website Comments",Others,"English, German",integrative complexity,Complexity,"Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary",na,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Enhancing_Dobbrick,",10.1080/19312458.2021.1999913,Communication Methods and Measures,2021,2,b,"The random forest regression model that was combined with the off-the-shelf dictionary additionally
allows to generate a variable importance table. This measurement quantifies the reduction in predictive
performance of the random forest regression model when a variable of interest is randomly shuffled along rows. Applying the original terminology from Breiman’s landmark paper (Breiman,
2001), the variable of interest is “noised” (being turned into meaningless random noise). A greater
reduction in performance after a variable being “noised” indicates that the variable is relatively more
important for generating the prediction. It is important to note that the value is comparative only
across the variables in the random forest model and thus there is no cutoff point. Due to the complex
interactions among variables in a random forest regression model, the metric does not tell us anything
about the relative importance of variables in the case of a simple linear combination of variables (e.g.,
equation 2). Nonetheless, this makes it possible to find those LIWC categories that were used (most
heavily) by the regression trees in the random forest ensemble to generate the prediction. Figure 2
shows the variable importance for the random forest classifiers fed with all available LIWC dimensions
(2nd level analysis), trained on the German and English corpus.In both models the top categories comprise several ones that also appear in the baseline LIWC
formula used by Wyss et al. (2015), including for example, “Excl,” “Discr,” “Sixl,” “Incl,” “Negate,”
“Tent,” and “Cause.” This suggests that the resulting classification model is closely linked to the
existing theory, which further validates this approach. The conjunctions (“Conj”) that are rather
important in the English model have also previously been part of theoretically derived LIWC-based
operationalizations of integrative complexity (Abe, 2011). Moreover, formal features such as punctuation
(e.g., “AllPunc,” “OtherP,” “comma”) or words per sentence (“WPS”) seem to carry important
information about the integrative complexity of the user comments. Additional top categories such as
job-related words, quantifiers, or achievement words have a further impact on explaining the operationalization
of integrative complexity in our case study.
The variable importance analysis presented in Figure 2 is tied closely to our use case and especially
our dataset. Its main use is to open the black box to diagnose the random forest model. This allows for
further insights into our dataset and the specific operationalization of integrative complexity that we
have based our study on. Yet, further research is required to show which categories carry importance
when this method is transferred to other datasets. This reiterates our point for the thorough validation
of instruments when carrying them over to new datasets or application domains.",error anaylsis,Content Validation,No
2022-11-24T19:00:58Z,"Dictionaries, Supervised Learning, and Media Coverage of Public Policy","Dun, Lindsay; Soroka, Stuart; Wlezien, Christopher","Yes, Continue with Coding",NA,Newspaper,English,NA,Media Policy Signal,Discourse,"Supervised: Machine Learning, Rule-based: Development dictionary","self-developed dictionary, random forest",NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Dictionaries,_Dun,",10.1080/10584609.2020.1763529,Political Communication,2021,2,a,"We use two indicators to judge model performance in our test set: precision and recall.
Recall is the proportion of correct predictions made by the model out of all human coded
sentences in a class (correct predictions divided by column sums in our confusion
matrices presented in Table 1).",hand-coding,External: Human Annotated Scores,Yes
2022-11-24T19:00:58Z,"Dictionaries, Supervised Learning, and Media Coverage of Public Policy","Dun, Lindsay; Soroka, Stuart; Wlezien, Christopher","Yes, Continue with Coding",NA,Newspaper,English,NA,Media Policy Signal,Discourse,"Supervised: Machine Learning, Rule-based: Development dictionary","self-developed dictionary, random forest",NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Dictionaries,_Dun,",10.1080/10584609.2020.1763529,Political Communication,2021,2,b,"The correspondence between the two media signals is striking, as is their correspondence
with spending. The two media signals – one based entirely on dictionaries, and the
other based on supervised learning – have a Pearson’s correlation of 0.85. How does each
measure compare with spending change? The correlation with spending change is only
marginally larger for the supervised learning signal (0.63) than for the dictionary signal
(0.60), and the difference is not reliable (N = 39; for the difference in correlation
coefficients, p = .84). In sum, dictionary-plus-supervised-learning approach yields only
a very small gain in the accuracy of our media policy signal.",using both CTAM methods on a new sample,External: Scores from other CATM,No
2022-11-30T13:55:21Z,"Living in the Past or Living in the Future? Analyzing Parties’ Platform Change In Between Elections,The Netherlands 1997–2014","Van Der Velden, Mariken; Schumacher, Gijs; Vis, Barbara","Yes, Continue with Coding",NA,Party Politics: Press releases,Others,Dutch,Party Plattform Change,Discourse,Unsupervised: Topic Modeling,LDA,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,Living_Van,10.1080/10584609.2017.1384771,Political Communication,2018,3,a,"On the basis of
reading 50 of the documents per topic, 50 titles of the press releases, and the 20 best-word
matches per topic, we selected the model that identified 25 topics (supplemental Appendix
B describes this process in more detail). The 25-topic model balances specificity in terms
of the issues the model describes (see Table B2 in supplemental Appendix B) and its
unique topics. For instance, in some of the models the issue of the economy was dispersed
over multiple issues (see supplemental Appendix B for our approach and the topics we
identified).",content valdiation of top words,Content Validation,Yes
2022-11-30T13:55:21Z,"Living in the Past or Living in the Future? Analyzing Parties’ Platform Change In Between Elections,The Netherlands 1997–2014","Van Der Velden, Mariken; Schumacher, Gijs; Vis, Barbara","Yes, Continue with Coding",NA,Party Politics: Press releases,Others,Dutch,Party Plattform Change,Discourse,Unsupervised: Topic Modeling,LDA,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,Living_Van,10.1080/10584609.2017.1384771,Political Communication,2018,3,b,"The correlation between the dependent variable based on models with a
different number of topics and our measure based on the 25-topic model is very high
(between 0.79 and 0.92; see Figure B1 in the supplemental Appendix)",rerunning analysis,Unsure,Yes
2022-11-30T13:55:21Z,"Living in the Past or Living in the Future? Analyzing Parties’ Platform Change In Between Elections,The Netherlands 1997–2014","Van Der Velden, Mariken; Schumacher, Gijs; Vis, Barbara","Yes, Continue with Coding",NA,Party Politics: Press releases,Others,Dutch,Party Plattform Change,Discourse,Unsupervised: Topic Modeling,LDA,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,Living_Van,10.1080/10584609.2017.1384771,Political Communication,2018,3,c,"As an additional validation, we compare
our measure to the average attention to issues in an electoral term to the average attention
to issues in election manifestos using the Manifesto Project (Volkens et al., 2014). A
problem with this latter approach is that the topics identified by our topic model and by the
Manifesto Project do not exactly match. Therefore, we focus only on a number of cases of
clear matches: the issue of the economy, the environment, multiculturalism, and the
European Union (EU). The correlation between the average attention for these four issues
in our data set with the average attention in the Manifesto Project is 0.76. This indicates
that what is salient to parties in our data corresponds with the salience of a topic in parties’
election manifestos. For instance, the correlation between attention to the environment in
our data and in the manifesto data for the Greens is 0.67. For the Freedom Party, the
correlation on multiculturalism as measured in our data and by the Manifesto Project is
0.98. This indicates that our measure conforms to the main data source used to measure
party platform change (for overviews of studies using this data, see Adams, 2012;
Fagerholm, 2015).",comparison of topics with manifesto project scores,External: Criterion data / Predictive validation,No
2022-09-05T16:00:16Z,Policy Diffusion: The Issue‐Definition Stage,"Gilardi, Fabrizio; Shipan, Charles R.; Wüest, Bruno","Yes, Continue with Coding",NA,Newspaper,English,NA,Policy Framing,Framing,Unsupervised: Topic Modeling,Structural Topic Model,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Policy_Gilardi,",10.1111/ajps.12521,American Journal of Political Science,2021,1,a,Figure 1 shows how topic prevalence is distributed over time across all states; a detailed validation is discussed in SI Appendix C.3.,Visual Inspection of Topic prevalende (supplementary materials can not be found),Content Validation,No
2022-09-07T11:39:29Z,"Computational Identification of Media Frames: Strengths, Weaknesses, and Opportunities","Nicholls, Tom; Culpepper, Pepper D.","Yes, Continue with Coding",NA,Newspaper,English,NA,Frame Identification,Framing,"Supervised: Machine Learning, Unsupervised: Topic Modeling, Unsupervised: K-Means Clustering, Unsupervised: Evolutionary Factor Analysis","K-Means, Evolutionary Factor Analysis, Structural Topic Model (STM)",NA,Yes,comparison of different methods to identify media frames,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Computational_Nicholls,",10.1080/10584609.2020.1812777,Political Communication,2021,3,a,"We analyze two separate versions of this USA Today dataset. The first is the full collection of 4,653 news articles, covering all aspects of banking (and, indeed, containing some articles about non-bank related entities such as Red Bank (New Jersey), Tyra Banks, and the overflowing banks of various rivers). The second is a smaller 2,394 article subset produced by aggressively filtering out articles from the full dataset that match keywords associated with non-bank entities and subjects6. Thus, we have one narrow dataset (Australia Royal Commission) and one broad dataset (banking in USA Today), with the latter having both a filtered and unfiltered version. The value of using two contrasting kinds of data in this context is that they reflect two ends of a range of possibilities for the frame analyst. On the one hand, the Australian corpus is a fairly conventional tightly demarcated group of articles reflecting a single (extended) story. On the other, the USA Today corpus is much broader. It is the kind of dataset that would",Using two datasets to evaluate model performance,Content Validation,Yes
2022-09-07T11:39:29Z,"Computational Identification of Media Frames: Strengths, Weaknesses, and Opportunities","Nicholls, Tom; Culpepper, Pepper D.","Yes, Continue with Coding",NA,Newspaper,English,NA,Frame Identification,Framing,"Supervised: Machine Learning, Unsupervised: Topic Modeling, Unsupervised: K-Means Clustering, Unsupervised: Evolutionary Factor Analysis","K-Means, Evolutionary Factor Analysis, Structural Topic Model (STM)",NA,Yes,comparison of different methods to identify media frames,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Computational_Nicholls,",10.1080/10584609.2020.1812777,Political Communication,2021,3,b,"We also carried out a manual analysis on the Australian dataset, for comparison with the computational methods. The basic structure of the analysis was based upon the procedure recommended by Chong & Druckman (2007, pp. 106–108). In the first stage, the authors specified the event for which frames would be sought; in this case, the Australian banking Royal Commission. In the second stage, the frame-defining attitudes were chosen to be attitudes to the banking scandals. For the third stage, two researchers (the first author and a research assistant well-versed in the Australian banking scandals) worked separately after being briefed on the event and attitudes, and also the Chong & Druckman definition of emphasis frames. Each was issued an identical 300-article randomly ordered random sample of the Australian data, and instructed to inductively identify a framing schema. We do not claim that the results of this manual analysis are “the” gold standard set of frames against which to measure the computational approaches. Given the challenges of operationalizing framing and the low reliability of human coding as a result of researcher effects (Matthes and Kohring, 2008), we offer them as one plausible set of frames, not as the only one. However, they do stand up as reasonably distinct statements of problem definition and causal interpretation of Australian banking scandal of 2018.",manual identification of overarching frames,External: Human Annotated Scores,Yes
2022-09-07T11:39:29Z,"Computational Identification of Media Frames: Strengths, Weaknesses, and Opportunities","Nicholls, Tom; Culpepper, Pepper D.","Yes, Continue with Coding",NA,Newspaper,English,NA,Frame Identification,Framing,"Supervised: Machine Learning, Unsupervised: Topic Modeling, Unsupervised: K-Means Clustering, Unsupervised: Evolutionary Factor Analysis","K-Means, Evolutionary Factor Analysis, Structural Topic Model (STM)",NA,Yes,comparison of different methods to identify media frames,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Computational_Nicholls,",10.1080/10584609.2020.1812777,Political Communication,2021,3,c,"We therefore evaluate the different frame identification methods subjectively, according to the quality of the frames they have selected. There are no quantitative measures of each of these criteria; they are abstract theoretical qualities about which we make judgments. We use three criteria for quality, with the latter two drawing on Gerring (2001) and Roberts et al. (2014). First, the extent to which the frames identified by the methods fit with our understanding of what a media frame is – an interpretive lens that includes the definition of a problem and a diagnosis of its causes (Entman, 1993, p. 52). Second, the extent to which each identified frame is internally coherent. Third, the extent to which each identified frame is externally differentiable from other potential frames.",human evaluation of frames identified by different methods,Content Validation,No
2022-09-13T12:01:22Z,Strategy Framing in News Coverage and Electoral Success: An Analysis of Topic Model Networks Approach,"Walter, Dror; Ophir, Yotam","Yes, Continue with Coding",NA,Newspaper,English,NA,Strategy Framing,Framing,Unsupervised: Topic Modeling,Analysis of Topic Model Network (ANTMN),NA,Yes,"Strategy frames focus on candidates’ standing in polls (using horse race metaphors), their motivations, performance, character, and style. Issue frames focus on candidates’ record and opinions on policies. Media emphasis of strategy at the expense of issue frames (Aalberg et al., 2012; Patterson, 1994) was found to yield cynicism toward the political process (Cappella & Jamieson, 1997) and the media (Hopmann et al., 2015) and to decrease political knowledge (Cappella & Jamieson, 1997).",Yes,Yes,No,No,lukas.birkenmaier@outlook.de,"Strategy_Walter,",10.1080/10584609.2020.1858379,Political Communication,2021,1,a,"The strategy and issue estimation per document were further validated using manual content analysis of 150 articles by two independent trained coders (not the authors; Krippendorff’s α = .85). As coders used the common five-level strategy issue scale (Aalberg et al., 2012) and our estimation was a continues 0–100 scale (the percentage of the language that related to strategyoriented topics), we used Pearson’s correlation to examine the similarity between the two scales at the article level. Results showed high agreement between the manual coders and the auto mated estimations (r = 0.81, p < .001; see Appendix 2 for detail).",Human annotation,External: Human Annotated Scores,No
2022-11-14T11:24:07Z,Quantitative analysis of large amounts of journalistic texts using topic modelling,"Jacobi, Carina; van Atteveldt, Wouter; Welbers, Kasper","Yes, Continue with Coding","general template, but replication of study for political framing",Newspaper,English,NA,political frame (on nuclear power),Framing,Unsupervised: Topic Modeling,LDA,NA,No,NA,Yes,Yes,No,Yes,lukas.birkenmaier@outlook.de,"Quantitative_Jacobi,",10.1080/21670811.2015.1093271,Digital Journalism,2016,2,a,"In topic 3, Cold War, the words “United”, “States”, “Soviet”, “Union” and “weapon”
immediately suggest that this is a topic about the US–Soviet conflict. Lower-ranking
words include variations on the “weapon” theme as well as more diplomatic terms such
as “agreement” and “proposal”. The topic occurs most frequently between the mid1950s and the mid-1980s, with a peak in the early 1960s that can be easily identified as
the Cuban missile crisis",inspecting the top words for each topic,Content Validation,Yes
2022-11-14T11:24:07Z,Quantitative analysis of large amounts of journalistic texts using topic modelling,"Jacobi, Carina; van Atteveldt, Wouter; Welbers, Kasper","Yes, Continue with Coding","general template, but replication of study for political framing",Newspaper,English,NA,political frame (on nuclear power),Framing,Unsupervised: Topic Modeling,LDA,NA,No,NA,Yes,Yes,No,Yes,lukas.birkenmaier@outlook.de,"Quantitative_Jacobi,",10.1080/21670811.2015.1093271,Digital Journalism,2016,2,b,"Our purpose here, however, was to compare the topics we found using LDA with
the outcome of the framing research by Gamson and Modigliani (1989). Compared to
the frames or interpretive packages found in their analysis, the topics of our LDA analysis seem to be more concrete and specific. Similar to Gamson and Modigliani’s study,
we found that topics change over time, but not all of them increase or decrease in a
linear way. Also, topics seem to either cluster a number of related events together (nuclear proliferation talks, nuclear accidents) or represent issues that are continuously present over a longer period of time (the economics of nuclear energy, nuclear weapons
tests). It is not possible, however, to deduct a particular viewpoint or frame from a
topic directly—for example, we found no clear “anti-nuclear” cluster, whereas Gamson
and Modigliani found multiple frames that are critical of nuclear energy. It is quite likely
that the coverage of nuclear accidents and danger is predominantly covered from an
anti-nuclear perspective, but even in this case there is a clear difference between the
“issue” or event being covered (nuclear accidents) and the frame with which it iscovered. That said, for some of the topics, such as the Research and Accidents topics,
we do see that the temporal pattern is similar to that identified for the Progress and
Runaway packages, respectively.
To see whether increasing the number of clusters helps find word patterns that
are more fine-grained and more frame-like than those representing issues, we will
explore what happens if we increase the number of topics from 10 to 25",replication study thus comparing it with the original results,Content Validation,No
2022-11-30T15:04:29Z,You’ve got mail: how the Trump administration used legislative communication to frame his last year in office,"Tripodi, Francesca; Ma, Yuanye","Yes, Continue with Coding",NA,Party Politics: Government Communication,English,NA,Frame,Framing,Unsupervised: Topic Modeling,STM,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"You’ve_Tripodi,",10.1080/1369118X.2021.2020873,"Information, Communication & Society",2022,1,a,"he topics appear cohesive and exclusive to each other, i.e., no overlapping or repeated
words were found across topics. Operationally, STM supports the retrieval of exemplar
documents through its embedded function ﬁndThoughts. Using this output, we analyzed
ﬁve exemplar documents from each cluster to determine the top seven topics being dis-
cussed (e.g., ‘the conﬁrmation of Judge Barrett to the Supreme Court,’ or ‘COVID-19’).",Evaluting top words per topic,Content Validation,No
2022-09-15T10:33:55Z,Ideological Scaling of Social Media Users: A Dynamic Lexicon Approach,"Temporão, Mickael; Vande Kerckhove, Corentin; van der Linden, Clifton; Dufresne, Yannick; Hendrickx, Julien M.","Yes, Continue with Coding",NA,Social Media: Twitter,Others,"English, French",Ideological Position,Ideological position,Unsupervised: Text Scaling,dynamic lexicon approach,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Ideological_Temporão,",10.1017/pan.2018.30,Political Analysis,2018,3,a,"We begin with a focus on the face validity (Dinas and Gemenis 2010) of the estimates produced by the dynamic lexicon approach, drawing on an overview of the ideological landscapes in each of our three case studies. Our objective is to develop an intuitive rendering of the ideological positioning of the major parties in each case, so that we have a frame of reference by which to compare the estimates generated by the dynamic lexicon approach
",face validity,Content Validation,Yes
2022-09-15T10:33:55Z,Ideological Scaling of Social Media Users: A Dynamic Lexicon Approach,"Temporão, Mickael; Vande Kerckhove, Corentin; van der Linden, Clifton; Dufresne, Yannick; Hendrickx, Julien M.","Yes, Continue with Coding",NA,Social Media: Twitter,Others,"English, French",Ideological Position,Ideological position,Unsupervised: Text Scaling,dynamic lexicon approach,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Ideological_Temporão,",10.1017/pan.2018.30,Political Analysis,2018,3,b,"We then test the convergent validity of the calibration process for the dynamic lexicon of political terms. Here we focus on the subset of political candidates, whose user-generated content defines the dynamic lexicon, by comparing the positions derived from this approach to an ideological scaling approach based on network information (Barberá 2015; Bond and Messing 2015)","comparing it with network analysis (so other method, but not CATM)",Unsure,Yes
2022-09-15T10:33:55Z,Ideological Scaling of Social Media Users: A Dynamic Lexicon Approach,"Temporão, Mickael; Vande Kerckhove, Corentin; van der Linden, Clifton; Dufresne, Yannick; Hendrickx, Julien M.","Yes, Continue with Coding",NA,Social Media: Twitter,Others,"English, French",Ideological Position,Ideological position,Unsupervised: Text Scaling,dynamic lexicon approach,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Ideological_Temporão,",10.1017/pan.2018.30,Political Analysis,2018,3,c,"Finally, we extend this approach to individual citizens in our sample and compare their estimated positions to a reference position derived from survey data collected from Vote Compass

""Having demonstrated that the dynamic lexicon approach can derive valid ideological dimensions from the content that political elites generate on social media, and that it can position those political elites in ideological space in ways that correlated highly with more established methods, we now examine its ability to classify the individual ideologies of the broader population of social media users. To do so, we compare the dynamic lexicon approach to a basic Wordfish approach, in which we ignore the precomputed values and instead simply apply the algorithm. Two different baseline strategies are proposed, depending on whether or not we consider the use of a lexicon of political terms to create the TDM for citizens. In the first strategy (Wordfish-all), users’ TDM is generated keeping all the N -grams (i.e., following the same building process as for elites’ TDM Z elit). The second strategy (Wordfish-political) considers the adapted TDM Z cit built from the dynamic lexicon of political terms. The reference ideological position for each users is derived from the survey data collected by Vote Compass.6 We use an expectation–maximization algorithm to solve the maximum likelihood problem that generates these estimates (Barber 2012). We consider a social media extraction to be effective if two social media users with similar ideologies according to the Vote Compass survey data are also situated in close ideological proximity to one another using the dynamic lexicon approach. The quality of estimates is then defined as the Pearson correlation coefficient (ρ) between the vector of textual estimates and the vector of reference ideologies.""",Criterion validity with survey scores,External: Criterion data / Predictive validation,No
2022-11-14T10:27:56Z,Parametrizing Brexit: mapping Twitter political space to parliamentary constituencies,"Bastos, Marco; Mercea, Dan","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,"ideological coordinates of Globalism, Economism, Nationalism, and Populism",Ideological position,Supervised: Machine Learning,newly develpoed machine learning,NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Parametrizing_Bastos,",10.1080/1369118X.2018.1433224,"Information, Communication & Society",2018,1,a,"We relied on the abovementioned set of 10,000 manually coded tweets to assign a value
(positive or negative) to each of the concepts we have sought to map, with the algorithm
calculating the probability of positiveness and negativeness for each ideological polarity
(Globalism vs. Nationalism and Economism vs. Populism). For each ideological pair,
the classifier returns a range of values from 0 (completely globalist) to 1 (completely
nationalist), so that values from 0.45 to 0.55 are somewhere in the middle of this scale
and assumed to be relatively neutral. The algorithm was trained using Document-Term
Matrix (DTM), vocabulary-based vectorization, and the TF-IDF method for text preprocessing. Figure 2 shows the area under the curve on train and test datasets for the Economism–Populism and Globalism–Nationalism ideological pairs (AUC = 0.8697 and
AUC = 0.901, respectively). The algorithm performed well for the set of 565,028 tweets
explored in this study and we expect it to perform reasonably well in other national contexts in which nativist and populist sentiments might be emerging. In the last step of the
classification, the algorithm calculates the best fit, projects the results along spatial coordinates comprising the four ideological dimensions, and estimate significant oscillations
between any of the ideological pairs.",hand coding,External: Human Annotated Scores,No
2022-11-29T10:11:14Z,Validating Wordscores: The Promises and Pitfalls of Computational Text Scaling,"Bruinsma, Bastiaan; Gemenis, Kostas","Yes, Continue with Coding",NA,Party Politics: Party Manifestos,Others,Multilingual,ideological position,Ideological position,Unsupervised: Text Scaling,Wordscores,NA,No,NA,NA,Yes,No,No,lukas.birkenmaier@outlook.de,"Validating_Bruinsma,",10.1080/19312458.2019.1594741,Communication Methods and Measures,2019,2,a,"We begin by examining the degree to which the Wordscores estimates correlate with other known measures of party positions as outlined in the study design section. Carmines and Zeller (1979) refer to this type of validity as criterion validity while other authors use the term concurrent validity. Our analysis uses the concordance correlation coefficient (CCC) defined as the product between Pearson’s product-moment correlation coefficient that measures dispersion (i.e. the degree of random measurement error) and a bias correction factor that measures the deviation from the 45º line of perfect concordance (Lin, 1989). We use CCC as both Pearson’s and Spearman’s correlation coefficients are likely to overestimate the degree of validity in case of the presence of systematic measurement error.6","predicting expert ratings (difficult to code, since it is a measure between content and predictive Validity)",External: Criterion data / Predictive validation,Yes
2022-11-29T10:11:14Z,Validating Wordscores: The Promises and Pitfalls of Computational Text Scaling,"Bruinsma, Bastiaan; Gemenis, Kostas","Yes, Continue with Coding",NA,Party Politics: Party Manifestos,Others,Multilingual,ideological position,Ideological position,Unsupervised: Text Scaling,Wordscores,NA,No,NA,NA,Yes,No,No,lukas.birkenmaier@outlook.de,"Validating_Bruinsma,",10.1080/19312458.2019.1594741,Communication Methods and Measures,2019,2,b,"In addition to criterion/concurrent validity, we also examine what Carmines & Zeller (1979) call construct validity, and others refer to as predictive validity. Construct/predictive validity refers to the extent to which a measure behaves as expected within a given theoretical context. Following, Klingemann et al. (2006,p.36–39) and McElroy and Benoit (2007) who use a similar approach to validate Manifesto Project and expert survey data respectively, we hypothesize that we can use the ideological positions of parties to predict membership in the political groups of the European Parliament (EP). To do so, we estimate a multinomial regression model where the dependent variable takes eight values, one for each of the seven party groups in the EP (as of 2009) with nonattached parties forming the eighth group. The predictors are party positions on the socio-economic, socio-cultural, and European integration dimensions.",prediction of party label,External: Criterion data / Predictive validation,No
2022-11-29T10:21:05Z,Word Embeddings for the Analysis of Ideological Placement in Parliamentary Corpora,"Rheault, Ludovic; Cochrane, Christopher","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,ideological position,Ideological position,Unsupervised: Text Scaling,shallow neural network,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Word_Rheault,",10.1017/pan.2019.26,Political Analysis,2020,6,a,"To assess the external validity of estimates derived from our models, we evaluate the predicted
ideological placement against gold standards: ideology scores based on roll-call votes (for the
US), surveys of experts, and measures based on the CMP data. For each gold standard, we report
the Pearson correlation coeicient with the first principal component of our party embeddings.
We also report the pairwise accuracy, that is, the percentage of pairs of party placements that
are consistently ordered relative to the gold standard. Pairwise accuracy accounts for all possible
comparisons, within parties and across parties. Table 2 presents the results",external,External: Criterion data / Predictive validation,Yes
2022-11-29T10:21:05Z,Word Embeddings for the Analysis of Ideological Placement in Parliamentary Corpora,"Rheault, Ludovic; Cochrane, Christopher","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,ideological position,Ideological position,Unsupervised: Text Scaling,shallow neural network,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Word_Rheault,",10.1017/pan.2019.26,Political Analysis,2020,6,b,"To further validate our methodology, we rely upon a second set of benchmarks based on
human evaluations, namely expert surveys. Such surveys have been conducted sporadically in
the discipline, asking country experts to locate national parties on a le–right scale. The average
expert position is commonly used to examine party ideology at fixed points in time (see Benoit
and Laver 2006). We retrieved expert surveys from three dierent sources covering the three
countries under study in this paper (Castles and Mair 1984; Huber and Inglehart 1995; Benoit and
Laver 2006). Two points on measurement should be emphasized. First, since expert surveys come
from dierent sources, they may vary in construction and measurement scales. As a result, we
standardize the scores before combining data points fromdierent sources. We compute z -scores
for each of the three surveys by subtracting the mean for all parties across the three countries and
then dividing by the standard deviation. Second, we should point out that expert surveys provide
us with only a few data points, in contrast to the other benchmarks reported in Table 2. In the
United States, we retrieved expert party placements from three sources, which means six data
points (i.e. Democrats and Republicans at three dierent points in time).
The third and fourth rows of Table 2 report the Pearson correlation coeicient and pairwise
accuracy of our ideological placement—again, the first principal component of the party
embeddings—evaluated against expert surveys. For both the US House and Senate, the two
goodness-of-fit scores suggest that our methodology produces ideology scores consistent with
the views of experts. The correlation coeicients are very high (r  :), and the pairwise
accuracy reaches 100% for the Senate. Despite the low number of comparison points, validating
with a dierent source helps to give further credence to the interpretation of our unsupervised
method of party placement. Experts surveys represent a more challenging test for the other
two countries, which contain multiple parties, hence additional data points. The fit with expert
surveys in Canada and Britain remains very strong, however, with correlation coeicients near or
above 0.9.",external with expert surveys,External: Criterion data / Predictive validation,Yes
2022-11-29T10:21:05Z,Word Embeddings for the Analysis of Ideological Placement in Parliamentary Corpora,"Rheault, Ludovic; Cochrane, Christopher","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,ideological position,Ideological position,Unsupervised: Text Scaling,shallow neural network,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Word_Rheault,",10.1017/pan.2019.26,Political Analysis,2020,6,c,"Finally, we validate our ideological placement variables using data from the CMP (Budge and
Laver 1992; Budge et al. 2001). The CMP is based on party manifestos and relies on human
annotations to score the orientation of political parties on a number of policy items, following a
normalized scheme. We test whether the three ideology indicators derived fromthe project’s data
are consistent with our estimated placement of the same party in the parliament that immediately
follows. The Rile measure is the original le/right measure in the CMP. It is an additive index
composed of 26 policy-related items, as described in Budge et al. (2001). The Rile metric, however,
excludes important dimensions of ideology of theCMP, such as equality and the environment. The
Vanilla measure proposed by Gabel and Huber (2000) uses all 56 items in the CMP and weights
them according to their loadings on the first unrotated dimension of a factor analysis. Finally, the
Legacy measure is a weighted index based on a network analysis of party positions and a model
that assigns exponentially decaying weights to party positions in prior elections (Cochrane 2015).
Overall, the CMP-based indicators are consistent with our ideology scores, in particular when
considering the Vanilla and Legacy measures. For the US, the correlation is positive but very
modest when considering the more basic “right minus le” measure (Rile). Accuracy reaches
r  :, however, when considering the Legacy score.16 Looking at the British case, party",external other scores,External: Criterion data / Predictive validation,Yes
2022-11-29T10:21:05Z,Word Embeddings for the Analysis of Ideological Placement in Parliamentary Corpora,"Rheault, Ludovic; Cochrane, Christopher","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,ideological position,Ideological position,Unsupervised: Text Scaling,shallow neural network,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Word_Rheault,",10.1017/pan.2019.26,Political Analysis,2020,6,d,"To illustrate, Table 1 reports the 20 expressions with the shortest Euclidean distances to the
four cardinal points of the two-dimensional space for the US House of Representatives.13 We
use the minimum and maximum values of the party projections on each axis to determine the
cardinal points. Expressions located closest to the le end of the first component comprise “civil
rights,” “racism,” “decent housing,” and “poorest,” indicating that these terms are semantically
associated with this end of the ideological spectrum. These words refer to topics one would
expect in the language of liberal (or le-wing) parties in the United States. Conversely, terms like
“bureaucracy,” “free enterprise,” and “red tape” are associated with the right. As for the seconddimension, the keywords refer to Southern and Northern locations or industries associated with
each region, which supports our interpretation of that axis as a South–North divide.
We should emphasize that these lists may contain unexpected locutions, for instance, the
“Missouri River” among the terms closest to the right edge of the first component. As argued
earlier, political ideology cannot be reduced easily to any single idea or core. Attempting to
summarize concepts such as liberal or conservative ideologies with individualwords entails losing
the context-dependent nature of semantics, which our model is designed to capture. For instance,
some words contained in the lists of Table 1 may reflect idiosyncratic speaking styles of some
Members of Congress on each side of the spectrum. Therefore, the interpretation ultimately
involves the domain knowledge of the researcher to detect an overarching pattern. In this case,we
believe the word associations provide relatively straightforward clues that facilitate a substantive
interpretation of each axis. Since there are several dierent ways to explore relations between
words and political actors in this model, an objective for future research would be to develop
robust techniques for interpretation.",evaluating top words,Content Validation,Yes
2022-11-29T10:21:05Z,Word Embeddings for the Analysis of Ideological Placement in Parliamentary Corpora,"Rheault, Ludovic; Cochrane, Christopher","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,ideological position,Ideological position,Unsupervised: Text Scaling,shallow neural network,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Word_Rheault,",10.1017/pan.2019.26,Political Analysis,2020,6,e,"We now emphasize the properties of these embeddings using a comparison with estimates from
the WordFish model.17 The proposedmethodology diers fromWordFish in at least two important
ways. Our model fits embeddings by predicting words from political variables and a window
of context words. Each word in the corpus represents a distinct unit of analysis. In contrast,
WordFish relies on word frequencies tabulated within groupings of interest. The data required to
fit the WordFish estimator can be contained in a term–document matrix (for full expositions of
this model, see Slapin and Proksch 2008; Proksch and Slapin 2010; Lowe and Benoit 2013). This
matrix representation loses the original sequence in which the terms were used in the document,
and the information loss is where the two models fundamentally dier. A second dierence is
that WordFish produces a single estimate of party placement, whereas the embeddings contain
a flexible number of dimensions, which can be projected onto lower dimensional spaces, as
illustrated above.
To illustrate these dierences,we compute WordFish estimates on the entire US House corpus,
restricting the vocabulary to the 50,000 most frequent terms.18 We combined speeches by
members of the same party into single “documents” for each Congress, following the approach
used in Proksch and Slapin (2010). Figure 4(a) plots the estimated party positions for the full time
period, which can be contrasted against our placement in Figure 2(b). When fitting the WordFish
model on the full corpus, the estimator fails to capture the ideological distinctiveness of the
parties. In fact, the dominant dimension appears to be the distinction between the language used
in the late 19th century and that of more recent Congresses. Put another way, the estimates for
both parties appear confounded by the change in language. Both parties are located next to each",comparison with WordFish,External: Scores from other CATM,Yes
2022-11-29T10:21:05Z,Word Embeddings for the Analysis of Ideological Placement in Parliamentary Corpora,"Rheault, Ludovic; Cochrane, Christopher","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,ideological position,Ideological position,Unsupervised: Text Scaling,shallow neural network,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Word_Rheault,",10.1017/pan.2019.26,Political Analysis,2020,6,f,"To assess whether the low-dimensional projection captures political ideology, we proceed
once more with a validation against external measures. By focusing on a recent Congress, we are
able to use a variety of measures of ideological placement for US Senators: the Voteview scores
for individual Senators, ratings from the American Conservative Union (ACU) and GovTrack.us
ideology scores.21 Table 4 reports Pearson correlation coeicients and pairwise accuracy
scores using the first principal component of our model and each gold standard. Since our
corpus is restricted to members ailiated with major party labels, this analysis excludes two
independent Senators. For each gold standard considered,we obtain correlation coeicients of at
least 0.85. The fit with the GovTrack ideology scores is particularly strong. Overall, these accuracy
results support the conclusion that the first principal component extracted from our “Senator
embeddings” is related to political ideology",validation against external measure,External: Criterion data / Predictive validation,No
2022-11-29T16:07:06Z,Measuring Political Positions from Legislative Speech,"Lauderdale, Benjamin E.; Herzog, Alexander","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,ideology,Ideological position,Unsupervised: Text Scaling,wordshoal,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Lauderdale,",10.1093/pan/mpw017,Political Analysis,2016,4,a,"The top row in Figure. 1 shows mean party positions estimated from our approach against these
two benchmarks. Based on these results, it appears that in the Irish data our approach is primarily
recovering government versus opposition conflict, rather than left–right ideology. There are two
ways to see this. First, while the largest parties FF and FG are generally viewed to be ideologically moderate in left–right terms, we estimate them at or near the extremes of our dimensions. Note, in
particular, the fact that the Labour Party is estimated to be more centrist than FG, which only
makes sense if we think of this as government–opposition. Second, when the Green Party joins the
coalition in the 30th Da´ il, it moves from having a similar average position to FG to having nearly
the same position as FF.
In contrast, Wordfish estimates do not seem to consistently reflect the coalition structure of the
Da´ il, as is evident from the two scatterplots in the middle row in Figure. 1. The Green Party has a
similar estimated position to FF, both when they are in coalition and when they are not. The
Progressive Democrats are at one extreme of the dimension in the 29th Da´ il and the other in the
30th, despite no change in coalition status. Neither do these estimates seem to reflect the ideological
cleavages of the Da´ il as assessed by expert surveys. In particular, experts do not place the Labour",based on expert knowledge,Content Validation,Yes
2022-11-29T16:07:06Z,Measuring Political Positions from Legislative Speech,"Lauderdale, Benjamin E.; Herzog, Alexander","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,ideology,Ideological position,Unsupervised: Text Scaling,wordshoal,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Lauderdale,",10.1093/pan/mpw017,Political Analysis,2016,4,b,"When we look at the 1D estimates for individual TDs, rather than the party means, we can see the
association between our estimates and coalition status even more clearly. Figure 2 shows the relationship
between the estimated legislator positions and the coalitions under both Wordfish and our estimates in the 30th Da´ il (the very similar plots for the 29th are included in the Supplementary
Appendix). In the 30th Da´ il, the (Pearson) correlation between being in the coalition government
and Wordshoal score is 0.94, versus a correlation of 0.31 with Wordfish.",predicting party,External: Criterion data / Predictive validation,Yes
2022-11-29T16:07:06Z,Measuring Political Positions from Legislative Speech,"Lauderdale, Benjamin E.; Herzog, Alexander","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,ideology,Ideological position,Unsupervised: Text Scaling,wordshoal,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Lauderdale,",10.1093/pan/mpw017,Political Analysis,2016,4,c,"Figure 3 shows the average Wordshoal positions for cabinet ministers, junior ministers, government
backbench TDs, and opposition members.8 Consistent with the expectation of collective
responsibility, we find that cabinet members are the most pro-government speakers. In the 29th
Da´ il, the average cabinet minister position is 1.52, versus the average position of backbench TDs at
0.98. In the 30th Da´ il, the difference is slightly smaller, with positions at 1.23 and 0.95, respectively.
The posterior probabilities of these differences having these signs are both greater than 0.99. The
average position of junior ministers is slightly, but less significantly, more moderate than the
average minister position, indicating that junior ministers speak similarly to cabinet ministers,
either because of collective responsibility, career concerns, or some other factor",evaluation of specific positions within parliament,Content Validation,Yes
2022-11-29T16:07:06Z,Measuring Political Positions from Legislative Speech,"Lauderdale, Benjamin E.; Herzog, Alexander","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,ideology,Ideological position,Unsupervised: Text Scaling,wordshoal,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Lauderdale,",10.1093/pan/mpw017,Political Analysis,2016,4,d,"How do our measures of U.S. Senators’ relative positions compare to other scales of the same
legislators based on different kinds of data? For roll-call votes, we fit a standard Bayesian Item
Response Theory (IRT) model to all votes over the same period as our speeches.11 For campaign
donations, we use career CFscores (Bonica 2014) for each Senator.12
In essence, a comparison of these three measures is a comparison of roll-call behavior, speech
behavior, and donor behavior, each modeled in terms of a single spatial dimension. If we calculate
correlations between within-party variation in these three scores, we find that roll-call scores are
correlated with speech scores at 	 ¼ 0:46, while donor scores are correlated with roll-call scores at
	 ¼ 0:57 and with speech scores at 	 ¼ 0:55.13
One could make the argument that there is only a single meaningful latent political dimension,
and that the three measures differ only because of measurement uncertainty. The relatively high
correlations with donor behavior might simply indicate less measurement error in those scores than
the other two.14 If this was the case, it would strongly indicate the value of having all three of these
measures: forming a joint scale based on all three that would then improve measurement of this
“one true” latent dimension. In fact, though we think this is a relatively implausible account for the
differences between these measures, speech, roll call, and donor behavior are distinct political
behaviors subject to distinct political forces, which becomes clearer when we examine the ways
in which they differ.",predicting roll-call votes,External: Criterion data / Predictive validation,No
2022-11-29T16:23:01Z,Positioning under Alternative Electoral Systems: Evidence from Japanese Candidate Election Manifestos,"Catalinac, Amy","Yes, Continue with Coding",NA,Party Politics: Party Manifestos,Others,Japanese,ideological position,Ideological position,Unsupervised: Text Scaling,Wordfish,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Positioning_Catalinac,",10.1017/S0003055417000399,American Political Science Review,2018,3,a,"First, we examined the position of the average
candidate of each party. Figure 1 plots these averages,
sorted from most ideologically left (smallest), to most
ideologically right (largest), with the lines around the
dots representing 95% confidence intervals. Several
aspects of this figure suggest that the model recovered
substantively meaningful positions. The average
JCP candidate is located to the left of all other parties
in all elections. The average LDP candidate is located
to the right of the average socialist candidate
in all elections, where socialist candidates ran from
the JSP in 1986, 1990, and 1993, from the SDP and
New Socialist Party (NSP) in 1996, and from the SDP
thereafter. The average LDP candidate is located to
the right of the average DPJ candidate in all elections
in which both parties ran. The confidence intervals
around the average LDP and NLC position overlap
in 1986 and a difference-in-means test confirmed
that their means were indistinguishable. These parties
merged a month after the election. Strikingly, the ordering
of parties in 2003 exactly matches their ordering
in a 2003 survey in which experts were asked to locate
them on a left-right dimension (Laver and Benoit
2005).
Second,",examaning position,Content Validation,Yes
2022-11-29T16:23:01Z,Positioning under Alternative Electoral Systems: Evidence from Japanese Candidate Election Manifestos,"Catalinac, Amy","Yes, Continue with Coding",NA,Party Politics: Party Manifestos,Others,Japanese,ideological position,Ideological position,Unsupervised: Text Scaling,Wordfish,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Positioning_Catalinac,",10.1017/S0003055417000399,American Political Science Review,2018,3,b,"Second, we examined the correlation between our
estimates and those obtained from other data. The
University of Tokyo and one of Japan’s largest newspapers,
the Asahi Shimbun, have conducted several
waves of theAsahi-Todai Elite Survey, which asks candidates
contesting HOR elections their positions on a
battery of policy issues. In the 2003 and 2005 HOR
elections, candidates were asked to locate themselves
on a 10-point ideological scale, in which they were told
1 represented the most “progressive” (left) position
and 10 represented the most “conservative” (right).
These surveys boast high response rates: 95% in 2003
(1,104 of 1,159 candidates) and 91% in 2005 (1,034 of
1,131 candidates). In both elections, the correlation between
our estimates and these self-reported positions
was positive and highly significant. In 2003, Pearson’s
r = .81 (n = 904), while in 2005, Pearson’s r = .80
(n = 853).7",comparison against survey questions,External: Criterion data / Predictive validation,Yes
2022-11-29T16:23:01Z,Positioning under Alternative Electoral Systems: Evidence from Japanese Candidate Election Manifestos,"Catalinac, Amy","Yes, Continue with Coding",NA,Party Politics: Party Manifestos,Others,Japanese,ideological position,Ideological position,Unsupervised: Text Scaling,Wordfish,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Positioning_Catalinac,",10.1017/S0003055417000399,American Political Science Review,2018,3,c,"Third, we read manifestos located at the left and
right to evaluate whether the meaning of the dimension
coheres with work on the nature of ideological
competition in Japan. This has drawn two main conclusions.
First, Curtis (1999, 29-30) describes how “violent
ideological conflict” between conservatives committed
to overturning the postwar order and progressives committed to furthering democratization gave way to
a less-polarized system in the 1970s in which conservatives
became “committed to a policy of making
cautious, incremental adjustments in the status quo”
and progressives remained committed to reorganizing
Japan’s political and economic system but had abandoned
their most-extreme positions. Second, Proksch,
Slapin, and Thies (2011) and Laver and Benoit (2005)
show that conservatives and progressives differ little
on economics, but substantially on social, foreign, and
environmental policy. Consistent with both, we found
that the dimension was support versus opposition to
the establishment, and the issues distinguishing the
ends tended to concern social and foreign policy, not
economics.
Manifestos on the right spoke of traditional values,
the hometown, the family unit, established career
paths,mainstream life choices, the agencies of the state,
law and order, and the status quo in foreign policy. For
example, candidates extolled the virtues of working
together as a group and lamented the disappearance
of warm, local communities were everyone had a role
to play and respected others. They promised to pursue
freedom in moderation, restore Japan’s spiritual
backbone, realize a proud society built on deep bonds
between people, encourage young people to remain in
the hometown, encourage the buying and selling of locally
made products, increase the health of youngsters
through physical sports such as baseball and soccer, and
further strengthen the U.S.-Japan alliance. In contrast,",examaning top left- and right manifestos,Content Validation,No
2022-11-30T17:36:10Z,Economic decline and extreme-right electoral threat: How district-level factors shape the legislative debate on immigration,"Tzelgov, Eitan; Olander, Petrus","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,Others,Swedish,position on immigration,Ideological position,Unsupervised: Text Scaling,Wordfish,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Economic_Tzelgov,",NA,Legislative Studies Quarterly,2018,2,a,"First, in Figure 1, we present the distribution of me-
dian MP positions by party. two things emerge from the figure. 
First, MPs across all parties—with the exception of the sweden 
democrats—express positions across the entire range of the 
scale.8 this echoes Widfeldt’s (2015) argument that one would 
be hard-pressed to find differences between parties on immigra-
tion. second, the median position for the sweden democrats (sd 
in the figure) is the most cultural difference oriented, and the 
distribution is truncated from below, indicating that these MPs 
rarely engage in socioeconomic responsibility oriented rhetoric. 
this reflects its emphasis of swedish nationalism and nativism 
(Hellstrom, Nilsson, and stoltz 2012).",face validity,Content Validation,Yes
2022-11-30T17:36:10Z,Economic decline and extreme-right electoral threat: How district-level factors shape the legislative debate on immigration,"Tzelgov, Eitan; Olander, Petrus","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,Others,Swedish,position on immigration,Ideological position,Unsupervised: Text Scaling,Wordfish,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Economic_Tzelgov,",NA,Legislative Studies Quarterly,2018,2,b,"We also validate the scale by comparing it to estimates from 
a structural topic model (stM) (roberts, stewart, and tingley 
2018). the stM offers a framework to estimate the topic of each 
document (speech). Within this framework, each topic is con-
ceived to be a mixture over words, and each word has a prob-
ability of belonging to a topic. Below, we present the results of 
a 10-topic model, which was selected based on its log-likelihood 
(griffiths and steyvers 2004) and perplexity (Cao et al. 2009).table 2 presents the characteristic words of the topics.10 the 
table reveals that some topics identified by the stM model are 
closely related to the poles estimated by the wordfish model. 
thus, for example, words such as hate crime, racism, and reli-
gion appear as having high discriminatory value according to the 
wordfish model and as being indicative of a topic we label identity. 
similarly, words such as unaccompanied youth, municipality, and 
reception are informative according to the wordfish model and 
are indicative of an stM topic we have labeled reception. Further 
examination reveals a number of stM topics characterized by 
words that the wordfish model has also identified as informative.
given that the stM has identified similar sets of words as 
the wordfish model, we move to compare the topics. specifically, 
we examine the degree to which the wordfish-scale estimates 
correspond to the relevant topic probabilities from the stM al-
gorithm. We should expect strong positive correlations between 
our scale and the stM identity and security topics, and negative 
correlations with the reception and economy topics. in Figure 2, we present the relevant correlations, which support our expecta-
tions. six stM topics have substantively significant correlations 
with the wordfish scale. as expected, identity and security topics 
are aligned with the wordfish scale. We also note that the legality 
topic (dealing with the possible deportation of migrants) is highly 
correlated with the scale. in addition, there are significant nega-
tive correlations between the wordfish scale and socioeconomic 
responsibility aspects captured in stM model in topics such as 
reception, economy, and education.",comparison with STM scores,External: Scores from other CATM,No
2022-11-11T10:31:46Z,Reverting trajectories? UKIP’s organisational and discursive change after the Brexit referendum,"Klein, Ofra; Pirro, Andrea L. P.","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,issue salience,Issue Salience,Rule-based: Development dictionary,na,NA,Yes,extremly short validation paragraph,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Reverting_Klein,",10.1080/1369118X.2020.1792532,"Information, Communication & Society",2021,1,a,"These dictionaries were refined by comparing the outcomes of the dictionary classification with those by a human coder. Keywords were added to or removed from the dictionaries on the basis of potential inconsistencies. This process was repeated several times to improve the precision (i.e. how often a tweet that is identified by the dictionary is also about the topic of the dictionary) and recall (i.e. how often the dictionary fails to detect relevant tweets) of the dictionaries. Table 3 outlines the precision, recall, and F1 measures for the dictionaries used in this study. These measures are based on a manual coding of 300 randomly selected tweets (see also Sheafer et al., 2014).",manual coding of a subsample (even though not sure if they used the validation or test set),External: Human Annotated Scores,No
2022-11-14T10:46:55Z,Social Media and Political Agenda Setting,"Gilardi, Fabrizio; Gessler, Theresa; Kubli, Maël; Müller, Stefan","Yes, Continue with Coding",NA,"Newspaper, Social Media: Twitter, Party Politics: Press Releases",Others,"Swiss, French",issue salience (Attention to Political Issues),Issue Salience,Supervised: Machine Learning,na,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Social_Gilardi,",10.1080/10584609.2021.1910390,Political Communication,2022,2,a,"The classifiers are built around an ensemble procedure which uses different algorithms to
classify the texts (Géron, 2019). The feature engineering is based on a Word to Vector
(Word2Vec) approach. Word2Vec is a neural network trained to reconstruct linguistic
context of words using a vector space built form a large corpus of words. We base the vector
space on our text corpus which enables us to reconstruct the linguistic context of words
within the political domain. The ensemble classifiers for tweets and longer texts both
perform reasonably well over all topics of interest. The main difference between the two
classifier systems, besides the different training data, is that the social media classifier
classifies texts into ten different topics, while the second classifier considers a slightly
wider range of topics (see also Table A1). The ensemble method results in out-of-sample
accuracy of at least 80% for all classes in German and French. Tables A2 and A39 provide
detailed information on the classifier performance",hand-coding,External: Human Annotated Scores,Yes
2022-11-14T10:46:55Z,Social Media and Political Agenda Setting,"Gilardi, Fabrizio; Gessler, Theresa; Kubli, Maël; Müller, Stefan","Yes, Continue with Coding",NA,"Newspaper, Social Media: Twitter, Party Politics: Press Releases",Others,"Swiss, French",issue salience (Attention to Political Issues),Issue Salience,Supervised: Machine Learning,na,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Social_Gilardi,",10.1080/10584609.2021.1910390,Political Communication,2022,2,b,"The classifiers are built around an ensemble procedure which uses different algorithms to
classify the texts (Géron, 2019). The feature engineering is based on a Word to Vector
(Word2Vec) approach. Word2Vec is a neural network trained to reconstruct linguistic
context of words using a vector space built form a large corpus of words. We base the vector
space on our text corpus which enables us to reconstruct the linguistic context of words
within the political domain. The ensemble classifiers for tweets and longer texts both
perform reasonably well over all topics of interest. The main difference between the two
classifier systems, besides the different training data, is that the social media classifier
classifies texts into ten different topics, while the second classifier considers a slightly
wider range of topics (see also Table A1). The ensemble method results in out-of-sample
accuracy of at least 80% for all classes in German and French. Tables A2 and A39 provide
detailed information on the classifier performance",error analysis (discussing different Performances of classifiers on subgroups),Content Validation,No
2022-11-10T21:57:39Z,The Mobilizing Effect of Parties' Moral Rhetoric,"Jung, Jae‐Hee","Yes, Continue with Coding",NA,Party Politics: Party Manifestos,English,NA,moral rhetoric,Morality/Norms,Rule-based: Adjusted dictionary,Moral Foundations Dictionary,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"The_Jung,",10.1111/ajps.12476,American Journal of Political Science,2020,5,a,"First, I have checked that Moral rhetoric is consistent with conventional wisdom on the moral foundations of leftist voters and rightist voters. One of the most notable findings in the literature on the moral foundations theory is that the left and the right have different moral worlds (Graham, Haidt and Nosek 2009). That is, leftists tend to prioritize the care/harm and fairness/cheating foundations (i.e., individualizing foundations) over the authority/subversion, loyalty/betrayal, and sanctity/degradation foundations (i.e., binding foundations). Rightists, on the other hand, have a more even emphasis on individualizing and binding foundations. If so, my measure should pick up this pattern since parties are likely to use language that aligns with the moral worlds of their base. Therefore, I classified the ideology of the parties in my data using the ‘rile’ score from the Manifesto Project (Volkens et al. 2016). I coded parties with a score smaller than or equal to the mean value in the data as left-wing and parties with a score larger than the mean value as right-wing. Then, I checked whether left-wing parties have a higher percentage of quasi-sentences appealing to the care/harm and fairness/cheating foundations than the other three foundations. I find that left-wing parties indeed have higher proportions of quasisentences appealing to the individualizing foundations (mean difference = 0.02, p = 0.00 in a paired one-tailed t-test). I also checked whether right-wing parties have an even usage of individualizing and binding foundations. Results show that right-wing parties have no difference in the proportion of quasi-sentences appealing to the care/harm and fairness/cheating foundations and the proportion of quasi-sentences appealing to the loyalty/betrayal, authority/subversion, and sanctity/degradation foundations (mean difference = −0.01, p = 0.42 in a paired two-tailed t-test). The results are plotted in Figure SI3.1. These patterns remain the same when I code parties with a rile score of ‘0’ or below as left-wing and the rest as right-wing. I have also checked whether (1) left-wing parties appeal to individualizing foundations more than right-wing parties do and (2) right-wing parties appeal to binding foundations more than left-wing parties do. These are the expectations one might have based on Lipsitz (2018)’s finding that (1) liberal candidates (in the U.S. context) are more likely to emphasize the care and fairness foundations in their ads than conservative candidates do and (2) conservative candidates are more likely to emphasize the authority, loyalty, and sanctity foundations than liberal candidates do. In my data, I find that left-wing parties’ emphasis on individualizing foundations is one-percentage point higher than right-wing parties’ emphasis on individualizing foundations and that the difference is not statistically significant (p = 0.27 in a one-tailed t-test). On the other hand, I find that right-wing parties’ emphasis on bindings foundations is three-percentage points higher than left-wing parties’ emphasis on binding foundations and that the difference is statistically significant (p = 0.06 in a one-tailed t-test). The results are plotted in Figure SI3.2. Although the results are partially supportive of the expectations, I do not believe that I need to take great stock of the results because the difference in individualizing (binding) appeals between left-wing parties and right-wing parties does not necessarily have to play out in political texts. For example, suppose that a left-wing party A devotes 20% of its manifesto to moral appeal while a right-wing party",examaning differences between groups based on knowledge about the data,Content Validation,Yes
2022-11-10T21:57:39Z,The Mobilizing Effect of Parties' Moral Rhetoric,"Jung, Jae‐Hee","Yes, Continue with Coding",NA,Party Politics: Party Manifestos,English,NA,moral rhetoric,Morality/Norms,Rule-based: Adjusted dictionary,Moral Foundations Dictionary,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"The_Jung,",10.1111/ajps.12476,American Journal of Political Science,2020,5,b,"Second, I have assessed both convergent and discriminant construct validity by looking at the relationship of my measure with a measure that is likely to be positively correlated but distinct: sociocultural issue emphasis. If my measure captures moral rhetoric well, it should have a positive correlation with the extent to which a party emphasizes social issues. Although moral rhetoric can emerge in a wide range of policy areas, intuition suggests that social issues like culture and family policy are more conducive to moral rhetoric than economic issues like trade and jobs because of the nature of the issue area. Hence, there should be a positive correlation between moral rhetoric and sociocultural issue emphasis. However, moral rhetoric is a different concept from sociocultural issue emphasis, so the positive correlation should not be high if my measure is measuring what it is supposed to measure. To test this, I calculated the proportion of sociocultural quasi-sentences for all the manifestos in my data. To identify sociocultural quasi-sentences, I followed the categorization developed in Tavits and Potter (2015). The correlation I find between Moral rhetoric and sociocultural issue emphasis is 0.21. The correlation is significant in a one-tailed test (p = 0.03). This provides evidence in support of the construct validity of Moral rhetoric",correlation with sociocultural issue emphasis as an unrelated concept...but it is unclear to me how the author tests sociocultural issue emphasis!,Unsure,Yes
2022-11-10T21:57:39Z,The Mobilizing Effect of Parties' Moral Rhetoric,"Jung, Jae‐Hee","Yes, Continue with Coding",NA,Party Politics: Party Manifestos,English,NA,moral rhetoric,Morality/Norms,Rule-based: Adjusted dictionary,Moral Foundations Dictionary,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"The_Jung,",10.1111/ajps.12476,American Journal of Political Science,2020,5,c,"Third, I have checked whether moral rhetoric is used more in the sociocultural dimension than the economic dimension. This is again based on the intuition that moral rhetoric is more easily used in sociocultural issues than economic issues. I have classified the quasi-sentences into the two dimensions using the classification developed in Tavits and Potter (2015). I find that parties do have a higher proportion of moral quasi-sentences among quasi-sentences in the sociocultural dimension than among quasi-sentences in the economic dimension. The mean proportion in the sociocultural dimension is 0.43; the mean proportion in the economic dimension is 0.31 (mean difference = 0.12, p = 0.00 in a paired one-tailed t-test)",Examinatino of differneces between groups,Content Validation,Yes
2022-11-10T21:57:39Z,The Mobilizing Effect of Parties' Moral Rhetoric,"Jung, Jae‐Hee","Yes, Continue with Coding",NA,Party Politics: Party Manifestos,English,NA,moral rhetoric,Morality/Norms,Rule-based: Adjusted dictionary,Moral Foundations Dictionary,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"The_Jung,",10.1111/ajps.12476,American Journal of Political Science,2020,5,d,"Fourth, I find that my measure of moral rhetoric does not have a systematic correlation with ideology, indicating that moral rhetoric is a distinct concept rather than a proxy for ideological extremism. The correlation between Moral rhetoric and the ‘rile’ score is 0.10 and not statistically significant (p = 0.38). Moreover, Figure SI3.3 shows that there is no Ushaped relationship. I also find that Moral rhetoric is not systematically different between mainstream parties and niche parties. The mean for mainstream parties is 0.29, and the mean for niche parties is 0.30 (p = 0.79 in a two-tailed t-test).",examination with left-right scale (rile-score),External: Criterion data / Predictive validation,Yes
2022-11-10T21:57:39Z,The Mobilizing Effect of Parties' Moral Rhetoric,"Jung, Jae‐Hee","Yes, Continue with Coding",NA,Party Politics: Party Manifestos,English,NA,moral rhetoric,Morality/Norms,Rule-based: Adjusted dictionary,Moral Foundations Dictionary,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"The_Jung,",10.1111/ajps.12476,American Journal of Political Science,2020,5,e,"Lastly, I find some evidence that the level of moral rhetoric reflected in my manifestobased measure is played out in campaign speech data. With the dictionary I used to create Moral rhetoric, I created a speech-based measure of moral rhetoric at the sentence level for ten parties that I could find data on: the two major parties in the 2004, 2007, 2010, and 2013 Australian elections3 and the two major parties in the 2011 New Zealand general election.The correlation between Moral rhetoric and the speech-based measure is 0.82 (p = 0.00).",comparison with scores from the same measure for both manifesto data as well as campaign speech data (so kind of an robustness check),Unsure,No
2022-11-18T12:26:12Z,"Toward an Aggregate, Implicit, and Dynamic Model of Norm Formation: Capturing Large-Scale Media Representations of Dynamic Descriptive Norms Through Automated and Crowdsourced Content Analysis","Liu, Jiaying; Siegel, Leeann; Gibson, Laura A; Kim, Yoonsang; Binns, Steven; Emery, Sherry; Hornik, Robert C","Yes, Continue with Coding",NA,"Newspaper, Social Media: Twitter, Social Media: Youtube",English,NA,social norms,Morality/Norms,Supervised: Machine Learning,SML,NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Toward_Liu,",10.1093/joc/jqz033,Journal of Communication,2019,1,a,"we assessed the validity of our classifiers by looking at the testset correlations. Additionally, we looked at the classifiers’ precision and recall in the
training and test sets (Table 2). Precision refers to the proportion of cases hand coded
as fitting in a norms category, out of all items coded by the classifier as fitting in this
norms category; a high precision would suggest that a classifier is highly accurate
in finding relevant items, which is an indication of exactness. Recall, on the other
hand, refers to the proportion of cases the classifier coded as fitting in a norms
category, out of all hand coded, relevant items; thus, a high recall would suggest that a
classifier returned most of the relevant results, which is an indication of a high degree
of completeness in retrieval. We applied the final classifiers to the whole long-form
corpus to calculate the overall prevalence and trends for each norms category",hand-coding (MTURK),External: Human Annotated Scores,No
2022-11-10T20:57:55Z,Twitter made me do it! Twitter's tonal platform incentive and its effect on online campaigning,"Mueller, Samuel David; Saeltzer, Marius","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,(negative) tone,Negativity,"Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary",Lexicoder,NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Twitter_Mueller,",10.1080/1369118X.2020.1850841,"Information, Communication & Society",2020,2,a,"First, we test the accuracy of the dictionary on coded data, second, we use machine learning to classify the rest of the tweets. To validate our sentiment dictionary measurement, we had two coders code sentiment of a sample of 2000 tweets. Tweets were coded following a code book developed by Mohammad (2016), separating them in five categories: positive, negative, neutral, sarcastic, and ambivalent. We then compared the sentiment score computed by the dictionary, coded down to the categorical level (negative tone < 0, positive tone > 0) to all tweets in the first three categories. We achieved a classification accuracy of 0.7 (see appendix for details).",comparing hand coding,External: Human Annotated Scores,Yes
2022-11-10T20:57:55Z,Twitter made me do it! Twitter's tonal platform incentive and its effect on online campaigning,"Mueller, Samuel David; Saeltzer, Marius","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,(negative) tone,Negativity,"Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary",Lexicoder,NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Twitter_Mueller,",10.1080/1369118X.2020.1850841,"Information, Communication & Society",2020,2,b,"To extend the validation from coding to all observations, we used a Naive Bayes classifier from the Quanteda package to classify tweets into three categories: neutral, negative, and positive. After training the classifier on the 2000 coded tweets, we used it to classify the remaining 95,909 tweets. Since the measurement is now categorical and not continuous from −1 to 1, coefficients are not directly comparable. To produce comparable output, we created a categorical measurement based on the sentiment analysis. Using the new categorical measurements of tone, we performed an identical analysis to our main analysis to see whether our previous results are supported by (1) the categorical measurement based on the sentiment analysis and (2) by the completely unrelated measurement produced by the Naive Bayes classifier.7 The results are summarized in Table 2. Most findings from the main analysis are supported by the analysis that uses a categorical measurement of sentiment for both measurements, based on a sentiment analysis",running the same analysis using a different measurement method,Unsure,No
2022-11-28T10:22:37Z,More than Bags of Words: Sentiment Analysis with Word Embeddings,"Rudkowsky, Elena; Haselmayer, Martin; Wastian, Matthias; Jenny, Marcelo; Emrich, Štefan; Sedlmair, Michael","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,Others,German,negativity,Negativity,Supervised: Machine Learning,Supervised sentiment analysis approach with distributed word embeddings,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"More_Rudkowsky,",10.1080/19312458.2018.1455817,Communication Methods and Measures,2018,2,a,"First, we measure the accuracy of our model on
previously unseen training examples and compare its accuracy with a bag-of-words approach.",hand-coding,External: Human Annotated Scores,Yes
2022-11-28T10:22:37Z,More than Bags of Words: Sentiment Analysis with Word Embeddings,"Rudkowsky, Elena; Haselmayer, Martin; Wastian, Matthias; Jenny, Marcelo; Emrich, Štefan; Sedlmair, Michael","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,Others,German,negativity,Negativity,Supervised: Machine Learning,Supervised sentiment analysis approach with distributed word embeddings,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"More_Rudkowsky,",10.1080/19312458.2018.1455817,Communication Methods and Measures,2018,2,b,"Second, we test its external validity with several hypotheses on expected patterns of negativity in
parliamentary speeches in the Austrian parliament",external validity (testing hypothesis),Content Validation,No
2022-11-24T13:51:46Z,Multi-Label Prediction for Political Text-as-Data,"Erlich, Aaron; Dantas, Stefano G.; Bagozzi, Benjamin E.; Berliner, Daniel; Palmer-Rubin, Brian","Yes, Continue with Coding",NA,Party Politics: Access to Information Requests,Others,Spanish,"Multilabel predictions (use of legalistic and technical language, the number of distinct pieces of information requested, and the appropriateness of the request for the targeted agency)",Others,Supervised: Machine Learning,n.a.,NA,Yes,only smaller case studies,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Multi-Label_Erlich,",10.1017/pan.2021.15,Political Analysis,2021,1,a,"With this sample of 4,925 human coded requests,we then trained all aforementioned classifiers
on this sample, classifying all 1,021,028 remaining (non-human coded) ATI requests for each
of our 26 labels. All relevant text processing steps that were applied to the raw texts prior to
classification—and the hyperparameters used for each model—are described in the Supplementary
Material. The overall level of correlation for our 26 labels is not high, suggesting that this
application is a “hard test” for the potential benefits of multi-label classification. On average, the
correlation between our pairs of hand-coded labels is 0.06 with the lowest and highest pairwise
correlations being 0 and 0.40, respectively.
We next evaluate the value added of the multi-label framework (i.e., of considering inter-label
relations for our document-level labels). In order to do so, we compare the results obtained
from four plausible approaches to handling multi-label data. The first pair of general approaches
that we consider are BR and ML-kNN. Recall that neither of these two approaches consider label
relations. By contrast, the second pair of generalmulti-label approaches thatwe consider (CC and
LP) do consider label correlations.",hand-coding,External: Human Annotated Scores,No
2022-11-08T11:37:24Z,Measuring Congressional Partisanship and Its Consequences,"Gelman, Jeremy; Wilson, Steven Lloyd","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,political partisanship,Partisanship,Supervised: Machine Learning,convolutional neural network,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Gelman,",10.1111/lsq.12331,Legislative Studies Quarterly,2022,5,a,"in order to evaluate whether the neural networks were effective, we took a random sample of 2,000 hand-coded tweets and used them as testing data. We trained the algorithm on the other 38,000 hand-coded posts and had it predict whether tweets in the test set were partisan or not. the neural network performs well, accurately classifying 92% of the out-of-sample test data.21",Testing on hand-coded sample,External: Human Annotated Scores,Yes
2022-11-08T11:37:24Z,Measuring Congressional Partisanship and Its Consequences,"Gelman, Jeremy; Wilson, Steven Lloyd","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,political partisanship,Partisanship,Supervised: Machine Learning,convolutional neural network,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Gelman,",10.1111/lsq.12331,Legislative Studies Quarterly,2022,5,b,"First, we expect party leaders to rate as strong partisans. they are tasked with protecting the party’s reputation (Cox and McCubbins 2005), orchestrating its messaging campaigns (Lee 2016), and regularly use partisan rhetoric on twitter (russell 2018). We ran a one-tailed t- test to determine if leaderships’ scores (0.39) are significantly higher than the rank and file (0.22).22. as we expect, the t-test shows party leaders are significantly more partisan than the rank and file (p < 0.0001). We find similar results when we expand the definition of leadership to include committee chairs and ranking members.",Identifying single MPS,Content Validation,Yes
2022-11-08T11:37:24Z,Measuring Congressional Partisanship and Its Consequences,"Gelman, Jeremy; Wilson, Steven Lloyd","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,political partisanship,Partisanship,Supervised: Machine Learning,convolutional neural network,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Gelman,",10.1111/lsq.12331,Legislative Studies Quarterly,2022,5,c,"second, we use our training set to predict the partisan intensity of nine out-of-sample twitter accounts: President trump, the House and senate republicans, the House and senate democrats, and each congressional party’s campaign account (the NrCC, NrsC, dCCC, and dsCC). President trump regularly invoked partisan language on twitter, praising republicans and disparaging democrats. the party accounts are used to advance partisan online messaging campaigns. as such, we expect our model to rate each account as highly partisan. table 4 lists these scores and their within-party percentile. Not surprisingly, these accounts, especially the parties’ electioneering arms, emphasize partisan rhetoric much more often than the average member of Congress. the parties themselves, and President trump, also display high levels of partisan intensity and consistently rate as posting the highest proportion of partisan messages within their parties.",predicting out of sample accounts,Content Validation,Yes
2022-11-08T11:37:24Z,Measuring Congressional Partisanship and Its Consequences,"Gelman, Jeremy; Wilson, Steven Lloyd","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,political partisanship,Partisanship,Supervised: Machine Learning,convolutional neural network,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Gelman,",10.1111/lsq.12331,Legislative Studies Quarterly,2022,5,d,"as a third validation check, we assess whether our measure captures the partisan intensity surrounding some particularly bitter disputes between democrats and republicans. When events largely viewed as partisan arise, we should observe an increase in partisanship. to assess this, we use the eight partisan events from table 1 as moments we expect to see spikes in interparty fighting and calculated aggregate congressional partisanship scores during the week those events occurred. For ones that spanned more than a week, we calculated the score during the moments we expect partisan fighting to peak.23. We conducted a one-tailed test to assess whether legislators displayed more partisan intensity during these high-profile fights. as expected, the average partisanship score the week after these events occurred was 0.29. during the rest of these two congresses, it was significantly lower at 0.23 (p < 0.0001).",comparing with external events,External: Criterion data / Predictive validation,Yes
2022-11-08T11:37:24Z,Measuring Congressional Partisanship and Its Consequences,"Gelman, Jeremy; Wilson, Steven Lloyd","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,political partisanship,Partisanship,Supervised: Machine Learning,convolutional neural network,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Gelman,",10.1111/lsq.12331,Legislative Studies Quarterly,2022,5,e,"Finally, we assess how similar our measure is to political ideology. One possibility is our approach simply captures the same liberal-conservative dimension others have previously calculated. However, our results show that partisanship and ideology are different. to measure their correlation, we calculated the absolute value of each member’s first-dimension dW-NOMiNate score, which reflects a legislator’s ideological extremity. in doing so, both the ideology and partisan measures are scaled between 0 and 1. table 5 includes their bivariate correlations by party, chamber, and congress. unlike other measures of ideology, whose correlation with dW-NOMiNate hovers around 0.9 (Bonica 2014), we are capturing a different dimension of political conflict. Put differently, members’ partisan intensity, measured using their twitter posts, is weakly related to their ideological voting patterns. to better display these variables’ relationship, or lack thereof, we plot them against each other, by party, in Figure 3. if moderates were always weak partisans, we would see clusters of dots near the graphs’ origins. those clusters do not exist. rather, moderates vary in their partisanship. although many employ this rhetoric sparingly, others devote 20% or 30% of their twitter posts to explicitly partisan messages. interestingly, the most ideological members display middling levels of partisan intensity. the most variation comes from the more ideologically typical.",assessment of similiarity to ideology,Content Validation,No
2022-09-07T09:20:10Z,Predicting Partisan Responsiveness: A Probabilistic Text Mining Time-Series Approach,"Bustikova, Lenka; Siroky, David S.; Alashri, Saud; Alzahrani, Sultan","Yes, Continue with Coding",NA,Party Politics: Websites,Others,Slovak (slovakia),Partisan Responsiveness,Polarization,"Supervised: Machine Learning, Unsupervised: Topic Modeling","LDA Topic Model, sparse-learning classifier (SLEP)",NA,Yes,"two step approach; First LDA, then Classification",Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Predicting_Bustikova,",10.1017/pan.2019.18,Political Analysis,2020,2,a,"Determining the number of topics can be done using various methods (e.g., elbow curves, AIC, BIC, etc.). Among these approaches, LDA tends to be most resilient when the number of topics, k , increases (Blei et al. 2010). However, larger k imposes additional computational costs and makes convergence of the posterior probability estimate more difficult. Finding the right k also requires qualitative validation by experts. Aer multiple trials, we determined that the most applicable k was 100. Later, we determined that the results are robust to minor changes to k (","Topic Model: ""qualitative evaluation""",Content Validation,Yes
2022-09-07T09:20:10Z,Predicting Partisan Responsiveness: A Probabilistic Text Mining Time-Series Approach,"Bustikova, Lenka; Siroky, David S.; Alashri, Saud; Alzahrani, Sultan","Yes, Continue with Coding",NA,Party Politics: Websites,Others,Slovak (slovakia),Partisan Responsiveness,Polarization,"Supervised: Machine Learning, Unsupervised: Topic Modeling","LDA Topic Model, sparse-learning classifier (SLEP)",NA,Yes,"two step approach; First LDA, then Classification",Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Predicting_Bustikova,",10.1017/pan.2019.18,Political Analysis,2020,2,b,"Table 1, in the Supplementary materials, shows the accuracy for predicting ethnic party spikes that lead to radical right party responses. The accuracy varies between 81% and 89% for different issues (F-measure). Table 2 shows the accuracy for predicting ethnic spikes that the radical right parties ignore, which varies between 78% and 84% (F-measure). The average F-measure for predicting outcomes of ethnic spikes is therefore 82.9%. Similarly, Table 3 shows the accuracy for predicting ethnic party responsiveness to radical right party spikes, which varies between 80% and 86% depending on the issue (F-measure). Table 4 shows the accuracy for predicting radical spikes that are ignored by the ethnic parties, which varies between 78% and 86% (F-measure). The average F-measure for predicting outcomes of radical spikes is about the same: 82.7%.",Accuracy/F1 Score of ML classifier on topic escalation on the basis of LDA topic (changes),External: Scores from other CATM,No
2022-11-11T10:53:59Z,Dynamics of Polarizing Rhetoric in Congressional Tweets,"Ballard, Andrew O.; DeTamble, Ryan; Dorsey, Spencer; Heseltine, Michael; Johnson, Marcus","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,polarizing rhetoric,Polarization,Supervised: Machine Learning,roBerta,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Dynamics_Ballard,",10.1111/lsq.12374,Legislative Studies Quarterly,2022,3,a,"after training, we tested how  well  our  trained  model  classified  polarizing  rhetoric  in  the  test data, or the 800 tweets (20%) not used to train the model. We present statistics on the accuracy of  our model in classifying po-larizing rhetoric in the test data in table 2. Overall, our model is highly accurate: the out- of- sample predictive accuracy is 90% with a  weighted  F1  score  of   0.9.  While  our  model  under-  performs  on  classifying polarizing tweets relative to non- polarizing tweets, the model’s overall accuracy is higher than the rate of  agreement be-tween our human coders— a testament to the power of  the trans-former in its own right.",manual coding,External: Human Annotated Scores,Yes
2022-11-11T10:53:59Z,Dynamics of Polarizing Rhetoric in Congressional Tweets,"Ballard, Andrew O.; DeTamble, Ryan; Dorsey, Spencer; Heseltine, Michael; Johnson, Marcus","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,polarizing rhetoric,Polarization,Supervised: Machine Learning,roBerta,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Dynamics_Ballard,",10.1111/lsq.12374,Legislative Studies Quarterly,2022,3,b,"We further test our model’s performance by comparing it to the valence aware dictionary and sentiment reasoner (vader), a commonly- used dictionary- based sentiment analysis tool that is designed for use on social media (Hutto and gilbert 2014). vader is  designed  to  handle  slang,  negation  (e.g.,  “not  good”),  and  the  unique punctuation styles commonly found on social media (e.g., “!!!”). although polarizing rhetoric is distinct from sentiment, the concepts are similar enough to warrant comparison.using negative sentiment as a proxy for polarizing rhetoric, we  ran  vader  on  our  test  data.  as  can  be  seen  in  table  2,  our  model  substantially  outperforms  vader.  When  vader  is  ap-plied to our coded test data, the overall out- of- sample predictive accuracy  is  68%  and  the  weighted  F1  score  is  0.75.  vader  ap-pears  unable  to  handle  the  nuances  of   polarizing  rhetoric  at  the  same level as our custom model.",comparison against scores for dictionairy method,External: Scores from other CATM,Yes
2022-11-11T10:53:59Z,Dynamics of Polarizing Rhetoric in Congressional Tweets,"Ballard, Andrew O.; DeTamble, Ryan; Dorsey, Spencer; Heseltine, Michael; Johnson, Marcus","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,polarizing rhetoric,Polarization,Supervised: Machine Learning,roBerta,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Dynamics_Ballard,",10.1111/lsq.12374,Legislative Studies Quarterly,2022,3,c,"One  of   the  traditional  critiques  of   many  machine  learning  models is that while powerful, they are uninterpretable black boxes. Fortunately, researchers can dissect and interpret transformer mod-els. using integrated gradients, it is possible to precisely attribute the predicted classification of  a text based on portions of  the text such as words and phrases (sundararajan, taly, and yan 2017). in Figure 1, we detail how the model classifies tweets using a polar-izing  tweet  from  sen.  tina  smith  (d-  MN)  and  a  non-  polarizing  tweet  from  rep.  virginia  Foxx  (r-  NC  5).  the  model  classified  tweets  as  (non-  )polarizing  based  on  the  highlighted  portions  of   the text, with darker shaded text contributing more to the attribu-tion  of   polarizing  or  not  polarizing.13  in  her  tweet,  sen.  smith  lambasted  the  trump  administration  for  a  proposed  policy  that  would  prohibit  certain  paycheck  deductions  approved  by  home  care  workers.  two  phrases  contributed  the  most  to  our  model’s  lassification of  this tweet as polarizing: “an unnecessary process that  would  hurt  Minnesota  home  care  workers”  and  “misguided  attack with my fellow Minnesota colleagues.” each phrase clearly shows that the rhetoric of  the tweet denigrates an out- group (the trump administration) and rallies allies around the speaker’s posi-tion. rep. Foxx’s tweet, by contrast, is a standard Memorial day message  honoring  fallen  soldiers.  the  model  relied  most  on  this  phrase in classifying the tweet as not polarizing: “honor those who made the ultimate sacrifice.” there is no division created between in-  and out- groups in this tweet, and the highlighted phrase evokes unifying around a common sentiment",nice discussion of integrated gradients application for explainable AI,Content Validation,No
2022-11-18T13:35:33Z,"Hybrid Content Analysis: Toward a Strategy for the Theory-driven, Computer-assisted Classification of Large Text Corpora","Baden, Christian; Kligler-Vilenchik, Neta; Yarchi, Moran","Yes, Continue with Coding",NA,"Social Media: Twitter, Social Media: Facebook, Social Media: WhatsApp",Others,Hebrew,"interpretative polarization, interactinoal polarization",Polarization,"Supervised: Machine Learning, Unsupervised: Topic Modeling",Hybrid content analysis,NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Hybrid_Baden,",10.1080/19312458.2020.1803247,Communication Methods and Measures,2020,3,a,"Coders were instructed to
first consider the lists of 10 top words associated with each topic (by probability, FREX, Lift and
Score; see Roberts et al., 2019) and determine what logic appeared to be responsible for these tokens
being associated with the same topic. Logics did not have to be topical, but could include also
rhetorical practices, social relationship management, or other achievements found in interactive
discourse. Coders then had to validate this inference by referring to the top three associated
documents. Only once coders were able to validate their understanding of the associated tokens
could they apply the definitions and criteria laid down for each variable to classify the topic. Besides
the logical decision rules, some codes also included formal criteria regarding the minimum number
of top tokens that could be associated with a specific meaning. The full codebook is available in the
methodological appendix.",Semantic validation in the pipeline,Content Validation,Yes
2022-11-18T13:35:33Z,"Hybrid Content Analysis: Toward a Strategy for the Theory-driven, Computer-assisted Classification of Large Text Corpora","Baden, Christian; Kligler-Vilenchik, Neta; Yarchi, Moran","Yes, Continue with Coding",NA,"Social Media: Twitter, Social Media: Facebook, Social Media: WhatsApp",Others,Hebrew,"interpretative polarization, interactinoal polarization",Polarization,"Supervised: Machine Learning, Unsupervised: Topic Modeling",Hybrid content analysis,NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Hybrid_Baden,",10.1080/19312458.2020.1803247,Communication Methods and Measures,2020,3,b,"To validate the classification, we created a gold standard of 200 original documents, which
were coded manually by the latter two authors based on the same definitions and criteria as in the
codebook introduced above. Validating the HCA-based classification against this gold standard, we
found mostly very high values for precision (M = 0.89, SD = 0.10, range: 0.64–1.00) and recall
(M = 0.89, SD = 0.07, range: 0.77–1.00).11",hand-coding,External: Human Annotated Scores,Yes
2022-11-18T13:35:33Z,"Hybrid Content Analysis: Toward a Strategy for the Theory-driven, Computer-assisted Classification of Large Text Corpora","Baden, Christian; Kligler-Vilenchik, Neta; Yarchi, Moran","Yes, Continue with Coding",NA,"Social Media: Twitter, Social Media: Facebook, Social Media: WhatsApp",Others,Hebrew,"interpretative polarization, interactinoal polarization",Polarization,"Supervised: Machine Learning, Unsupervised: Topic Modeling",Hybrid content analysis,NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Hybrid_Baden,",10.1080/19312458.2020.1803247,Communication Methods and Measures,2020,3,c,"HCA offers several valuable opportunities for adding nuance to the analysis. Slight variations in
the classification threshold permit a quick and nuanced assessment of the prominence of coded
constructs: In our analysis, raising the threshold to 0.6 substantially increases the share of non classified messages, indicating that few interpretations and expressed stances completely dominate
the posted messages; decreasing it has a lesser effect on the share of non-classified messages, but
slightly shifts the distribution of thematic contexts, illuminating which of these are frequently
present without being dominant. An analysis without classification threshold – i.e., with continuous
weights for classified constructs within each document – diminishes inter-camp differences, suggesting that competing stances imply contrasting emphases, but not a complete neglect of contravening
perspectives",running the same analysis with different thresholds,Unsure,No
2022-11-29T10:36:53Z,"Political Polarization on the Digital Sphere: A Cross-platform, Over-time Analysis of Interactional, Positional, and Affective Polarization on Social Media","Yarchi, Moran; Baden, Christian; Kligler-Vilenchik, Neta","Yes, Continue with Coding",NA,"Social Media: Twitter, Social Media: Facebook, Social Media: WhatsApp",Others,Israeli,political polarization,Polarization,"Supervised: Machine Learning, Unsupervised: Topic Modeling",Hybrid Content Analysis,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Political_Yarchi,",10.1080/10584609.2020.1785067,Political Communication,2021,3,a,"To assess reliability, all 100 topics were double-coded by two coders (native speakers), yielding
a reliability of Krippendorff’s α = 0.88.",coding of topics,Content Validation,Yes
2022-11-29T10:36:53Z,"Political Polarization on the Digital Sphere: A Cross-platform, Over-time Analysis of Interactional, Positional, and Affective Polarization on Social Media","Yarchi, Moran; Baden, Christian; Kligler-Vilenchik, Neta","Yes, Continue with Coding",NA,"Social Media: Twitter, Social Media: Facebook, Social Media: WhatsApp",Others,Israeli,political polarization,Polarization,"Supervised: Machine Learning, Unsupervised: Topic Modeling",Hybrid Content Analysis,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Political_Yarchi,",10.1080/10584609.2020.1785067,Political Communication,2021,3,b,"To assess precision and recall, we selected 200 documents at random from the corpus that were
coded manually based on the same coding instructions used for the classification of topics.
A comparison between the manual classification and the classification obtained via the classification
of modeled topics yielded mostly very high values for precision (M = 0.89, SD = 0.10,
range: 0.64–1.00) and recall (M = 0.89, SD = 0.07, range: 0.77–1.00).",external hand coding,External: Human Annotated Scores,Yes
2022-11-29T10:36:53Z,"Political Polarization on the Digital Sphere: A Cross-platform, Over-time Analysis of Interactional, Positional, and Affective Polarization on Social Media","Yarchi, Moran; Baden, Christian; Kligler-Vilenchik, Neta","Yes, Continue with Coding",NA,"Social Media: Twitter, Social Media: Facebook, Social Media: WhatsApp",Others,Israeli,political polarization,Polarization,"Supervised: Machine Learning, Unsupervised: Topic Modeling",Hybrid Content Analysis,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Political_Yarchi,",10.1080/10584609.2020.1785067,Political Communication,2021,3,c,"To assess robustness, we repeated the entire topic modeling and classification procedure for
each platform separately, estimating topic models with k = 80 for Facebook, k = 80 for Twitter,
and k = 70 for WhatsApp, respectively. All topics were again coded and used to classify the
original documents. A comparison between both classifications yielded satisfactory robustness,
with a Holsti coefficient of 0.80.",running the same analysis again with differnet settings,Unsure,No
2022-11-30T16:43:12Z,Classification accuracy as a substantive quantity of interest: Measuring polarization in westminster systems,"Peterson, Andrew; Spirling, Arthur","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,polarization,Polarization,Supervised: Machine Learning,ml classifier,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Classification_Peterson,",NA,Political Analysis,2018,3,a,"irst we want to show that if the parties dier systematically in terms of the tokens they use, our
approach separates them as an increasing function of that dierence in vocabulary.
We model speech as follows. There are three types of words: “le” and “right” which have no
overlap, and “noise” words which have no relationship to partisanship. For a fixed degree of a
speech which is noise, for the rest of the speech token slots, a Conservative (Labour) member
chooses a “right” (“le” in the Labour case) word with probability a ≥ 21 and a “le” (“right”)
word with probability 1 − a. We denote a the “separation” parameter, and as it approaches 1,
polarization is increasing. At a = 1, members use completely disjoint partisan vocabularies, and
their speeches overlap only in terms of noise words. A “parliament” is 600 members, half from
each party, with each giving one speech of 100 words selected as discussed. We perform a TFIDF
weighting of the relevant matrix, apply the learner(s), and output a predicted probability that each
speech/member is Conservative.As hoped, as a increases for a fixed degree of noise (0.05, 0.1, 0.25, 0.5), we see from Figure 1
that accuracy—i.e., polarization—increases. There, the x -axis represents values of a. When the
separation is suiciently large at these noise levels (a & 0.06, though these magnitudes are not
directly interpretable), the classification rate (on the y -axis) is perfect (1.0). As the two parties
become more similar in their word choices, the classification accuracy declines until the algorithm
is doing no better than chance (at separation ≈0.01)",simulation application,Content Validation,Yes
2022-11-30T16:43:12Z,Classification accuracy as a substantive quantity of interest: Measuring polarization in westminster systems,"Peterson, Andrew; Spirling, Arthur","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,polarization,Polarization,Supervised: Machine Learning,ml classifier,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Classification_Peterson,",NA,Political Analysis,2018,3,b,"e plot our session accuracy results in Figure 3, and it strongly accords with our priors and
those of others for the period (Addison 1994; Seldon 1994; Fraser 2000). In the 1930s, polarization
drops rapidly, reaching a nadir in the years of the Second World War. This makes sense given
the (Churchill led) coalition government of that time. Soon aer, when elections begin in earnest with the 1945 Labour landslide, polarization ticks up. It then enters a long period of approximate
stasis—the “postwar consensus” (Kavanagh and Morris 1994)—between circa 1945 and circa 1979,
with small movements around the mean, though it is gradually sloping upwards. From the first
session of 1979, i.e., the session in which Margaret Thatcher assumed the premiership, polarization
jumps and reaches its zenith around the session corresponding to 1987. It then falls, gradually at
first and then more quickly, as Tony Blair becomes leader of Labour aer 1994. By the sessions
around 2001, polarization is falling sharply, with the end of Gordon Brown’s government and
the beginning of the Conservative–Liberal Democrat coalition marking a further decline. The
dark vertical [green] lines represent structural breaks, in the sense of Bai and Perron (2003) (as
implemented by Zeileis et al. (2002)). These provide more formal evidence of our validation claims,
with change points in September 1948, November 1978 and June 2001. We note in passing that, by
our estimates, polarization in the contemporary House of Commons is on a par with that of the
mid-1960s.
Figure 4 presents the mean variance in speaker estimates for the time period under study.
Importantly, it is not noticeably higher during claimed periods of consensus (i.e., postwar). This",identification of events (face validity),Content Validation,Yes
2022-11-30T16:43:12Z,Classification accuracy as a substantive quantity of interest: Measuring polarization in westminster systems,"Peterson, Andrew; Spirling, Arthur","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,polarization,Polarization,Supervised: Machine Learning,ml classifier,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Classification_Peterson,",NA,Political Analysis,2018,3,c,"We can also compare our accuracy results to more quantitative evidence. In Figure 5 we plot the
two main UK parties in terms of their manifesto “RILE” scores (a measure of where they lie in some
overall sense on the standard le–right spectrum) as provided by the Manifesto Project (Lehmann
et al. 2016; Volkens et al. 2016) for the post-1945 period. The individual points refer to parties in
dierent years (with higher scores implying positions are more right wing), while the solid line is
the (absolute) dierence between the parties. The broken line is a lowess of the same. When these
lines are relatively high, the parties are more polarized (literally more dierent). When they fall, the
parties are closer together.
Of course, manifestos are written prior to a parliament being formed, and there are many
reasons to believe the polarization we see in electoral promises may not show up in identical
magnitudes in a legislature. Comfortingly though, we see the same broad pattern as in Figure 3:
polarization is relatively low aer the war, reaching a peak in the Thatcher years, before enteringsecular decline again. Comparing the manifesto dates to the closest parliamentary session, we
note a reasonable positive correlation of approximately 0.16.",correlation with external RILE score,External: Criterion data / Predictive validation,No
2022-11-14T12:29:05Z,When Do Politicians Use Populist Rhetoric? Populism as a Campaign Gamble,"Dai, Yaoyao; Kustov, Alexander","Yes, Continue with Coding",NA,Party Politics: Campaign Speeches,English,NA,poulist rhetoric,Populism,Supervised: Machine Learning,"""new method""",NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"When_Dai,",10.1080/10584609.2022.2025505,Political Communication,2022,3,a,"Populism is thus a multi-dimensional concept which follows a certain necessary and
sufficient structure (Wuttke et al., 2020; Goertz, 2006). Following such structure, in our
measurement a text is considered populist if and only if it (1) recognizes the people
instead of the elite as the only legitimate source of power (people-centric); (2) creates
separation between us and them (anti-pluralist); and, in doing so, (3) stipulates the
separation of us and them on moral grounds (good versus evil) (Hawkins, 2009; Dai,
2019). None of the necessary components of populism alone can clearly distinguish it
from other related concepts. For example, people-centrism is a shared feature between
populism and liberal democracy. What separates populism from liberal democracy is
moralized anti-pluralism (M¨uller, 2017).",defining the level of measurement,Content Validation,Yes
2022-11-14T12:29:05Z,When Do Politicians Use Populist Rhetoric? Populism as a Campaign Gamble,"Dai, Yaoyao; Kustov, Alexander","Yes, Continue with Coding",NA,Party Politics: Campaign Speeches,English,NA,poulist rhetoric,Populism,Supervised: Machine Learning,"""new method""",NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"When_Dai,",10.1080/10584609.2022.2025505,Political Communication,2022,3,b,"In the end, our model achieves an 91% accuracy in the test set (i.e., making the
same prediction as the human coder). Because populist rhetoric is a rare event in our
data (only 11.79% of sub-speeches are coded as populist in the initial random sample),we provide two additional performance metrics to evaluate the out-of-sample model
performance in Figure 1. We first report the receiver operating characteristic (ROC)
curve and examine the corresponding area under the curve (AUC) in Figure 1a. The
ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) in
out-of-sample prediction for all thresholds. As can be seen from the plot, our model
achieves a high AUC of 0.93, comparing to a 0.50 AUC for an unskilled or random
classification model, while suffering only a minor false positive rate to obtain a high
true positive rate in out-of-sample prediction. For example, to correctly identify 80%
of populist documents, our model makes only about 10% of false positive predictions.
In other words, less than 10% of the documents that are predicted as populist are not
populist",hand-coding,External: Human Annotated Scores,Yes
2022-11-14T12:29:05Z,When Do Politicians Use Populist Rhetoric? Populism as a Campaign Gamble,"Dai, Yaoyao; Kustov, Alexander","Yes, Continue with Coding",NA,Party Politics: Campaign Speeches,English,NA,poulist rhetoric,Populism,Supervised: Machine Learning,"""new method""",NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"When_Dai,",10.1080/10584609.2022.2025505,Political Communication,2022,3,c,"To better illustrate the model performance or what a
populist sub-speech may look like, below, we discuss two randomly drawn sub-speeches
from either party, which were classified as populist by the algorithm (outside of the hand-coded sample). Because the sub-speeches are still quite long, we only include
the highlighted parts of these sub-speeches for demonstration. In the first example
of Barack Obama’s campaign speech in 2008, he creates a separation between Main
Street (us) and Wall Street (them). While Main Street is innocent, Wall Street is
greedy, irresponsible, and corrupted. Furthermore, Wall Street is the reason behind
the economic suffering of Main Street. To that end, Obama also claims that, while he
represents the millions of innocent people, the political establishment in Washington
represents special interests.
",manual identificatio of specific sentences,Content Validation,No
2022-11-17T14:55:29Z,How Populist are Parties? Measuring Degrees of Populism in Party Manifestos Using Supervised Machine Learning,"Di Cocco, Jessica; Monechi, Bernardo","Yes, Continue with Coding",NA,Party Politics: Party Manifestos,English,NA,populism,Populism,Supervised: Machine Learning,"English, Spanish, Italien, Durch, German, French",NA,Yes,has been critizized by other authors,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,How_Di,10.1017/pan.2021.29,Political Analysis,2022,2,a,"Before building the score, we tested the accuracy of the Random Forest with the best parameter
sets found for each country. Thus, we used the six country-specific models to classify all the
sentences in the test set, and we computed the corresponding AuROCs. For completeness’ sake,
we also computed the F1-score for the validation and test sets, which can be used as an alternative
accuracy score for the Grid Search. Table 1 shows the AuROCs and the F1-scores for the test and validation sets. While AuROCs for the test sets are not far from the corresponding average validation
score, the F1-scores for the test sets are generally higher than those for the validation sets. This
fact is due to the Youden index (Ruopp et al. 2008)","test against populism scores of external source ( PopuList classification), thus model fit",External: Criterion data / Predictive validation,Yes
2022-11-17T14:55:29Z,How Populist are Parties? Measuring Degrees of Populism in Party Manifestos Using Supervised Machine Learning,"Di Cocco, Jessica; Monechi, Bernardo","Yes, Continue with Coding",NA,Party Politics: Party Manifestos,English,NA,populism,Populism,Supervised: Machine Learning,"English, Spanish, Italien, Durch, German, French",NA,Yes,has been critizized by other authors,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,How_Di,10.1017/pan.2021.29,Political Analysis,2022,2,b,"For the validation, we first relied on the 2017 CHES (Polk et al. 2017). The
two CHES dimensions that we selected are “anti-elite salience” and “people vs elite.” Anti-elitism,
which is commonly used in the narrative of challenger parties in general (Hobolt and de Vries
2015), can be defined as an explicit attack on “the elites,” portrayed as a homogeneous power
bloc (Zulianello 2019). As for “people vs elite,” according to the 2017 CHES codebook, it measures
the positions of direct vs representative democracy.3 However, support for referendums does not constitute a defining feature of the ideational approach, and the way the question was framed
can be misleading (Meijers and Zaslove 2020a). Framing the question as it was in the survey gives
the impression that populists, per definition, oppose representative democracy and implies that
populism is associable with plebiscitary democracy. Nevertheless, we decided to use this attribute
because of the close relationship between populism and referendums. For example, populist
parties consider themselves as the saviors of democracy and claim that direct democracy can help
them save the people from the elites (Jacobs, Akkerman, and Zaslove 2018).
We excludedfrom the validation the Spanish regionalist parties, since they stand out as outliers;
furthermore, their manifestos are sometimes in Catalan. We also excluded FI as the score that
we have for the Italian 2013 national elections refers to the People of Freedom (PdL), and even
if Berlusconi was the main leader of this party, it also included National Alliance (AN), plus some
other minor parties. Figure 2 shows the correlations and 95% confidence intervals for parties
clustered according to the left–center–right classification,4 plus a pooled analysis that includes
all parties. “Anti-elite salience” and “people vs elite” are significantly correlated with our scores
regardless of parties’ ideological positions. When looking at correlations between our score and
anti-elitism for all the parties together, Pearson’s coefficient is ρ = 0.81 (p < 0.001) and 95%
confidence interval [0.67,0.90], and it is 0.75 when looking at the correlation between the score
and “people vs elite” (p < 0.001 and 95% confidence interval [0.57,0.87]). These outcomes are
consistent with previous studies on the main components of populism (Canovan 2002; Laclau
2005; Mudde 2004; Rooduijn 2018) and referenda as one of the key elements of a populist
democracy (Mudde and Kaltwasser 2013).",comparison with expert surveys,External: Criterion data / Predictive validation,No
2022-11-18T11:41:49Z,Media Coverage of Campaign Promises Throughout the Electoral Cycle,"Müller, Stefan","Yes, Continue with Coding",NA,Party Politics: Political news statements,English,NA,promises (classifciation),Reference,Rule-based: Development dictionary,own dictionary,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Media_Müller,",10.1080/10584609.2020.1744779,Political Communication,2020,5,a,"First, the dictionary-based analysis resulted in a more reliable separation of classes than supervised classifiers when comparing the results to 400 English crowd-coded sentences (Benoit
et al., 2016), randomly sampled from the text corpus (SI Section C.1)",hand-coding,External: Human Annotated Scores,Yes
2022-11-18T11:41:49Z,Media Coverage of Campaign Promises Throughout the Electoral Cycle,"Müller, Stefan","Yes, Continue with Coding",NA,Party Politics: Political news statements,English,NA,promises (classifciation),Reference,Rule-based: Development dictionary,own dictionary,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Media_Müller,",10.1080/10584609.2020.1744779,Political Communication,2020,5,b,"First,
the dictionary-based analysis resulted in a more reliable separation of classes than supervised classifiers when comparing the results to 400 English crowd-coded sentences (Benoit
et al., 2016), randomly sampled from the text corpus (SI Section C.1)",comparison against other CTAM measure,External: Scores from other CATM,Yes
2022-11-18T11:41:49Z,Media Coverage of Campaign Promises Throughout the Electoral Cycle,"Müller, Stefan","Yes, Continue with Coding",NA,Party Politics: Political news statements,English,NA,promises (classifciation),Reference,Rule-based: Development dictionary,own dictionary,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Media_Müller,",10.1080/10584609.2020.1744779,Political Communication,2020,5,c,"Second, I investigate the immediate context of terms used to classify broken and
fulfilled promises analyzing word co-occurrences for sentences on promises in each
class (SI Section C.2). This textual network analysis offers strong evidence that the
classification into the three categories indeed picks up sentences on politics, political
promises, and policy-making, indicated by terms such as budget, coalition, government,
manifesto, minister, and policy. Moreover, we observe clear differences in word cooccurrences between the classes “broken” and “fulfilled",content,Content Validation,Yes
2022-11-18T11:41:49Z,Media Coverage of Campaign Promises Throughout the Electoral Cycle,"Müller, Stefan","Yes, Continue with Coding",NA,Party Politics: Political news statements,English,NA,promises (classifciation),Reference,Rule-based: Development dictionary,own dictionary,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Media_Müller,",10.1080/10584609.2020.1744779,Political Communication,2020,5,d,"Third, I test whether the articles retrieved through the keyword search correspond to
the articles about policy areas in which a promise has been made (SI Section C.3). For the
three months before the 2015 UK General election, I download all articles that mention
keywords from at least one of the six pledges analyzed in Thomson and Brandenburg
(2019). Around 50% of these articles also include words starting with pledge or promise. To
be clear, a large share of articles on these policy issues includes keywords used for
retrieving the sample of newspaper articles. The search query based on promise-related
seed words appears justifiable. Articles on promises that have been classified as broken in
Thomson and Brandenburg (2019) should also be more contain more terms from the
keywords used to classify the breaking of promises. Indeed, the focus on the breaking of
a promise is highest for the two unfulfilled pledges.",correspondence of article keyworys to article keywords,Content Validation,Yes
2022-11-18T11:41:49Z,Media Coverage of Campaign Promises Throughout the Electoral Cycle,"Müller, Stefan","Yes, Continue with Coding",NA,Party Politics: Political news statements,English,NA,promises (classifciation),Reference,Rule-based: Development dictionary,own dictionary,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Media_Müller,",10.1080/10584609.2020.1744779,Political Communication,2020,5,e,"Finally, I examine whether the class of “ongoing promises” focuses mostly on political
promises or whether these sentences may in fact clarify that a promise has been fulfilled
(or broken) using words not included in the set of keywords for the classification.
I randomly selected 400 sentences from the class of “ongoing promises” (100 sentences
per country) and check whether a sentences is related to a political promise and whether
a statement mentions the breaking or fulfillment. Around 80% of the random sample of
“ongoing promises” indeed relates to political promises. 7% of the sentences contain
information on the breaking of a promise, 3% of the statements point to the (potential)
fulfillment of a pledge. Yet, in almost all of these instances, the details on the fulfillment or
breaking are very vague (SI Section C.4). This finding mirrors the results of the crowdsourced coding task. The dictionary-approach performs well in separating promis",Examinaton of specific sentences,External: Human Annotated Scores,No
2022-11-29T11:44:56Z,Measuring Discretion and Delegation in Legislative Texts: Methods and Application to US States,"Vannoni, Matia; Ash, Elliott; Morelli, Massimo","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,delegations and constraints in texts,Reference,Language Representations,own,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Vannoni,",10.1017/pan.2020.9,Political Analysis,2021,2,a,"First, we compare our machine-annotated counts to hand-annotated
counts from a previous paper (Franchino 2004).",hand-annotation,External: Human Annotated Scores,Yes
2022-11-29T11:44:56Z,Measuring Discretion and Delegation in Legislative Texts: Methods and Application to US States,"Vannoni, Matia; Ash, Elliott; Morelli, Massimo","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,delegations and constraints in texts,Reference,Language Representations,own,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Vannoni,",10.1017/pan.2020.9,Political Analysis,2021,2,b,"Second, we compare it to the lexicon-based
strategy of counting modals",comparison with dictionary,External: Scores from other CATM,No
2022-11-29T15:21:24Z,Sentiment is Not Stance: Target-Aware Opinion Classification for Political Text Analysis,"Bestvater, Samuel E.; Monroe, Burt L.","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,stance,Reference,Supervised: Machine Learning,BERT,NA,Yes,multiple case studies,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Sentiment_Bestvater,",10.1017/pan.2022.10,Political Analysis,2022,2,a,"Given the fact that sentiment and stance appear to be only weakly correlated in tweets about
the Women’s March, it becomes relevant to question whether amore accuratemeasure of stance
might have changed the conclusions Felmlee et al. (2020) arrived at regarding the overall level of
approval for the movement contained within the tweets they analyzed. To examine this question,
we used a neural network classifier built on top of BERT (“Bidirectional Encoder Representations
for Transformers”), a massive pretrained language representation model (Devlin et al. 2018) that
represents the current state of the art in language modeling.4 We trained this classifier on theground-truth stance labels for each document in the human-coded Women’sMarch tweets corpus
described above, using random undersampling to account for the class imbalance in the training
data. The resultingmodel achieved an average F1-score of 0.853 on a held-out test sample",hand-coding,External: Human Annotated Scores,Yes
2022-11-29T15:21:24Z,Sentiment is Not Stance: Target-Aware Opinion Classification for Political Text Analysis,"Bestvater, Samuel E.; Monroe, Burt L.","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,stance,Reference,Supervised: Machine Learning,BERT,NA,Yes,multiple case studies,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Sentiment_Bestvater,",10.1017/pan.2022.10,Political Analysis,2022,2,b,"We began by taking a new sample from the Women’s March corpus containing 1,500 tweets
posted by distinct users, and scored each author on an ideological scale ranging form −2.5 (very
liberal) to 2.5 (very conservative) according to the Bayesian ideal point estimation approach
suggested and validated in Barberá (2015). Barberá’s (2015) method leverages the social network
structure of Twitter, inferring the latent political preferences of individual Twitter users from
the known preferences of elites that user follows.",creating a ideology measure using another text model,External: Scores from other CATM,No
2022-11-30T17:07:07Z,Time and meaning-making in the “hybrid” media: Evidence from the 2016 US election,"Lavi, Liron","Yes, Continue with Coding",NA,"Newspaper, Social Media: Twitter",English,NA,relating to time,Reference,Rule-based: Adjusted dictionary,Timex,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Time_Lavi,",NA,Journal of Communication,2020,1,a,"The validation procedure fol-
lowed the automated content analysis validation guidelines of Grimmer and Stewart
(2013). Manual coding of the10 categories by a human coder on a subset of the data
resulted in significant Kappa statistics of inter-rater reliability across all categories in both datasets.Theaveragemachine-humanagreementratewas 96% in the news
media data and 86% in the social media data",hand-coding,External: Human Annotated Scores,No
2022-11-07T12:17:29Z,A wall of incivility? Public discourse and immigration in the 2016 U.S. Primaries,"Rossini, Patrícia; Sturm-Wikerson, Heloisa; Johnson, Thomas J.","Yes, Continue with Coding",NA,"Social Media: Facebook, Party Politics: Campaign Speeches",English,NA,"campaign message strategies (attack, advocacy)",Rhetorical Style,"Supervised: Machine Learning, Rule-based: Adjusted dictionary","SMV, lexicon based approach by Jackson et al. (2017)",NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"A_Rossini,",10.1080/19331681.2020.1858218,Journal of Information Technology & Politics,2021,2,a,"The performance of the classifiers is evaluated
with an F1 score, a weighted average of precision
and recall. Precision measures how often machinepredicted types align with gold-labeled data (coded
by humans), and it is sensitive to false positives.
Recall measures whether relevant human-coded
message categories can be identified by the classifier
and it is vulnerable to false negatives. An F1 score of
1 represents perfect precision and recall.
Contrariwise, a 0 would indicate totally incorrect precision and recall. The performance of the multilabel model is in Table 1.",hand-coding,External: Human Annotated Scores,Yes
2022-11-07T12:17:29Z,A wall of incivility? Public discourse and immigration in the 2016 U.S. Primaries,"Rossini, Patrícia; Sturm-Wikerson, Heloisa; Johnson, Thomas J.","Yes, Continue with Coding",NA,"Social Media: Facebook, Party Politics: Campaign Speeches",English,NA,"campaign message strategies (attack, advocacy)",Rhetorical Style,"Supervised: Machine Learning, Rule-based: Adjusted dictionary","SMV, lexicon based approach by Jackson et al. (2017)",NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"A_Rossini,",10.1080/19331681.2020.1858218,Journal of Information Technology & Politics,2021,2,b,"deriving seed words from these sources, human coders annotated and corrected the
lexicon. For immigration, the lexicon included 289 words. The performance of the lexicon for the topic of immigration had an F1 score of .92, with .86 precision and .98 recall (Jackson et al., 2017).",hand-coding of topics for a dictionary,External: Human Annotated Scores,No
2022-11-17T16:27:34Z,"The relationship between race competitiveness, standing in the polls, and social media communication strategies during the 2014 U.S. gubernatorial campaigns","Rossini, Patrícia; Stromer-Galley, Jennifer; Kenski, Kate; Hemsley, Jeff; Zhang, Feifei; Dobreski, Brian","Yes, Continue with Coding",NA,"Social Media: Twitter, Social Media: Facebook",English,NA,persuasive / conversatinal messages (style),Rhetorical Style,Supervised: Machine Learning,SVM,NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"The_Rossini,",10.1080/19331681.2018.1485606,Journal of Information Technology & Politics,2018,1,a,".
We evaluated all classifiers’ performance with a
micro-average F1-score, which is a weighted average of two measures: precision and recall.
Precision measures how often machine-predicted
message types align with human annotated judgments, so it is sensitive to false positives (e.g.,
messages predicted by machine as informative
are indeed coded as informative by human
coders). Recall measures whether relevant human
coded message categories can be identified by the
classifier, so it is sensitive to false negatives (e.g.,
human coded informative messages can be predicted correctly by classifiers). Micro-average, or
weighted average, is a precise measure especially
for unbalanced datasets. In addition to raw scores,
it also considers the number of instances in each
class (Van Asch, 2013) and allows for comparison
across results sets. An F1-score of 1 represents
perfect precision and recall. Contrariwise, a 0
would indicate no correct classifications and no
real observations were included. Table 2 shows the micro-average F-1 scores are
0.73 and 0.71 for Facebook and Twitter, respectively.
The F-1 scores of our high-level main-category classifiers are in the range of 0.67–0.79 except ceremonial.
",hand-coding,External: Human Annotated Scores,No
2022-11-24T18:43:14Z,Informing the Leader: Bureaucracies and International Crises,"Schub, Robert","Yes, Continue with Coding",NA,Party Politics: adviser input,English,NA,political or military attributes,Rhetorical Style,"Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary","random forest, SVM",NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Informing_Schub,",10.1017/S0003055422000168,American Political Science Review,2022,6,a,"Beyond high accuracy, the
analysis shows that inaccurate classifications were clustered close to the cut point between the two categories.
Figure 2 plots the 25 terms with the highest relative
frequency for each category, generated inductively
from the training set codings. These terms can be used
for both categories (e.g., “believ”), but in practice
within the training set they are far more likely to be
used with one over the other. Word stems that most
distinguish military texts include “forc,” “attack,”
“nuclear,” and “general.” Word stems to the right are
indicative of political texts. State leaders
—“Khrushchev,” “Nasser”—help distinguish these
texts, as do references to “govern” and “talk.” Invoking
foreign leaders suggests a concern with that state’s
domestic politics or the individual’s preferences. Nonetheless, tests in SI §5.2 demonstrate that results hold
when excluding leader names from the training set. The
most telling term for political texts, “will,” concerns
projections of how actors will respond to stimuli and
their willpower or resolve. A follow-up coding evaluated how speakers employed “will” in practice. Within
a random subset of the training data, “will” preceded
political (vs. military) content in 59% of uses, which is
high given that only 39% of texts were classified as
political",error analysis,Content Validation,Yes
2022-11-24T18:43:14Z,Informing the Leader: Bureaucracies and International Crises,"Schub, Robert","Yes, Continue with Coding",NA,Party Politics: adviser input,English,NA,political or military attributes,Rhetorical Style,"Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary","random forest, SVM",NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Informing_Schub,",10.1017/S0003055422000168,American Political Science Review,2022,6,b,"Overall, I coded
61% and 39% of the training set as military and political, respectively. A research assistant produced identical codings for 82% of the 475 texts, with further
measures of intercoder reliability provided in the
SI. Several standard preprocessing steps—removing
numbers and punctuation and stemming to reduce
words to common roots (e.g., “operate” and
“operation” to “oper”)—preceded the analysis.
I tested a variety of algorithms, including random
forest models and support vector machines, and
achieved the best performance with a simple naive
Bayes classifier, which accurately classified 88% of
texts in 10-fold cross-validation checks",hand-coding,External: Human Annotated Scores,NA
2022-11-24T18:43:14Z,Informing the Leader: Bureaucracies and International Crises,"Schub, Robert","Yes, Continue with Coding",NA,Party Politics: adviser input,English,NA,political or military attributes,Rhetorical Style,"Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary","random forest, SVM",NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Informing_Schub,",10.1017/S0003055422000168,American Political Science Review,2022,6,c,"Below is a sample text with the uncertainty dictionary words italicized.
Ball and Thompson believe that the Chinese decision to intervene on the ground would, in
the final analysis, probably depend largely on the extent to which Peiping felt assured of Soviet
support. There is no way that we can safely predict whether or not such support would be
forthcoming. They are convinced, however, that the risks of Chinese ground intervention would
be great and the costs of such intervention tremendous—particularly since the very taking of
this step by Peiping would presumably imply substantial Soviet involvement, perhaps even to
the point of a large-scale Soviet-US confrontation.5
Dictionary methods are more difficult to validate than supervised learning approaches because there
is no analogue to k-fold cross-validation (Grimmer and Stewart 2013). I adopt four approaches. The first
hand codes a subset of all documents (roughly 5%) from the corpus into three levels of uncertainty. These
human codings strongly correlate (0.53) with the dictionary method uncertainty scores",dictionary validation: comparison with hand-coding,External: Human Annotated Scores,Yes
2022-11-24T18:43:14Z,Informing the Leader: Bureaucracies and International Crises,"Schub, Robert","Yes, Continue with Coding",NA,Party Politics: adviser input,English,NA,political or military attributes,Rhetorical Style,"Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary","random forest, SVM",NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Informing_Schub,",10.1017/S0003055422000168,American Political Science Review,2022,6,d,"The second approach leverages texts where there are clear expectations for the relative degree of uncertainty. For instance, during the EC-121 incident Defense Secretary Laird wrote to Nixon suggesting that
the JCS was overly certain in its estimate of what outcome US forces could impose on North Korea. He
wrote, “If U.S. losses occur in the strike (and I believe there is more chance they may than the JCS papers
indicate). . . ”6 Laird’s memo should, and does, register a higher uncertainty score than the JCS communications (6.2% vs. 2.6%). I repeat the exercise with two texts from during the Pleiku crisis in 1965. Thomas
Hughes, Director of the State Department’s Bureau of Intelligence and Research, was dismayed by an earlier
memo from McGeorge Bundy. In Bundy’s memo concerning probable reactions to escalated US bombing
in Vietnam, he understated the risk of bombing actions by ignoring probable Chinese reactions. Chinese
intervention or support for the North Vietnamese would amount to greater enemy military capabilities and
thus is an added source of uncertainty. Hughes writes:
Incomprehensibly to me, the White House memorandum discusses the risks of sustained US air
strikes against North Vietnam without examining Chinese Communist responses. However, the
two intelligence community products estimate Chinese Communist air intervention to be quite
likely at some stage in this very process.7
Again, the measure appropriately identifies Hughes’ memo to be more uncertain than Bundy’s memo (uncertainty scores of 7.7% and 5.3% respectively).",dictionary validation: identification of stand-out texts,Content Validation,Yes
2022-11-24T18:43:14Z,Informing the Leader: Bureaucracies and International Crises,"Schub, Robert","Yes, Continue with Coding",NA,Party Politics: adviser input,English,NA,political or military attributes,Rhetorical Style,"Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary","random forest, SVM",NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Informing_Schub,",10.1017/S0003055422000168,American Political Science Review,2022,6,e,"A third validity check compares uncertainty scores of those with historically well-known assessments.
For instance, George Ball is widely thought to have been uncertain about what outcomes the US could secure through force in Vietnam. Walt Rostow, in contrast, was an adamant believer that North Vietnamese
resolve was limited and the continued application of force would prove effective. Consistent with expectations, the uncertainty score for Ball (5.4%) is higher than that for Rostow (4.6%).",dictionary validation: identifying specific historic events,Content Validation,Yes
2022-11-24T18:43:14Z,Informing the Leader: Bureaucracies and International Crises,"Schub, Robert","Yes, Continue with Coding",NA,Party Politics: adviser input,English,NA,political or military attributes,Rhetorical Style,"Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary","random forest, SVM",NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Informing_Schub,",10.1017/S0003055422000168,American Political Science Review,2022,6,f,"The fourth validation approach uses an alternative dictionary to measure uncertainty. The Loughran
and McDonald (2011) dictionary is less appropriate than the Lasswell one because the former was designed for financial contexts rather than political ones. Nonetheless, the generated measures are positively
correlated (0.61) and, as shown in a robustness check below, all results hold with the alternative LoughranMcDonald scores.
",validation dictionary: comparison with other dictionary,External: Scores from other CATM,No
2022-09-07T13:36:44Z,Latent Semantic Scaling: A Semisupervised Text Analysis Technique for New Domains and Languages,"Watanabe, Kohei","Yes, Continue with Coding",NA,Newspaper,Others,"English, Japanese",sentiment,Sentiment/Emotion,Unsupervised: Text Scaling,New semisupervised document scaling technique Latent Semantic Scaling (LSS,NA,Yes,development and application of a new method,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Latent_Watanabe,",10.1080/19312458.2020.1832976,Communication Methods and Measures,2021,3,a,"Following Young and Soroka (2012), I computed mean sentiment scores for articles in the sample to compare the results of manual and machine coding. In Figure 4, the mean scores correspond almost linearly with the manual coding in LSD, while the mean scores for moderately positive (+1) articles are lower than they should in LSS; the standard errors for very positive articles are also smaller in LSD than in LSS; both LSD and LSSscaled negative articles accurately. The overlapped 95% confidence intervals indicate that some of the mean differences are not statistically significant, but their standard errors will be much smaller when the sample is larger. Figure 5 shows the correlation between machine and manual coding in a longitudinal setting. Although the mean sentiment scores can be less accurate in both manual and machine coding as the articles are spread over 30 years, I can observe the strong correlation between them, especially humans and LSS until 1995. However, neither LSD nor LSS replicate negative overall shifts in manual coding from 1993; one of the largest discrepancies between humans and machines can be found in 2006 when the number of articles is the smallest. In this setting, the correlation between",comparison with hand-coded scores,External: Human Annotated Scores,Yes
2022-09-07T13:36:44Z,Latent Semantic Scaling: A Semisupervised Text Analysis Technique for New Domains and Languages,"Watanabe, Kohei","Yes, Continue with Coding",NA,Newspaper,Others,"English, Japanese",sentiment,Sentiment/Emotion,Unsupervised: Text Scaling,New semisupervised document scaling technique Latent Semantic Scaling (LSS,NA,Yes,development and application of a new method,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Latent_Watanabe,",10.1080/19312458.2020.1832976,Communication Methods and Measures,2021,3,b,"However, neither LSD nor LSS replicate negative overall shifts in manual coding from 1993; one of the largest discrepancies between humans and machines can be found in 2006 when the number of articles is the smallest. In this setting, the correlation between humans and machines is r = 0.62 in LSD and r = 0.70 in LSS (Figure 6), although the correlation between LSD and LSS is only r = 0.34.",comparing model estimates (LSS) with dictionary (LSD),External: Scores from other CATM,Yes
2022-09-07T13:36:44Z,Latent Semantic Scaling: A Semisupervised Text Analysis Technique for New Domains and Languages,"Watanabe, Kohei","Yes, Continue with Coding",NA,Newspaper,Others,"English, Japanese",sentiment,Sentiment/Emotion,Unsupervised: Text Scaling,New semisupervised document scaling technique Latent Semantic Scaling (LSS,NA,Yes,development and application of a new method,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Latent_Watanabe,",10.1080/19312458.2020.1832976,Communication Methods and Measures,2021,3,c,"In order to demonstrate how LSS can be used in real research projects, I applied the fitted model to the entire corpus of the English articles (Figure 7). I can confirm that the news articles were largely positive in the period of Reaganomics (1982–1989) but they became very negative after the savings and loan crisis; they became very positive after 1995, when America’s service sector enjoyed prosperity, but very negative after the occurrence of the economic crisis in Asia; the bursting of the dot-com bubble made their sentiment even more negative; the subprime mortgage crisis that triggered the global economic crisis also changed the sentiment dramatically. Figure 8 shows that mean sentiment scores by LSD and LSS are very strongly correlated with each other (r = 0.77) and with the changes in gross domestic product (GDP) of the United States (r = 0.65). Large discrepancies between the mean sentiment scores and the economic indicator are found in 1984 and 1997–1999, but the fall in sentiment in the latter period was caused mainly by the financial crisis in Asia",Face validation of scores on a long time frame,Content Validation,No
2022-09-07T14:12:20Z,The Automatic Analysis of Emotion in Political Speech Based on Transcripts,"Cochrane, Christopher; Rheault, Ludovic; Godbout, Jean-François; Whyte, Tanya; Wong, Michael W.-C.; Borwein, Sophie","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,emotional content,Sentiment/Emotion,"Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary, Rule-based: Development dictionary","Lexicoder 3.0, Sentiwordnet 3.0, Hu-Lui Lexicon, Jockers-Ringers Lexicon, VADER",NA,Yes,comparison of different methods to analyse parliamentary data on the sentence level,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"The_Cochrane,",10.1080/10584609.2021.1952497,Political Communication,2022,1,a,"Figure 3 compares tools that we tested for predicting the sentiment scores of our human text coders, including the dictionary induced using word2vec embeddings described above. For each measure, the confusion matrix overlays a jitterplot to show the proportional reduction in error alongside the classification accuracy of the different tools. For the measure of accuracy, we consider a classification successful if the method produces a score in the positive range – past the midway point – while the human coders also coded a text as positive on average; the opposite must be true for negative predictions. This corresponds to the percent correctly predicted commonly used in binary classification tasks. Some of the dictionary tools performed well overall, but results were invariably mixed. Lexicoder was attuned very effectively to negative sentiment, but not as well to positive sentiment. Notably, many of the video transcripts contained no words in the Lexicoder, HuLiu, and Vader dictionaries. In total, 33% of the sentences were unclassified by Hu-Liu, 29% by Lexicoder, and 19% by Vader. Among the dictionaries, Sentiwordnet classified the largest proportion of the sentences, but it was also the least accurate.9 As we discussed earlier, there is no pre-classified or human annotated Hansard on which to train supervised learners, and manually annotating Hansard would be time-consuming, costly, and potentially predetermine the results to the same extent as human curated dictionaries. Nonetheless, it might be possible to leverage models trained on large annotated corpora from other domains. Thus, we also tested a number of supervised learners trained",comparison with hand-coded date,External: Human Annotated Scores,No
2022-09-12T15:17:35Z,"Role-based Association of Verbs, Actions, and Sentiments with Entities in Political Discourse","Fogel-Dror, Yair; Shenhav, Shaul R.; Sheafer, Tamir; Van Atteveldt, Wouter","Yes, Continue with Coding",NA,Newspaper,English,NA,sentiment oriented sentiment analysis,Sentiment/Emotion,Supervised: Part of Speech Tagging,"Lexicoder Sentiment Dictionary, role-based association method",NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Role-based_Fogel-Dror,",10.1080/19312458.2018.1536973,Communication Methods and Measures,2019,4,a,"We initially reviewed the results on a timeline by aggregating the sentiment scores (per entity, per article) by summing them per day. While reviewing the entity-oriented sentiment timeline separately for Israel and the PA, observations with high levels of negative sentiment scores for both entities were correlated with the two main violent events in this period —“Operation Protective Edge” in the Gaza strip (July-August 2014), and the Temple Mount/Al-Aqsa Mosque clashes (October 2015). Please refer to Figure 1 (the graph for the 10-word proximity method is practically identical to the graph created by the sentence proximity method). These results might seem promising at first, as negative sentiment is expected to be attributed to rival entities in violent events. However, when placed side by side at the daily level (Figure 1), the sentiment–entity timeline of Israel and the PA looked fairly symmetrical. Assigning a negative sentiment to both entities may be a reasonable finding, as both participated in the violent events; yet, the level of symmetry still raises a concern. Is it really the case that the media is so balanced in terms of negative sentiment toward both actors, or is this a problem of discriminant validity (Adcock & Collier, 2001)? If the latter is true, this means that instead of measuring the specific sentiment that was supposed to be associated with each entity, we measured the volume of sentiment expressed in the text in general. If this is true, we actually measured the textual sentiment surrounding the entities but not the sentiment attributed to them.",face validation,Content Validation,Yes
2022-09-12T15:17:35Z,"Role-based Association of Verbs, Actions, and Sentiments with Entities in Political Discourse","Fogel-Dror, Yair; Shenhav, Shaul R.; Sheafer, Tamir; Van Atteveldt, Wouter","Yes, Continue with Coding",NA,Newspaper,English,NA,sentiment oriented sentiment analysis,Sentiment/Emotion,Supervised: Part of Speech Tagging,"Lexicoder Sentiment Dictionary, role-based association method",NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Role-based_Fogel-Dror,",10.1080/19312458.2018.1536973,Communication Methods and Measures,2019,4,b,"to test our concern, we measured the correlation between sentiment scores that were assigned to the two entities in each sentiment category—positive and negative. The results pointed at a strong correlation for negative sentiment (Pearson’s r > .74) and moderate correlations for positive sentiment (r > .50). To further test whether the scores measured the number of sentiment expressions instead of the sentiment assigned to each entity, we measured the correlations between the volume of sentiment expressions (positive/negative) for each entity and the total sentiment score (positive/negative). The results here pointed to the same validity-related concerns, as we found moderate-to-strong correlations for negative sentiments and positive sentiments for both entities (r > .76 and r > .42, respectively). All correlations were significant (p < .05). This means that these proximity methods of association do not clearly differentiate among the general sentiments expressed in the text and the sentiments intended to be associated with each entity.",comparison with Model estimates with two unassociated sentiment scores,External: Scores from other CATM,Yes
2022-09-12T15:17:35Z,"Role-based Association of Verbs, Actions, and Sentiments with Entities in Political Discourse","Fogel-Dror, Yair; Shenhav, Shaul R.; Sheafer, Tamir; Van Atteveldt, Wouter","Yes, Continue with Coding",NA,Newspaper,English,NA,sentiment oriented sentiment analysis,Sentiment/Emotion,Supervised: Part of Speech Tagging,"Lexicoder Sentiment Dictionary, role-based association method",NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Role-based_Fogel-Dror,",10.1080/19312458.2018.1536973,Communication Methods and Measures,2019,4,c,"Two trained human coders classified each sentiment verb expression as associated with Israel, the PA, neither, or both (Krippendorf’s α = .8). Differences between coders were discussed to form a gold standard that was used to train and test the algorithm. At this point, for every verb, the dataset contained the gold standard and the four features. We split the dataset into a training set (60%), to be used by the learning version, and a test set (40%), and created two versions of the RBA method: rule-based and learning. The rule-based version was easier to implement and did not need the addition of manually coded examples to train the algorithm. It had two rules: (1) If the verb is active, associate the sentiment measured in the verb with entities found in the subject, and otherwise with entities found in the predicate. (2) If the verb is a direct referencing verb, reverse the association. To improve the accuracy of the method by fine-tuning the weights of the features inductively and allowing for further expansion of the method, we created a version of the method that replaces the two a priori rules with a learning algorithm. To this end, we used a Python implementation of a support vector machine with a linear kernel (Pedregosa et al., 2011). The algorithm was trained on the training set (N = 615 verbs).",Human annotated test set,External: Human Annotated Scores,Yes
2022-09-12T15:17:35Z,"Role-based Association of Verbs, Actions, and Sentiments with Entities in Political Discourse","Fogel-Dror, Yair; Shenhav, Shaul R.; Sheafer, Tamir; Van Atteveldt, Wouter","Yes, Continue with Coding",NA,Newspaper,English,NA,sentiment oriented sentiment analysis,Sentiment/Emotion,Supervised: Part of Speech Tagging,"Lexicoder Sentiment Dictionary, role-based association method",NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Role-based_Fogel-Dror,",10.1080/19312458.2018.1536973,Communication Methods and Measures,2019,4,d,"As a remedy, we focused the manual validation on measuring negative sentiment only, following researchers who have found negativity to be clearer to measure (Haselmayer & Jenny, 2017; Soroka et al., 2015). We also instructed human coders to follow the perspective and assumptions of the lexicon. For example, as the sentiment lexicon did not assign different values to different expressions, we only measured the quantity of sentiments, and not their quality (e.g., murdering a person and injuring a person had similar negative weights). Two human coders were trained to code articles for sentiments expressed toward each entity—Israel and the PA on a three-level scale: 0 (no negative sentiment), 1 (low level of negative sentiment), and 2 (high level of negative sentiment—see the full coding instructions in Section C in the online Appendix). As we were interested in the bias toward one entity compared with that toward the other, we calculated the value of delta between the negative sentiments associated with Israel and the PA. The result was a five-point scale of negativity measure that spanned from −2 (PA was represented more negatively) to + 2 (Israel was represented more negatively). Intercoder reliability was no lower than Krippendorf’s α = .71.

Moving to the validation of the association method, this analysis provides further support for our main concerns regarding proximity-based approaches and supports our alternative method. First, we found support for our claim regarding discriminant validation, as 39% of all articles were scored with a non-zero negativity score, which means that sentiments associated with both entities were asymmetrical. This finding was even more convincing when articles containing no negative sentiment toward any entity were eliminated. From the remaining articles, 72% received a non-zero negativity score. Second, results at the week level show that while the two proximity methods of association (sentence and 10-word) were not significantly correlated with the manual analysis (in fact, the correlation was negative), the RBA method was positively and significantly correlated with it (Spearman’s ρ = .26, p < .05). Although modest (maybe because of the RBA’s focus on associations of verbs), this correlation supports the validity of the RBA method compared with the other two proximity methods of association considered. These findings show that associations formed by the RBA method are considerably more accurate at the expression level and more valid at the document level than those made by proximity methods.","Manual Validation to test for discriminant validity (using hand-coded data, but the goal here is to evaluate discriminant validity between their method and other methods)",Content Validation,No
2022-09-15T10:03:49Z,Clickbait for climate change: comparing emotions in headlines and full-texts and their engagement,"Xu, Zhan; Laffidy, Mary; Ellis, Lauren","Yes, Continue with Coding",NA,Newspaper,English,NA,Emotions,Sentiment/Emotion,Rule-based: Off-the-shelf dictionary,NRC Emotion Lexicon,NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Clickbait_Xu,",10.1080/1369118X.2022.2050416,"Information, Communication & Society",2022,1,a,"Dictionary has demonstrated high validity when examining short texts such as tweets and SMS (Kiritchenko et al., 2014) as well as long texts such as novels (Mohammad, 2012). NRC NRC Emotion Lexicon identifies the intensity level that each word and phrase is associated with positively- or negatively- valenced emotions and eight discrete emotions including anger, anticipation, disgust, fear, joy, sadness, surprise, and trust (S. M. Mohammad & Turney, 2010)",reference to previous validation evidence,Content Validation,No
2022-11-07T11:37:38Z,"The Bully Pulpit, Social Media, and Public Opinion: A Big Data Approach","Michael, Gabriel; Agur, Colin","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,public opinion,Sentiment/Emotion,Supervised: Machine Learning,"support-vector machine (SVM),",NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"The_Michael,",10.1080/19331681.2018.1485604,Journal of Information Technology & Politics,2018,1,a,"Coders were provided with the full text of tweets, user biographies, the expanded URLs of any shortened URLs embedded in a tweet, the user’sTwitter handle, the user’s self-provided “real name,” the user’s self-provided user location, and the handles and real names of any other Twitter users mentioned in the tweet. For net neutrality tweets, coding was divided among three coders, each of whom was provided with 2,400 tweets to classify. Of these, 2,000 were randomly selected from the complete training set, and the remaining 400 were randomly selected from each other coder’s set to provide a way to assess inter-coder reliability. Coders were instructed to classify each tweet as either support or opposing net neutrality; in case where support or opposition was unclear, coders marked the tweets as unclassifiable. After excluding unclassifiable tweets and duplicates, the training set comprised 5,026 unique tweets, all either marked as support or opposing net neutrality. By excluding unclassifiable tweets and including non-randomly selected tweets

Applying a support-vector machine (SVM) classifier trained on the 5,729 manually coded tweets from Section 3.3 to these transformed meta-documents yielded five-fold cross-validated classification accuracies of 90–91% for both classes, which are acceptable considering our upper-bound of approximately 98%.18 We found that including bigrams (counts of pairs of words, rather than just single words) as features slightly improved classification accuracy, at a cost of greatly expanded feature space; however, since our primary goal is model performance rather than interpretability, we included these bigrams",manual coding,External: Human Annotated Scores,No
2022-11-14T14:19:00Z,Playing to the Gallery: Emotive Rhetoric in Parliaments,"Osnabrügge, Moritz; Hobolt, Sara B.; Rodon, Toni","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,emotive rhetoric,Sentiment/Emotion,Rule-based: Adjusted dictionary,combining the Affective Norms for English Words dictionary with word-embedding techniques to create a domain-specific dictionary.,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Playing_Osnabrügge,",10.1017/S0003055421000356,American Political Science Review,2021,3,a,"Using the ANEW dictionary comes with several
advantages. First, the methodology for creating the
ratings has been validated and the dictionary is wellestablished in the scientific community (Bradley and
Lang 2017)",saying that the dictionary has been validated before,Unsure,Yes
2022-11-14T14:19:00Z,Playing to the Gallery: Emotive Rhetoric in Parliaments,"Osnabrügge, Moritz; Hobolt, Sara B.; Rodon, Toni","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,emotive rhetoric,Sentiment/Emotion,Rule-based: Adjusted dictionary,combining the Affective Norms for English Words dictionary with word-embedding techniques to create a domain-specific dictionary.,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Playing_Osnabrügge,",10.1017/S0003055421000356,American Political Science Review,2021,3,b,"types of debates. To provide contextual reference
points, the two dotted vertical lines correspond to the
beginning of the Iraq war and the Brexit referendum—
two highly polarizing and emotive issues in the UK
Parliament. The figure illustrates that the level of
emotive rhetoric is highest for PMQs and the opening
day of the Queen’s Speech debate, in line with our
expectation.
",comparison with external events,Content Validation,Yes
2022-11-14T14:19:00Z,Playing to the Gallery: Emotive Rhetoric in Parliaments,"Osnabrügge, Moritz; Hobolt, Sara B.; Rodon, Toni","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,emotive rhetoric,Sentiment/Emotion,Rule-based: Adjusted dictionary,combining the Affective Norms for English Words dictionary with word-embedding techniques to create a domain-specific dictionary.,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Playing_Osnabrügge,",10.1017/S0003055421000356,American Political Science Review,2021,3,c,"Figure A3 uses the standard ANEW dictionary, which we rescale to measure
emotive rhetoric (Bradley and Lang, 2017). The standard ANEW dictionary has
a scale from 1 to 9, where 1 refers to unpleasant/unhappy/negative and 9 to pleasant/happy/positive. A score of 5 means that the word is neutral. We create a scale
from 0 to 4, which captures the deviation from 5.
",comparison with other dictionary,External: Scores from other CATM,No
2022-11-17T15:39:58Z,"Stuck in a Nativist Spiral: Content, Selection, and Effects of Right-Wing Populists’ Communication on Facebook","Heiss, Raffael; Matthes, Jörg","Yes, Continue with Coding",NA,Social Media: Facebook,Others,German,anti-elitist sentiment,Sentiment/Emotion,"Rule-based: Off-the-shelf dictionary, Rule-based: Development dictionary",dictionary developed by Rooduijn and Pauwels (2011),NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Stuck_Heiss,",10.1080/10584609.2019.1661890,Political Communication,2020,1,a,"We validated
our approach by comparing the dictionary-based codes against hand-coded data.1 Despite
successful validation, misclassification was possible—for example, if left-wing parties
criticized other parties’ labeling of immigrants as “terrorists” or “criminals” in a political
post. Ultimately, anti-elitist references appeared in 5.19% of all posts, whereas antiimmigrant ones appeared in 2.31%",hand-coding,External: Human Annotated Scores,No
2022-11-17T17:11:17Z,Multilingual Sentiment Analysis: A New Approach to Measuring Conflict in Legislative Speeches: Multilingual Sentiment Analysis,"Proksch, Sven-Oliver; Lowe, Will; Wäckerle, Jens; Soroka, Stuart","Yes, Continue with Coding",NA,Party Politics: Legislative Documents,English,NA,Sentiment,Sentiment/Emotion,Rule-based: Adjusted dictionary,English-language Lexicoder Sentiment Dictionary,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Multilingual_Proksch,",10.1111/lsq.12218,Legislative Studies Quarterly,2019,1,a,"The SOTEU 2010 corpus allows us to apply all dictionaries
using the official EP translations of the debate. Our gold standard of sentiment is a set of hand-coded estimates of the speeches.
Student coders were instructed to code the whole debate cut into
separate four-sentence paragraphs on a five-point scale ranging
from very negative to very positive, with the possibility of indicating that a paragraph was uncodable. All in all, this yielded
300 paragraphs, of which each coder had to code 200. Each paragraph was thus coded by two coders. The intercoder reliability
(Cohen’s Kappa) was 0.8; 23 paragraphs were deemed uncodable by at least one coder; and only two paragraphs could not be
coded by any coder. We took the average of the codings across the
paragraphs to get a sentiment estimate aggregated for the political group",hand-coding,External: Human Annotated Scores,No
2022-11-18T11:21:13Z,"If You Have Choices, Why Not Choose (and Share) All of Them? A Multiverse Approach to Understanding News Engagement on Social Media","Pipal, Christian; Song, Hyunjin; Boomgaarden, Hajo G.","Yes, Continue with Coding",NA,Newspaper,Others,"German, English, Spanish, Dutch",sentiment,Sentiment/Emotion,Rule-based: Off-the-shelf dictionary,"LIWC Linguistic Inquiry and Word Count dictionary, LSD Lexicoder sentiment dictionary, NRC emotion dictionary",NA,Yes,multiverse approach (showing all options),Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"If_Pipal,",10.1080/21670811.2022.2036623,Digital Journalism,2022,1,a,"Once the set of analytical decisions is defined, the second step is to run all the analyses.
This means that for each combination of choices one has to: (1) construct a dataset, (2)
specify a model, (3) estimate the model and extract the quantities of interest (e.g. coefficients and standard errors). While the first implementations of the multiverse approach
simply looped over different sets of variables,3 this quickly becomes tedious as the set
of analytical options gets more complex. In our example, for instance, we do not want
to include every combination of operationalizing sentiment and arousal. Instead, we
only want to include combinations that use the same dictionary, e.g. in all models
where the LSD dictionary is used to measure sentiment, we also want arousal to be
measured with the LSD dictionary. One can easily see how the inclusion of such control
flows makes a loop-based implementation cumbersome and prone to error.
In our implementation of this multiverse analysis, we use the R-package multiverse
(Sarma et al. 2021) to estimate the models and extract results",comparison of results with different dictionaries,External: Scores from other CATM,No
2022-11-25T16:37:29Z,State as Salesman: International Economic Engagement and Foreign News Coverage in China,"Ji, Chengyuan; Liu, Hanzhang","Yes, Continue with Coding",NA,Newspaper,Others,mandarin (not sure),sentiment,Sentiment/Emotion,Supervised: Machine Learning,"Naive Bayes, SVM, Multi-layer perceptron",NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"State_Ji,",10.1080/10584609.2021.1960451,Political Communication,2022,1,a,"To measure what sentiment a piece of news conveys, we adopt an audience-oriented
approach and employ supervised machine-learning techniques. To start, a sample of
2,000 news reports were randomly chosen from the corpus. Two research assistants
were recruited and trained as independent validators, whose task was to independently
classify each of the 2,000 news reports as positive, negative, or neutral, i.e., a multiclass classification with three categories. Their independent outputs were then compiled and compared; in instances where the two disagreed (i.e., approximately 1.5% of
the sample), we as authors made the final determination. The classification follows
a set of coding rules, among which three are particularly important: 1) news reports on
diplomatic visits by Chinese leaders abroad are coded as positive; 2) news reports
involving multiple countries, where the underlying sentiment for each is inconsistent,
are coded as neutral; and 3) action and behavior of international organizations, as
mentioned in a news story, should not affect the classification for individual countries
involved. Each of the 2,000 pieces of news was then coded with a value: −5 for
negative, 0 for neutral, and 5 for positive.
Using the manual coding results from the random sample, we test the performance of
four supervised machine learning methods, including Naive Bayes (Zhang, 2004), Support
Vector Machine (Wu et al., 2004), K-Nearest-Neighbor (Altman, 1992), and Multi-layer
Perceptron Classifier (MLPC) in neural networks (Cybenko, 1989; Rumelhart et al., 1986).
Based on the accuracy rates (see Table A2 in Appendix), we choose MLPC for its superior
performance to code the remaining body of news",hand-coding,External: Human Annotated Scores,No
2022-11-28T10:50:36Z,"The Validity of Sentiment Analysis: Comparing Manual Annotation, Crowd-Coding, Dictionary Approaches, and Machine Learning Algorithms","van Atteveldt, Wouter; van der Velden, Mariken A. C. G.; Boukes, Mark","Yes, Continue with Coding",NA,Newspaper,Others,Dutch,sentiment,Sentiment/Emotion,"Supervised: Machine Learning, Rule-based: Adjusted dictionary","Affective Norms for English Words (AFINN),Augmented General Inquirer4, dictionary from Hu and Liu, Loughran and McDonald Sentiment Word Lists",NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,The_van,10.1080/19312458.2020.1869198,Communication Methods and Measures,2021,3,a,"Table 2 shows the overall performance of all tested methods, listed as percentage agreement (acc),
Krippendorff’s α for ordinal measures, as well as the precision, recall, and F1 score for both positive,
neutral, and negative sentiment. These latter scores are given since they are the standard for performance evaluation in machine learning, but they also give more insight into the type of error made by an algorithm. For example, a dictionary with only a few very clear sentiment words can have high precision but low recall, meaning that when it identifies a document as positive or negative it is generally correct (precision), but that it misses a lot of the documents that were actually positive or negative (recall) because these documents did not contain the words in the dictionary. The F1 score is the harmonic mean between precision and recall and will generally be closest to the lower of the two scores. Since Krippendorff’s α is the most commonly used measure for the validity of text analysis in communication science, we will mostly report performance using that measure, but overall accuracy, α, and F1 scores show the same pattern for all cases.
Starting with the top rows of Table 2, manual coding (using undergraduate students) yields the best results, with both accuracy and alpha above .8. Particularly, using three different students as manual annotators in combination with a majority vote to determine the final sentiment of the sentence achieves the highest performance scores, although this is of course expensive for large scale projects.
A good second place is taken by crowd coding – rows 3 to 5 in Table 2. Using just one crowd coder already yields a reasonably good performance (α ≥ 0.75). This can be further improved by applying a majority vote decision when using multiple crowd coders (an increase to α ≥ 0.8).
Moving from manual annotation to the realm where “computers do the work”, Table 2 demon
strates that machine learning performs worse than both students’ manual coding and crowd coding. Reaching α = 0.50 for deep learning (CNN) and slightly worse for classical machine learning (SVM; α = 0.41, NB; α = 0.40), machine learning still performs significantly better than chance. However, since these results are lower than generally accepted levels of inter-coder reliability, these models cannot directly be used for substantive analyses.",hand-coding,External: Human Annotated Scores,Yes
2022-11-28T10:50:36Z,"The Validity of Sentiment Analysis: Comparing Manual Annotation, Crowd-Coding, Dictionary Approaches, and Machine Learning Algorithms","van Atteveldt, Wouter; van der Velden, Mariken A. C. G.; Boukes, Mark","Yes, Continue with Coding",NA,Newspaper,Others,Dutch,sentiment,Sentiment/Emotion,"Supervised: Machine Learning, Rule-based: Adjusted dictionary","Affective Norms for English Words (AFINN),Augmented General Inquirer4, dictionary from Hu and Liu, Loughran and McDonald Sentiment Word Lists",NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,The_van,10.1080/19312458.2020.1869198,Communication Methods and Measures,2021,3,b,"Table 3 demonstrates the correlations between the Dutch and English dictionaries applied in this
paper. The cell shading in the table indicates the strength of the correlation. Most correlations
between the tested Dutch dictionaries are weak to moderate, ranging from 0.00 (Pattern and NRC)
and 0.40 (Muddiman approach and NRC). The dictionaries’ correlation with the gold standard is
even lower – varying between 0.12 for Pattern and 0.33 for NRC.
Table 3 also shows the correlations among the English dictionaries – indicated by E in Table 3 –
and between the Dutch and English versions dictionaries. Again, the correlations with the gold
standard are low, varying between 0.11 for RID (Martindale, 1975, 1990) and 0.34 for both Hu and
Liu (2004) and the Lexicoder Sentiment Dictionary (LSD) of Young and Soroka (2012a, 2012b).
Interestingly, correlations between the English dictionaries are substantially higher than for the
Dutch dictionaries. It remains to be seen whether this is because the dictionaries we used are indeed
more similar, or whether the translation introduced artifacts.
Overall, this table shows that the various dictionaries all measure something else, and none of
them can be seen as a valid measurement of the sentiment as defined in our gold standard.",correlations between different dictionatries,External: Scores from other CATM,Yes
2022-11-28T10:50:36Z,"The Validity of Sentiment Analysis: Comparing Manual Annotation, Crowd-Coding, Dictionary Approaches, and Machine Learning Algorithms","van Atteveldt, Wouter; van der Velden, Mariken A. C. G.; Boukes, Mark","Yes, Continue with Coding",NA,Newspaper,Others,Dutch,sentiment,Sentiment/Emotion,"Supervised: Machine Learning, Rule-based: Adjusted dictionary","Affective Norms for English Words (AFINN),Augmented General Inquirer4, dictionary from Hu and Liu, Loughran and McDonald Sentiment Word Lists",NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,The_van,10.1080/19312458.2020.1869198,Communication Methods and Measures,2021,3,c,"Error analysis
We conducted an error analysis to improve our understanding of the mistakes made by the various
automatic methods.7 For the error analysis on the off-the-shelf dictionaries, we picked the NRC
dictionary, the best performing Dutch dictionary (by alpha) that also has an English translation.
Many of the mistakes made in the Dutch NRC dictionary are missed negations. For example
positively classifying the word groei (growth) in sentences like afnemende groei hypotheken (reduced
growth in mortgages) or matige groei (tepid growth) or negatively classifying the word crisis in
Cyprus heft laatste restricties op na crisis (Cyprus lifts last restrictions after the crisis). In addition, the
error analysis revealed clear mistakes, such as misclassifying the word beurs (stock exchange) as
positive.
The English NRC dictionary applied to headlines translated by deepl shows a similar pattern, with
the words job and savings being misclassified as positive in sentences such as Best Buy deletes 1500
jobs and 300 million in savings gone in one day. The same happened to negative words like crisis and
inflation. To better understand the role of translation, we then looked at the sentences that were
correctly classified in Dutch but missed by the translated version. An inspection of these (n= 54)
sentences suggests that, although some are translated a bit clumsily, this does not seem to be the
cause of the errors as the translation errors are more concerned with function words rather than the
sentiment carrying words (for example, 300 million in savings gone in one day was actually translated
as 300 million in savings in one day away). An interesting detail is that the word interest caused most
of the translated misclassifications as the English word interest is ambiguous between the supposedly neutral meaning of interest rate and the positive concept of being interested or interesting, while the
Dutch translation (rente) is not ambiguous.
For the error analysis of our machine learning approaches, we first look at Naive Bayes as that
method has the most interpretable feature weights. Interestingly, the words faillissement (bankruptcy)
and werkloosheid (unemployment) were positive features, presumably because these words
are often negated in the training documents. Being based on word frequencies (bag of words), Naive
Bayes also suffers from the same lack of context as the dictionaries, for example classifying the
sentences minder woningen onder water (fewer underwater mortgages) as negative based on negative
values for both fewer and underwater, even though the result of the former is actually to negate the
latter.
The convolutional neural network can take word context into account, but unfortunately the
more complex parameters are not easy to inspect manually. When inspecting the misclassified
sentences manually, however, it turns out to make some of the same mistakes as the other methods,
for example, classifying more bankruptcies as positive. For many other sentences it is less clear why
they are misclassified, although they do seem to have a large amount of rare or new words such as
Werkgevers torpederen caos (employers torpedo collective-bargaining-agreements), dubbelfout bij
crisistaks (double-error with crisis-tax) and Grieken zijn weer platzak (Greeks are stone-broke
again). Then again, all these words were in the word embeddings and most had sensible synonyms,
for example, listing werkgeversheffing (employer charge) and graaitax (grabber tax) as synonyms for
crisistaks, although caos was misclassified (presumably due to the failed lemmatization to the
singular form CAO) and dubbelfout (double error) was related mostly to tennis terms. Still, since
the close synonyms of these rare words are mostly rare words themselves, it is possible that even
though the embeddings vectors does words not in the training data to be used for classification, if the
words occur in too few contexts in the documents used for creating the embeddings they will still
cause difficulties for the algorithm.",error analysis,Content Validation,No
2022-11-29T15:42:09Z,"Channeling Hearts and Minds: Advocacy Organizations, Cognitive-Emotional Currents, and Public Conversation","Bail, Christopher A.; Brown, Taylor W.; Mann, Marcus","Yes, Continue with Coding",NA,Social Media: Facebook,English,NA,cognitive and emotional language,Sentiment/Emotion,Rule-based: Off-the-shelf dictionary,LIWC,NA,No,NA,NA,Yes,Yes,No,lukas.birkenmaier@outlook.de,"Channeling_Bail,",10.1177/0003122417733673,American Sociological Review,2017,2,a,"As
we describe in the online supplement, we
further validated the appropriateness of LIWC
for measuring cognitive and emotional conversational
styles by comparing its codes to
those of human coders.",hand-coding,External: Human Annotated Scores,NA
2022-11-29T15:42:09Z,"Channeling Hearts and Minds: Advocacy Organizations, Cognitive-Emotional Currents, and Public Conversation","Bail, Christopher A.; Brown, Taylor W.; Mann, Marcus","Yes, Continue with Coding",NA,Social Media: Facebook,English,NA,cognitive and emotional language,Sentiment/Emotion,Rule-based: Off-the-shelf dictionary,LIWC,NA,No,NA,NA,Yes,Yes,No,lukas.birkenmaier@outlook.de,"Channeling_Bail,",10.1177/0003122417733673,American Sociological Review,2017,2,b,"To further validate our findings, we also applied
Liu’s (2010) sentiment analysis detection algorithm
to our data. The correlation of the LIWC classification
of emotional texts in our data and the Liu
sentiment analysis method is .73 (p < .001). We
are unable to calculate Krippendorf’s alpha or",comparing it with other CTAM,External: Scores from other CATM,No
2022-11-30T11:21:27Z,Validating a sentiment dictionary for German political language—a workbench note,"Rauh, Christian","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,Others,German,sentiment,Sentiment/Emotion,Rule-based: Development dictionary,not named,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Validating_Rauh,",10.1080/19331681.2018.1485608,Journal of Information Technology & Politics,2018,2,a,"So, how do the sentiment dictionaries fare in this
environment? Let us first look at the distributions of
scores in Figure 1. Two observations are particularly
noteworthy. First, the univariate distributions on the
diagonal of the figure are extremely ‘peaked’ with
much more observations at the zero points than a
normal distribution would suggest. This is most pro-
nounced for SentiWS which results in a score of 0 in
666 of the 1,500 sentences (44.4%). But while its spread
is visibly larger, the strong tendency toward suppo-
sedly neutral scores also holds for the considerablydictionary with its more enhanced negation control
improves this on a minor scale: 474 of the sampled
sentences (31.6%) are classified as neutral with regard
to expressed sentiment.The second key observation is that scores from
the three dictionaries are positively and robustly
correlated as the combination of the linear (solid
line) and the more flexible locally smoothed fit
(dashed line) in the lower panels shows. Framed
positively, this finding indicates that all three seem
to measure a similar construct and that even the
rather small SentiWS dictionary already offers a
pragmatic resource to tap into this concept.
Framed negatively, in this challenging sample of
small coding units, a significantly larger dictionary
size and an enhanced negation control do little to
improve the results.11 In the sample of 1,500 sen-
tences from plenary speech in the German
Bundestag, the more advanced dictionaries change
the sentiment judgment only for a few cases.",comparison with other dictionaires,External: Scores from other CATM,Yes
2022-11-30T11:21:27Z,Validating a sentiment dictionary for German political language—a workbench note,"Rauh, Christian","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,Others,German,sentiment,Sentiment/Emotion,Rule-based: Development dictionary,not named,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Validating_Rauh,",10.1080/19331681.2018.1485608,Journal of Information Technology & Politics,2018,2,b,"With this in mind, I asked three human coders
to rate the 1,500 sampled sentences. The coders
were one female and two male political science
students, all German native speakers with no
prior experience in automated text analyses and
no specific information about the experiment’s
purposes. They accessed the individual sentences
in an author-written, browser-based survey tool
without further context (see screenshots in
Appendix A). It asked coders for the basic senti-
ment that they perceive in a given sentence, allow-
ing them to choose among ‘negative,’ ‘positive,’ or
‘neutral’ (with the latter option being the fallback
for possibly lazy coders). Each coder rated all 1,500
sentences. In order to avoid fatigue, the coders
could freely distribute the coding task along six
chunks of 250 sentences each over a time span of 3
days. Running order of the sentences was rando-
mized within and across these chunks to avoid
possible halo effects.","still counts as human coding, even tough the goal here is to stress that human coding is not without erros",External: Human Annotated Scores,No
2022-11-30T15:12:15Z,Positive sentiments as coping mechanisms and path to resilience: the case of Qatar blockade,"El-Masri, Mazen; Ramsay, Allan; Ahmed, Hanady Mansour; Ahmad, Tariq","Yes, Continue with Coding",NA,Social Media: Twitter,Others,Arabic,emotion,Sentiment/Emotion,Supervised: Machine Learning,Weighted Conditional Probability. SVM,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Positive_El-Masri,",10.1080/1369118X.2020.1748086,"Information, Communication & Society",2021,2,a,"To validate the results, we looked for spikes in positive sentiments and we mapped them to
the most relevant news on Qatar of that day. In order to identify the most relevant news,
we looked at google news (news.google.com) and used google sorting feature to identifythe most important blockade event of the day based on news relevance. We also used Alja-
zeera’s dedicated page on events related to the blockade to identify the main events of the
day. Our results showed that the spikes in sentiments that our algorithm identiﬁed from
the collected tweets corresponded to signiﬁcant blockade-related events, simultaneously
conﬁrming that the results obtained by controlled testing in the SemEval experiments
transfer eﬀectively to real world scenarios and allowing us to probe the way that people
reacted to these events.",face validity,Content Validation,Yes
2022-11-30T15:12:15Z,Positive sentiments as coping mechanisms and path to resilience: the case of Qatar blockade,"El-Masri, Mazen; Ramsay, Allan; Ahmed, Hanady Mansour; Ahmad, Tariq","Yes, Continue with Coding",NA,Social Media: Twitter,Others,Arabic,emotion,Sentiment/Emotion,Supervised: Machine Learning,Weighted Conditional Probability. SVM,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Positive_El-Masri,",10.1080/1369118X.2020.1748086,"Information, Communication & Society",2021,2,b,"In order to ensure that the sentiments that Qatari residents exhib-
ited during these periods were indeed related to the blockade, we extracted a random sub-
set of 10% of the sentimental tweets of the events’ dates and the two following days and
asked two university students to label them as either blockade-related or not. The result
of this qualitative analysis revealed that the majority (approximately 89%) of the analysed
tweets were related to the blockade. We also evaluated the consistency of the labelling that
was conducted by the two students using the inter-observer agreement Kappa measure
(see Landis & Koch, 1977). The Kappa value was 0.96 indicating a high level of labelling
consistency as per Landis and Koch (1977).",qualitative coding,External: Human Annotated Scores,No
2022-11-30T15:40:55Z,Can social media reveal the preferences of voters? A comparison between sentiment analysis and traditional opinion polls,"Oliveira, Daniel José Silva; Bermejo, Paulo Henrique de Souza; dos Santos, Pâmela Aparecida","Yes, Continue with Coding",NA,Social Media: Twitter,Others,Portuguese,sentiment,Sentiment/Emotion,API,DiscoverText,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Can_Oliveira,",10.1080/19331681.2016.1214094,Journal of Information Technology & Politics,2017,1,a,"fter the classification, the results were validated
by calculating how accurate they were (Collis &
Hussey, 2005; Oliveira et al., 2015). The accuracy
was calculated based on a new random sample of 30% of the data set, considering only the automati-
cally classified tweets, which divided the number of
correct classifications by the total of each sample
(Eirinaki et al., 2012; Oliveira et al., 2015). The results
of the accuracy calculation are shown in Table 4.
The precision calculation revealed that the auto-
mated tweet classification was 81.05% of the mean
score. These values were below the precision dis-
played by the Datafolha (2014) survey, which was
95% precise.",prediction of test set (hand coded),External: Human Annotated Scores,No
2022-11-30T16:48:38Z,Does use of emotion increase donations and volunteers for nonprofits?,"Paxton, Pamela; Velasco, Kristopher; Ressler, Robert W.","Yes, Continue with Coding",NA,nonprofits mission statements in tax filings,English,NA,Emotionality,Sentiment/Emotion,Rule-based: Off-the-shelf dictionary,LIWC,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Does_Paxton,",NA,American Sociological Review,2020,1,a,"Positive includes 
comfort, dignity, and well-being. To illustrate 
this, in Table 1 we highlight two paired mis-
sions of organizations that are working on the 
same issue—(1) support for individuals experiencing neglect and abuse, and (2) free 
food distribution programs—but that discuss 
their work in opposing ways. Words that 
LIWC categorized as positive and negative 
are bolded.",evaluation of sentences,Content Validation,No
2022-11-30T17:27:49Z,What’s the tone? Easy doesn’t do it: Analyzing performance and agreement between off-the-shelf sentiment analysis tools,"Boukes, Mark; Van de Velde, Bob; Araujo, Theo; Vliegenthart, Rens","Yes, Continue with Coding",NA,Newspaper,Others,Dutch,sentiment,Sentiment/Emotion,"Rule-based: Off-the-shelf dictionary, Rule-based: Adjusted dictionary",mutliple dictionaries,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"What’s_Boukes,",NA,Communication Methods and Measures,2020,3,a,"These findings for headlines and full texts are replicated in analyses that use correlations instead
of alphas; these results can be found in the analysis of different levels of granularity (Table 6). The
agreement between manual coding and automated sentiment analysis tools in detecting the tone of
economic news, thus, is unacceptably low following the standards normally imposed on manual
content analysis. Answering Research Question 1: The best results are found for LIWC and Polyglot",comparison hand coding,External: Human Annotated Scores,Yes
2022-11-30T17:27:49Z,What’s the tone? Easy doesn’t do it: Analyzing performance and agreement between off-the-shelf sentiment analysis tools,"Boukes, Mark; Van de Velde, Bob; Araujo, Theo; Vliegenthart, Rens","Yes, Continue with Coding",NA,Newspaper,Others,Dutch,sentiment,Sentiment/Emotion,"Rule-based: Off-the-shelf dictionary, Rule-based: Adjusted dictionary",mutliple dictionaries,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"What’s_Boukes,",NA,Communication Methods and Measures,2020,3,b,"This brings us to Research Question 2: How strongly do automatic measurements of tone agree
amongst each other? Table 4 displays the agreement in terms of Krippendorff’s α-scores between the
different automated sentiment analysis tools for both the headlines (above) and full-text bodies
(below); Appendix A shows that the results are very similar when using pair-wise correlation
analysis. One will notice that the agreement between automated approaches, overall, is surprisingly
low – yet, they all (except for “recession” on full texts) point in the positive direction. It is
remarkable that most automatic approaches yield largely unrelated measurements of tone. The
overlap between these methods is weak in all instances. The only methods that relatively agree the
most are LIWC, Polyglot, and DANEW (.28 ≤α ≤.34 for the headline; .30 ≤α ≤.36 for the full text),
whereas the other combinations mostly yield completely unrelated tone assessments between each
other. This implies that sentiment tools often assess the tone of articles differently, most likely due to
non-overlapping lexicons that underlie their classification, and different contexts and applications
for which they were originally developed. Looking at correlations instead of α-values, we find very
comparable results (see Appendix A).
Remarkable is the lack of agreement between most approaches. For example, SentiStrength and
Pattern (α = .09 for headline, and α = .07 for full text) find almost completely unrelated sentiment
values, and similarly so for SentiStrength and LIWC (α = .19 for headline, and α = .17 for full text).
Interesting, moreover, is the comparably high (but still low) agreement between LIWC and the 65-",comparison of different dictionaries,External: Scores from other CATM,Yes
2022-11-30T17:27:49Z,What’s the tone? Easy doesn’t do it: Analyzing performance and agreement between off-the-shelf sentiment analysis tools,"Boukes, Mark; Van de Velde, Bob; Araujo, Theo; Vliegenthart, Rens","Yes, Continue with Coding",NA,Newspaper,Others,Dutch,sentiment,Sentiment/Emotion,"Rule-based: Off-the-shelf dictionary, Rule-based: Adjusted dictionary",mutliple dictionaries,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"What’s_Boukes,",NA,Communication Methods and Measures,2020,3,c,"Most scientific studies that investigate newspaper reporting on economic developments are not
interested in the tone of individual articles, but rather examine the aggregated sentiment at the daily,
weekly or sometimes even monthly level. Higher granularities are better suited for time-series
analysis, as reporting may be sparse and noisy at low temporal granularities. In Table 6, we present
the correlations of different tools at the article-level, daily-level, and weekly-level, with scores as
mean-aggregations of individual article scores. Logically, a greater aggregation comes together with loss of statistical power (i.e., fewer units-of-observation). On full-text content, only Damstra &
Boukes, LIWC, and Polyglot have statistically significant correlations at the weekly level (n = 22). On
headlines, only LIWC and Pattern achieved this distinction.
Most sentiment analysis tools have a stronger correlation with the manually coded tone on
coarser granularities (day and week); exceptions are Pattern and DANEW for the full texts. Most
obvious are the improvements of LIWC and the dictionary approach of Damstra & Boukes: Both
correlated moderately for the full text on the article level (respectively, r = .37 and r = .26) and this
improves to a considerable r ≈ .60 for the weekly level. LIWC showed a similar increase for the
headlines (from r= .26 to r= .60); this did not occur for the Damstra & Boukes approach, arguably,
due to the low agreement with manually coded headlines to begin with (see α-results presented
earlier). Thus, the agreement with manual annotations needs to be relatively high, in the first place,
for data aggregation to cause a better performance (also see the findings of Pattern for the full text).
Clear improvements are also observed for the combined approach, Polyglot, as well as Pattern
(headlines only). This shows that the validity of some (i.e., not all) approaches becomes better
when aggregating the data. H3, thus, is largely confirmed.",rerunning the analysis with differente aggregation,Unsure,No
2022-11-30T17:42:13Z,What’s in a post? How sentiment and issue salience affect users’ emotional reactions on Facebook,"Eberl, Jakob-Moritz; Tolochko, Petro; Jost, Pablo; Heidenreich, Tobias; Boomgaarden, Hajo G.","Yes, Continue with Coding",NA,Social Media: Facebook,Others,German,emotion,Sentiment/Emotion,Rule-based: Adjusted dictionary,SentiWS,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"What’s_Eberl,",NA,Journal of Information Technology & Politics,2020,2,a,"While this dictionary is vali-
dated for German-language political texts, we
compared its results to two other German diction-
aries, namely the German SentiStrength (Pirker,
2012; Thelwall, Buckley, Paltoglou, Cai, &
Kappas, 2010) and a negativity dictionary
(Rudkowsky et al., 2018).",coparison with other dictionary,External: Scores from other CATM,Yes
2022-11-30T17:42:13Z,What’s in a post? How sentiment and issue salience affect users’ emotional reactions on Facebook,"Eberl, Jakob-Moritz; Tolochko, Petro; Jost, Pablo; Heidenreich, Tobias; Boomgaarden, Hajo G.","Yes, Continue with Coding",NA,Social Media: Facebook,Others,German,emotion,Sentiment/Emotion,Rule-based: Adjusted dictionary,SentiWS,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"What’s_Eberl,",NA,Journal of Information Technology & Politics,2020,2,b,"Moreover, we compared
the outputted sentiment scores with the manually
coded scores of a subsample of 450 randomly
selected Facebook posts, resulting in an acceptable
correlation between both of r = 0.62, which is on
par with other studies in the field.7",hand-coding,External: Human Annotated Scores,No
2022-11-10T21:16:01Z,Presenting News on Social Media: Media logic in the communication style of newspapers on Facebook,"Welbers, Kasper; Opgenhaffen, Michaël","Yes, Continue with Coding",NA,Newspaper,Others,"Belgian, Dutch","subjectivity, tone",Subjectivity,Rule-based: Off-the-shelf dictionary,Dutch adjectives developed by De Smedt and Daelemans (2012),NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Presenting_Welbers,",10.1080/21670811.2018.1493939,Digital Journalism,2019,2,a,"The lexicon is based on 1,100 frequent adjectives that were manually annotated, and was expanded to 5,500 words using established machine learning techniques for dictionary expansion. Tested on a corpus of Dutch book reviews, the accuracy of the polarity is 82 per cent, with a precision of 0.80, a recall of 0.86, and an F1 score of 0.83.4 On our corpus, consisting of much shorter messages in a different domain, the accuracy will be lower for individual cases, but these scores indicate that the lexicon is suitable for our aggregate level analysis of the use of subjective adjectives.",Argumentation based on prior validation,Unsure,Yes
2022-11-10T21:16:01Z,Presenting News on Social Media: Media logic in the communication style of newspapers on Facebook,"Welbers, Kasper; Opgenhaffen, Michaël","Yes, Continue with Coding",NA,Newspaper,Others,"Belgian, Dutch","subjectivity, tone",Subjectivity,Rule-based: Off-the-shelf dictionary,Dutch adjectives developed by De Smedt and Daelemans (2012),NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Presenting_Welbers,",10.1080/21670811.2018.1493939,Digital Journalism,2019,2,b,"As illustration, Table 1 shows the results of the tool for several example sentences. The first two sentences describe a negative event, but the first sentence only states the facts, whereas the second sentence contains the adjective horrible, which is a negative subjective evaluation. Thus the first sentence is considered neutral, whereas the second is very subjective (1.00) and very negative (1.00). In the third sentence we see a combination of one positive and two negative words, which causes a moderate negative polarity (–0.66). Sentences four to eight show how different words have different intensities, and how negations (“not”) and intensifiers (“very”, exclamation marks) affect results.",evaluation of example sentences (weak validation effort),Content Validation,No
2022-09-06T15:04:06Z,Measuring Agenda Setting in Interactive Political Communication,"Rossiter, Erin L.","Yes, Continue with Coding",NA,"Party Politics: Others, Presidential Debates, online discussion (mturk), In-Person Deliberations",English,NA,Shift in Agenda Setting,Text Attributes,"Unsupervised: Topic Modeling, Unsupervised: Topic Segmentation","LDA, parametric Speaker Identity for Topic Segmentation (SITS)",NA,Yes,Topic Segmentation model builds up on LDA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Rossiter,",10.1111/ajps.12653,American Journal of Political Science,2022,4,a,"To assess if SITS can accurately identify where latent shifts in topic occur, I classify a speaking turn as shifting topic if the posterior probability of a shift is greater than or equal to 0.50. I then compare where shifts were inferred by SITS to where topic changes were prompted by the researchers. SITS identifies 81.40% of the locations that begin a new prompted topic segment by the researcher.7 I check for an SITS-inferred topic shift within two speaking turns after a researcher-prompted shift because often after reading the prompt, the first few speaking turns would simply answer the prompt’s question by saying “yes,” “no,” or “do you want to go first?” Instead of classifying this as a topic shift, SITS would classify a subsequent speaking turn that actually began discussing the topic at hand as a shift. This exercise demonstrates that SITS can accurately identify the speaking turns that should be attributed as shifting the agenda.8 Moreover, SITS provides a more nuanced view of how topics ebbed and flowed in these discussions than if we considered the locations of researcher-prompted topic changes as ground truth.",Comparison latent shifts with gold-standard coding,External: Human Annotated Scores,Yes
2022-09-06T15:04:06Z,Measuring Agenda Setting in Interactive Political Communication,"Rossiter, Erin L.","Yes, Continue with Coding",NA,"Party Politics: Others, Presidential Debates, online discussion (mturk), In-Person Deliberations",English,NA,Shift in Agenda Setting,Text Attributes,"Unsupervised: Topic Modeling, Unsupervised: Topic Segmentation","LDA, parametric Speaker Identity for Topic Segmentation (SITS)",NA,Yes,Topic Segmentation model builds up on LDA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Rossiter,",10.1111/ajps.12653,American Journal of Political Science,2022,4,b,"Influential work in computer science proposes that crowdsourced tasks are more useful than traditional metrics to assess if a topic model returns semantically meaningful and distinct topics (Chang et al. 2009). Therefore, I use the “topic intrusion” task proposed by Chang et al. (2009) to validate the topics from a SITS model estimated on 20 U.S. general election presidential debates held between 1992–2016.9
The topic intrusion task presents the human judge with a document, in this case a segment inferred by SITS. The judge is also presented with four word sets. Three of these word sets represent the three highest probability topics for the segment. The fourth word set is the intruder, drawn randomly from the segment’s low probability topics. Each word set contains the top eight frequent and exclusive (FREX) topwords for the topic (Roberts et al. 2014). I set up the topic intrusion task for 200 randomly drawn segments from the debates. Then, Amazon Mechanical Turk (MTurk) workers were asked to choose which word set was most unrelated to the passage. In line with recent work on validation procedures for topic models by Ying, Montgomery, and Stewart (2019), I ran two trials of the same 200 tasks. Figure 2 plots the results for each trial separately as well as the pooled result Workers competed 62% and 68% of the tasks correctly in Trial 1 and Trial 2, respectively. A difference of proportions test indicates that Trial 1 and Trial 2 are not significantly different (p = 0.25).Thisresultiscomparable to one, and better than three, of four models assessed by Ying, Montgomery, and Stewart (2019) using the topic intrusion task. In all, human coders and SITS largely agree about which topics are and are not associated with the inferred segments of the debates.
",topic intrusion from model topics,Content Validation,Yes
2022-09-06T15:04:06Z,Measuring Agenda Setting in Interactive Political Communication,"Rossiter, Erin L.","Yes, Continue with Coding",NA,"Party Politics: Others, Presidential Debates, online discussion (mturk), In-Person Deliberations",English,NA,Shift in Agenda Setting,Text Attributes,"Unsupervised: Topic Modeling, Unsupervised: Topic Segmentation","LDA, parametric Speaker Identity for Topic Segmentation (SITS)",NA,Yes,Topic Segmentation model builds up on LDA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Rossiter,",10.1111/ajps.12653,American Journal of Political Science,2022,4,c,"To do so, I follow a procedure proposed by Grimmer and King (2011) to evaluate “cluster quality,” which is the similarity of the documents (here, segments of the debates) estimated to belong to the same cluster (here, having similar topic distributions). Importantly, I evaluate SITS segments against segments derived from a hand-coding approach. Hand-coded data are from Boydstun, Glazier, and Pietryka (2013) for the 1992, 2004, and 2008 U.S. general election presidential debates. Boydstun, Glazier, and Pietryka hand code several variables from the debate transcripts, including the topic of each question posed to the candidates and the topic of each phrase in the candidates’ responses. Then, they deem a candidate as going “off-topic” and thus, engaging in agenda-setting behavior, if the phrase’s topic does not correspond to the question’s topic. Comparing the segments inferred by SITS to those derived from hand coding required five steps. First, I determined where topic changes occurred (and thus, formed segments of the debates) according to each method. Second, I determined the similarity of these segments according to each method’s topic assignments.10 Third, I set up the exercise outlined by Grimmer and King (2011). Separately with the segments from the SITS and the hand-coding approaches, I drew 25 random pairs of segments with the same most-assigned topic and 25 random pairs of segments with a different most-assigned topic.11 Fourth, four unique MTurk workers rated the similarity of the segments within each pair on a 3-point scale: (1) unrelated, (2) loosely related, or (3) closely related.12 Of interest is each method’s “cluster quality,” which is “the average similarity of pairs of documents from the same cluster minus the average similarity of pairs of documents from different clusters, as judged by human coders one pair at a time” (Grimmer and King 2011, p. 5). The fourth and final step is to use difference in means to compare the two methods. Figure 3 plots this point estimate along with the 80% (thick line) and 95% (thin line) confidence interval. The estimated difference between approaches is positive and significant. Therefore, the Grimmer and King (2011) evaluation suggests SITS can infer segments that are even more coherent than those derived from a hand-coding approach.","assessing the interrelated nature of where shifts in topic occur and the topics themselves by examining the resulting segments of an interaction. Using crowdsourced human judgments, I find that SITS segments are viewed as more coherent than segments derived from a hand-coding approach.",External: Human Annotated Scores,Yes
2022-09-06T15:04:06Z,Measuring Agenda Setting in Interactive Political Communication,"Rossiter, Erin L.","Yes, Continue with Coding",NA,"Party Politics: Others, Presidential Debates, online discussion (mturk), In-Person Deliberations",English,NA,Shift in Agenda Setting,Text Attributes,"Unsupervised: Topic Modeling, Unsupervised: Topic Segmentation","LDA, parametric Speaker Identity for Topic Segmentation (SITS)",NA,Yes,Topic Segmentation model builds up on LDA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Measuring_Rossiter,",10.1111/ajps.12653,American Journal of Political Science,2022,4,d,"In particular, the results for 1992, 2004, and 2008 are in line with results from the hand-coded content analysis. Boydstun, Glazier, and Pietryka (2013) note that in 1992 and 2008, the economy was a salient issue, but in 2004, defense was uniquely more important to the public than the economy. These patterns hold in Figure 4, as we see the 2004 election discussed the economy less than any other election",Face Validation by drawing on knowledge on the salient political issues,Content Validation,No
2022-09-05T15:26:00Z,Estimating Spatial Preferences from Votes and Text,"Kim, In Song; Londregan, John; Ratkovic, Marc","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,Speech Dimension,Text Attributes,"New statistical model, most close to text scaling",Sparse Factor Analysis,NA,Yes,Combining text and votes for estimating spatial locations of MPs,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Estimating_Kim,",10.1017/pan.2018.7,Political Analysis,2018,4,a,"Before applying the method, let’s consider the applicability of SFA in this case. First, voting is not always sincere in the US Senate, as there are always motions to recommit, etc. We note, though, that ideal point estimates from the US Congress have been used extensively in other studies and possess high face validity. To be particularly careful, if a bill is voted on several times due to different motions, we only include the final vote in our analysis. Second, we consider the sincerity when speaking. Previous work has shown, albeit in the US House, that floor speeches are expressive rather than deliberative (Hill and Hurley, 2002; Maltzman and Sigelman, 1996). Many floor speeches aren’t even read verbally, but simply entered into the record, also suggesting that floor speeches are vehicles of expression rather than persuasion. For that reason, we feel more comfortable applying the method to floor speeches rather than, say, conference committee meetings",Considerations Applicability Data and Method,Content Validation,Yes
2022-09-05T15:26:00Z,Estimating Spatial Preferences from Votes and Text,"Kim, In Song; Londregan, John; Ratkovic, Marc","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,Speech Dimension,Text Attributes,"New statistical model, most close to text scaling",Sparse Factor Analysis,NA,Yes,Combining text and votes for estimating spatial locations of MPs,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Estimating_Kim,",10.1017/pan.2018.7,Political Analysis,2018,4,b,"The left figure contains results from the 108th Senate, a Republican-led session during President George W. Bush’s tenure. The right figure contains results from the 112th Senate, a Democratic-led session during President Barack Obama’s time as President. We find a consistent pattern: for the majority party, the most extreme terms relate to parliamentary control words (consent committee, author meet, meet session). For the minority party, the first dimension identifies ideologically relevant terms. For the Democrats during the 108th Senate, these terms included administr, as the Democrats soured on the current Presidential administration, and health, a centerpiece of the Democratic policy agenda. In the 112th Senate, with the Democrats in the majority, parliamentary control terms switched their ideological polarity, aligning with the Democrats ( meet session, consent committee, author meet). The Republican end of this first dimension reflects that party’s programmatic fiscal concerns (budget, stimulus, debt, trillion).",Inspection of most relevant words,Unsure,Yes
2022-09-05T15:26:00Z,Estimating Spatial Preferences from Votes and Text,"Kim, In Song; Londregan, John; Ratkovic, Marc","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,Speech Dimension,Text Attributes,"New statistical model, most close to text scaling",Sparse Factor Analysis,NA,Yes,Combining text and votes for estimating spatial locations of MPs,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Estimating_Kim,",10.1017/pan.2018.7,Political Analysis,2018,4,c,"Next, we look at the preferred outcomes of legislators from the 112th. Points in Figure 4 are shaded in proportion to their first dimensional DW-NOMINATE score, showing the agreement between SFA and DW-NOMINATE on the first dimension (p ρ « 0.93). The left plot labels party leaders, whips, and top chairmen, showing the close relationship between locations on the second dimension and leadership. The first dimension captures the political battle lines, reflecting legislators left vs right policy differences, while the second, vertical, dimension reflects differences in the terms selected by leaders versus the rank and file members.",Comparison with DW-Nominate Scores for vote choice (https://en.wikipedia.org/wiki/NOMINATE_(scaling_method)),External: Criterion data / Predictive validation,Yes
2022-09-05T15:26:00Z,Estimating Spatial Preferences from Votes and Text,"Kim, In Song; Londregan, John; Ratkovic, Marc","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,Speech Dimension,Text Attributes,"New statistical model, most close to text scaling",Sparse Factor Analysis,NA,Yes,Combining text and votes for estimating spatial locations of MPs,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Estimating_Kim,",10.1017/pan.2018.7,Political Analysis,2018,4,d,"Scaling results informed by only words (α “ 1). We also apply SFA using only information from words. This is not our preferred model, as it ignores vote data, yet SFA still uncovers structure in the text data. [...] Figure 6 contains the top ten words at each of the first six dimensions of the 112th Senate. We note that the positive and negative level distinction along the y-axis is wholly arbitrary, as we only identify term levels up to a sign. Looking at the first column, we find that the first dimension starts with a set of non-controversial terms. These include parliamentary procedural terms (as opposed to parliamentary control terms) such as today wish, madam rise, and colleague support. Also on the non-controversial side are martial terms with universally positive affect during this Congress such as army, air forc, and deploy. On the other side are words that will be used in to differentiate issues in other dimensions, such as tax, vote, and peopl. The other dimensions have at their extremes words connoting some underlying dimension of policy. For example, the second dimension ranges from judiciary and women’s issues at one end to fiscal concerns at the other; the fourth goes from a broad set of social welfare concerns to the consideration of judicial nominees. These lower dimensions adapt to the issues of the day. Tobacco, for example is present in the 105th Senate; Iraq comes and goes as an issue, and health care goes from dealing with seniors and Medicare in the 107th Senate to dealing with students and families in the 112th. Even without including votes in",Visual inspection of word wheights in a scaling model,Content Validation,No
2022-09-12T15:47:26Z,"A General Model of Author “Style” with Application to the UK House of Commons, 1935–2018","Huang, Leslie; Perry, Patrick O.; Spirling, Arthur","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,distinctiveness in speech,Text Attributes,Unsupervised: Statistical Model based on Word Frequencies,not named,NA,No,NA,Yes,Yes,No,No,lukas.birkenmaier@outlook.de,"A_Huang,",10.1017/pan.2019.49,Political Analysis,2020,4,a,"One basic requirement is that our method ought to label “intrusive” texts, i.e. ones that were not clearly produced by the parliamentary data generating process, as distinctive. To put this to the test, we took the set of backbenchers from a modern session (2015–2016) added to them the State of the Union speeches given by U.S. President Barack Obama. We randomly sampled n “speeches” of m sentences each from the SotU speeches, where n is the mean speeches per MP and m is the mean speech length in sentences for the 2015 session. Aer inserting Obama in the corpus, we select the vocabulary using our standard cross-validation procedure. We used Obama because although his works are approximately contemporaneous with our data, his style is distinctive relative to our MPs: they come from an American rather than British political system, and they are long oratories consumed by the general public rather than speeches directed primarily at other politicians. With that in mind, our model should identify Obama as easily the most distinctive author. As we see from Figure 1, this is indeed the case: Obama’s ƒt point estimate is by far the largest in the data and appears at the far top right. Its confidence interval does not overlap with any other MP in the distribution (note that we do not include every MP in the graphic due to limited space, but we do include the full range in terms of distinctiveness estimates). Of course, such a test might be cherry-picking, and there is no obvious baseline for performance (other than identifying the intruder).7 So we now turn to a domain-specific assessment.",Intrusion of outlier speeches,Content Validation,Yes
2022-09-12T15:47:26Z,"A General Model of Author “Style” with Application to the UK House of Commons, 1935–2018","Huang, Leslie; Perry, Patrick O.; Spirling, Arthur","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,distinctiveness in speech,Text Attributes,Unsupervised: Statistical Model based on Word Frequencies,not named,NA,No,NA,Yes,Yes,No,No,lukas.birkenmaier@outlook.de,"A_Huang,",10.1017/pan.2019.49,Political Analysis,2020,4,b,"For more substantive performance evaluation, we look at the “most distinctive” and “least distinctive” backbench MPs for the parliamentary sessions on either side of Blair’s election landslide in 1997 (that is, 1995–1996 and 1998–1999). This has the advantage of being a period in which (a) control of the Commons switched (from Conservative to Labour), meaning we have variation in the party of the backbenchers, and (b) there are academic accounts which help ground our understanding of MPs at this time (Cowley 2002; Spirling and Quinn 2010; Kellermann 2012). In terms of measurement, we use a convergent validity approach: we compare our measure to another (computed independently) and show that they are related as expected.

To see how we proceed in practice, note that for each MP t , in each session, we have an estimate of their distinctiveness in log-odds terms: our ƒ, above. For current purposes, however, we focus on something related but more concrete and directly interpretable: the proportion of their speeches which are correctly predicted as being from them relative to all other MPs (proportion of speeches correctly predicted, or “PCP,” in the tables below). We use fivefold cross-validation to fit a model to texts from a given session, predict the speakers of held-out texts using this model, and calculate each speaker’s rate of correct predictions; we report each speaker’s mean PCP. To validate these estimates, we consider their extrema—their minimums and maximums. In the subsection tables below, we list the twenty names of the MPs who were most distinct and least distinct by this measure (subject to having made a minimum of twenty speeches). We do this for the two sessions in question: one in 1995–1996 and one in 1998–1999. We also list the number of mentions of each MP in the Times newspaper archives (via Gale Group Digital Archive) for the same period, specifically the “Politics and Parliament” subsection of the “News.”","criterion validity (even tough they claim it to be convergent validity, but the comparison is with MP mentions in the TIME newspaper and thus an external criterion)",External: Criterion data / Predictive validation,Yes
2022-09-12T15:47:26Z,"A General Model of Author “Style” with Application to the UK House of Commons, 1935–2018","Huang, Leslie; Perry, Patrick O.; Spirling, Arthur","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,distinctiveness in speech,Text Attributes,Unsupervised: Statistical Model based on Word Frequencies,not named,NA,No,NA,Yes,Yes,No,No,lukas.birkenmaier@outlook.de,"A_Huang,",10.1017/pan.2019.49,Political Analysis,2020,4,c,"More substantively, we note the presence of several Labour “rebels” among the most distinct. These include Tony Benn, Diane Abbott, John McDonnell, Roger Berry, and Tam Dalyell, all of whom consistently voted against the Labour government’s plan to reform the welfare state.8 Peter Temple-Morris was a party switcher, and “interesting” for that reason—he was elected as a Tory MP in 1997, but then crossed the floor to Labour the same year. The most interesting MPs here include Stuart Bell, who was the Church Estates Commissioner, meaning that he was one of the managers of the Church of England’s property. David Hinchliffe, chairman of the Select Committee on Health, was subsequently extremely critical of the Blair government’s proposed reforms to the National",face validity of most extreme MPs,Content Validation,Yes
2022-09-12T15:47:26Z,"A General Model of Author “Style” with Application to the UK House of Commons, 1935–2018","Huang, Leslie; Perry, Patrick O.; Spirling, Arthur","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,distinctiveness in speech,Text Attributes,Unsupervised: Statistical Model based on Word Frequencies,not named,NA,No,NA,Yes,Yes,No,No,lukas.birkenmaier@outlook.de,"A_Huang,",10.1017/pan.2019.49,Political Analysis,2020,4,d,"Finally, with respect to a broader understanding of validity, we ask what exactly we are capturing as “distinctiveness” in our measure? As regards our comments at the opening of Section 2, is it mere “phrasing” (different ways of saying the same thing) or “substance” (saying something different)? Put more directly with respect to the extant literature, does our model “improve” (in fit terms at least) over the original Mosteller and Wallace (1963) approach and capture something more than function word usage?

To assess this, we conducted a simple experiment. We ran a special case of the estimation using only the seventy function words (i.e. stop words) from the original Mosteller and Wallace (1963) study. Our contention is that if our model is simply capturing idiosyncratic stylistic differences (in the narrow sense meant in that earlier literature), the restricted version should perform approximately as well as the more general one that uses all words in the vocabulary. Studying Figure 2, we see this is clearly false: there, the bottom line with triangle points is the mean prediction rate (for each speaker, with fivefold cross-validation) from the stop word model. The top line is the mean prediction rate from our model, which has no restrictions on stop words (as in the rest of this paper). It performs about three to five times as well as the pure phrasing model, on average. This implies that there is certainly something more than expressive manner going on: we happily refer to that residual variation as “substance.” This does not mean, of course, that the Mosteller and Wallace (1963) approach vocabulary is “wrong” (it is just a special case of ours), but it does suggest our model is doing something statistically useful in terms of capturing practical variation between contemporaneous MPs. Why do we see this performance difference? From inspection, we note that the fit improvement comes mostly from the middle of the distribution (that is, both our approach and the more simple one perform similarly for the most and least distinctive MPs but not for the median and mean—at least for the sessions we looked at in detail). We suspect this is because while almost everyone will have non-zero use of all of the Mosteller and Wallace (1963) words, our richer vocabulary has much higher variance in use. At the top of the distribution—MPs who are distinctive whatever the vocabulary—this makes no difference. Conversely, at the bottom of the distribution—MPs who use neither vocabulary very much—this also makes no difference. But for MPs in the middle, our much larger vocabulary offers more opportunities to distinguish oneself (for a fixed amount of speaking), and, thus, our model does better for these people. Before moving to the results, we note that readers may be qualitatively interested in the underlying tokens that affect distinctiveness of individuals in the model: in Appendix C, we discuss how these might be obtained and examined.",Evaluation of Model Outputs with small subset of data (only stopwords) to test model features,Content Validation,No
2022-11-08T12:38:53Z,Convergent news? A longitudinal study of similarity and dissimilarity in the domestic and global coverage of the Israeli-Palestinian conflict,"Baden, Christian; Tenenboim-Weinblatt, Keren","Yes, Continue with Coding",NA,Newspaper,Others,"Hebrew, Arabic, English, German",Text similiarity,Text Attributes,"Rule-based: Development dictionary, Evolutionary Factor Analysis","dictionary, Evolutionary Factor Analysis",NA,Yes,Mh identification of text similiarity using a dictionary and factor analysis on co-occurences of topics,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Convergent_Baden,",NA,Journal of Communication,2017,1,a,"The resulting dictionary contains between 6,500 and 10,500 search terms and more than 34,000 disambiguation criteria for each language, and achieved an average precision of 0.89 with an average recall of 0.94 (validation on 3000 sentences per language; no value below 0.76)",Dictionary: Accuracy,External: Human Annotated Scores,No
2022-11-30T14:05:51Z,A territorial dispute or an agenda war? A cross-national investigation of the network agenda-setting (NAS) model,"Su, Yan; Hu, Jun","Yes, Continue with Coding",NA,Social Media: Twitter,Others,"Mandarin, Japanese, English","Theme, Valence, Directionality",Text Attributes,Supervised: Machine Learning,SVM,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"A_Su,",10.1080/19331681.2020.1756553,Journal of Information Technology & Politics,2020,3,a,"es. Specifically, we first
divided the annotated samples into “training set”
(70%) and “testing set” (30%). We then used 70% of
the manually annotated samples (N = 2,099) to train
the SVM. To test the performance of the trained algo-
rithm, we predicted coded variables with the testing
set by the SVM and compared its result with the same
30% of the manually coded sample. As Table 1 shows,
all our SVM models achieved satisfactory
performance.",hand-coding,External: Human Annotated Scores,Yes
2022-11-30T14:05:51Z,A territorial dispute or an agenda war? A cross-national investigation of the network agenda-setting (NAS) model,"Su, Yan; Hu, Jun","Yes, Continue with Coding",NA,Social Media: Twitter,Others,"Mandarin, Japanese, English","Theme, Valence, Directionality",Text Attributes,Supervised: Machine Learning,SVM,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"A_Su,",10.1080/19331681.2020.1756553,Journal of Information Technology & Politics,2020,3,b,"To further confirm the SVM performance, we
calculated the false positive (FP) and false-negative
(FN) percentages as additional metrics for the
machine-learning algorithm. Figure 1 shows the
heat charts of the coded data. Upon this basis,
cross-validations of all metrics (CV = 5) were
conducted. We used the K-fold method to split
the sample into training set and testing set, thelatter being 1/5 of the full sample. Initially, we
selected the first 1/5 for testing. Next, we iterated
this process five times per the figure. As a result,
all five iterates yielded similar outputs. Hence,
these SVM models performed satisfactorily and
were safe to be used to detect the variables in the
rest of the corpus","""error analysis"" light",Content Validation,Yes
2022-11-30T14:05:51Z,A territorial dispute or an agenda war? A cross-national investigation of the network agenda-setting (NAS) model,"Su, Yan; Hu, Jun","Yes, Continue with Coding",NA,Social Media: Twitter,Others,"Mandarin, Japanese, English","Theme, Valence, Directionality",Text Attributes,Supervised: Machine Learning,SVM,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"A_Su,",10.1080/19331681.2020.1756553,Journal of Information Technology & Politics,2020,3,c,"In addition, to ensure transparency of the black box
of coding, Figure 2 exhibits the top 10 impact factors of
each variable coded by SVM. In other words, upon
completion of training and performance tests, SVM
coded the sample based on these high-frequency words.",Feature Importance,Content Validation,No
2022-11-28T10:15:06Z,Fine-Grained Analysis of Diversity Levels in the News,"Amsalem, Eran; Fogel-Dror, Yair; Shenhav, Shaul R.; Sheafer, Tamir","Yes, Continue with Coding",NA,Newspaper,English,NA,news diversity,Text Attributes,"Supervised: Machine Learning, Unsupervised: Topic Modeling",not named,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Fine-Grained_Amsalem,",10.1080/19312458.2020.1825659,Communication Methods and Measures,2020,2,a,"Table 2 presents the full list of economic topics identified using our automated system. These are
the topics we later use to analyze diversity. As the table shows, the economic categories are distinct
from one another and represent highly relevant aspects of the economic domain, such as banking,
commerce, employment, investments, taxes, and many more.3 To ensure that the computational
tools we are using produce a valid classification of economic topics in the news, we have systematically
compared the outputs of our system with human judgment. The validation procedure, which
is presented in more detail in Online Appendix A, yielded sufficient intercoder reliability scores for
the coding of economic topics in texts (Krippendorff’s α = .9). As for validity, in 97% of the cases, the
human judges have coded news articles as dealing with the same economic topics the automatic
system has identified.",Topic Model: hand-coding,External: Human Annotated Scores,Yes
2022-11-28T10:15:06Z,Fine-Grained Analysis of Diversity Levels in the News,"Amsalem, Eran; Fogel-Dror, Yair; Shenhav, Shaul R.; Sheafer, Tamir","Yes, Continue with Coding",NA,Newspaper,English,NA,news diversity,Text Attributes,"Supervised: Machine Learning, Unsupervised: Topic Modeling",not named,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Fine-Grained_Amsalem,",10.1080/19312458.2020.1825659,Communication Methods and Measures,2020,2,b,"The result of this stage was a large training set of labeled sentences, compiled with minimal
human effort, that we used to train a deep-learning supervised classifier on hundreds of topics. This
classifier was then used to identify topics in new news articles. We implemented this method on
a dataset containing all news articles published in close to 700 newspapers over more than 22 years
(January 1995–July 2017). In all, about 30 million news articles were used to train 20 topic models on
different issues (e.g., the economy, defense, crime, education). After transforming the outputs of all
topic models, we aggregated the resulting groups of sentences into a very large training set containing
about 100 million sentences, which we labeled with 651 different categories; in other words, we decomposed each of the 20 general issue domains to more than 30 sub-domains, on average. The 651
categories represent a broad range of political and social issues, including sub-topics of the economy,
education, health, immigration, transportation, and the like.
At this point, we have included each article’s title as contextual information to improve the
classification accuracy of each of its sentences, as a sentence may be better understood given some
context. This training set of labeled sentences combined with their titles was used to train a deep
learning classifier to identify each category at the sentence level. The model reached precision (the
percentage of sentences where one of the identified categories was correct, from all identified
sentences) of 75%, and recall (the percentage of sentences where the model succeeded to identify
a category, from all expected sentences) of 76%, on a held-out dataset of one million sentences.",using Topic model output as training data for ML,External: Scores from other CATM,No
2022-09-06T10:45:25Z,"Right-Wing, Populist, Controlled by Foreign Powers? Topic Diversification and Partisanship in the Content Structures of German-Language Alternative Media","Müller, Philipp; Freudenthaler, Rainer","Yes, Continue with Coding",NA,Newspaper,Others,German,Topic Diversification,Topics,Unsupervised: Topic Modeling,LDA,NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Right-Wing,_Müller,",10.1080/21670811.2022.2058972,Digital Journalism,2022,1,a,"In the last step, two researchers independently validated and labeled the topics of the 70-topic solution based on an in-depth reading of the documents, using both lists of the most common words in each topic (at k ¼ .6, see Sievert and Shirley 2014, 67) and full articles with the highest k under each topic and coherence indicator to guide their decisions; they discussed their independent assessments to arrive at the final set of topics. During this step, boilerplate topics (which occur in many articles and are not internally coherent, see DiMaggio, Nag, and Blei 2013, 568; Maier et al. 2018, 108) were removed and similar topics were grouped together, resulting in 14 groups of topics encompassing the 66 remaining topics, which are summarized in Table 1",Human Evaluation of Topics by two independent researchers,Content Validation,No
2022-09-06T11:18:08Z,Connecting the (Far-)Right Dots: A Topic Modeling and Hyperlink Analysis of (Far-)Right Media Coverage during the US Elections 2016,"Kaiser, Jonas; Rauchfleisch, Adrian; Bourassa, Nikki","Yes, Continue with Coding",NA,Newspaper,English,NA,convergence of topics,Topics,Unsupervised: Topic Modeling,structural topic model analysis (STA),NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Connecting_Kaiser,",10.1080/21670811.2019.1682629,Digital Journalism,2020,1,a,"We randomly selected 100 articles and automatically assigned the topic with a probability above 0.49 to the article. Two coders without specific training coded the topics. The intercoder-reliability score between the two coders and the topic model was with a Krippendorff’s alpha of 0.67 acceptable for our case. When the more generic Republican topics are merged the alpha is even over 0.7. This is surprisingly high as most documents have a mix of topics (e.g., economy and republican primaries)",Comparison with 100 hand coded articles,External: Human Annotated Scores,No
2022-09-06T17:04:49Z,Machine Translation Vs. Multilingual Dictionaries Assessing Two Strategies for the Topic Modeling of Multilingual Text Collections,"Maier, Daniel; Baden, Christian; Stoltenberg, Daniela; De Vries-Kedem, Maya; Waldherr, Annie","Yes, Continue with Coding",NA,"Newspaper, Social Media: Twitter",Others,"Hebrew, Arabic, English",Topics,Topics,"Unsupervised: Topic Modeling, Rule-based: Adjusted dictionary",Structural Topic Models (STM),NA,Yes,"Two ""Preprocessing"" steps are compared to deal with multilingual texts for LDA",Yes,Yes,No,No,lukas.birkenmaier@outlook.de,"Machine_Maier,",10.1080/19312458.2021.1955845,Communication Methods and Measures,2022,3,a,"The dictionary was repeatedly validated and improved until it achieved precision and recall scores well in excess of .75 in each language (see Baden & Stalpouskaya, 2015; Tenenboim-Weinblatt & Baden, 2021 for more detail). As the dictionary uses structural features of the documents (e.g., word distances, syntactic structure) for concept identification and disambiguation,8 the dictionary was applied to the unprocessed text",reference to previous validation excercise,Content Validation,Yes
2022-09-06T17:04:49Z,Machine Translation Vs. Multilingual Dictionaries Assessing Two Strategies for the Topic Modeling of Multilingual Text Collections,"Maier, Daniel; Baden, Christian; Stoltenberg, Daniela; De Vries-Kedem, Maya; Waldherr, Annie","Yes, Continue with Coding",NA,"Newspaper, Social Media: Twitter",Others,"Hebrew, Arabic, English",Topics,Topics,"Unsupervised: Topic Modeling, Rule-based: Adjusted dictionary",Structural Topic Models (STM),NA,Yes,"Two ""Preprocessing"" steps are compared to deal with multilingual texts for LDA",Yes,Yes,No,No,lukas.birkenmaier@outlook.de,"Machine_Maier,",10.1080/19312458.2021.1955845,Communication Methods and Measures,2022,3,b,"Following the modeling stage, the obtained models were validated and compared using both quantitative and qualitative procedures. For the quantitative comparison, we calculated the extent to which different topic models obtained from the respective methodological approaches break down the variation included in each corpus in similar ways. To do so, we use the Correlation Matrix Distance (CMD), a distance measure designed to determine the (dis)similarity between two quadratic correlation matrices (Herdin et al., 2005; Motta & Baden, 2013). Theoretically, the CMD ranges from one (no association between both matrices) to zero (both correlation matrices are identical, up to a scale factor). For this estimation, we depart from the compared topic models’ n � k-sized document-topic (θÞ matrices, wherein each of the n ¼ 1; . . . ; N rows correspond to a document in the respective corpus, and each of the k ¼ 1; . . . ; K columns to a topic in the respective topic model. In these matrices, the cell entries θnk represent the probability of document n to contain a given topic k – usually interpreted as the topic proportion that the respective document contains. Since the identity of the compared topics is uninformative for the comparison (i.e., one can permutate the order of topics (columns) in each matrix without changing the structural similarity), we transform the raw θ matrices into quadratic correlation matrices by calculating the dot product of θ with its transpose θT. As a result, we obtain, for each topic model, an n � n matrix C that reflects the extent to which different documents are composed of the same topics. The CMD then computes the distance between each pair of matrices C, which provides a direct measure of the extent to which the topics obtained by two compared topic models are distributed in (dis)similar ways over the same set of documents. Since the dimensionality of C depends solely on the number of the n documents, this transformation allows us to compare the θ’s of different topic models independently of their number of topics K. Formula 1 defines the CMD for the two correlation matrices CMTand CMD that result from the two methods under investigation, as denoted by their subscripts",calculated the extent to which different topic models obtained from the respective methodological approaches break down the variation included in each corpus in similar ways,Unsure,Yes
2022-09-06T17:04:49Z,Machine Translation Vs. Multilingual Dictionaries Assessing Two Strategies for the Topic Modeling of Multilingual Text Collections,"Maier, Daniel; Baden, Christian; Stoltenberg, Daniela; De Vries-Kedem, Maya; Waldherr, Annie","Yes, Continue with Coding",NA,"Newspaper, Social Media: Twitter",Others,"Hebrew, Arabic, English",Topics,Topics,"Unsupervised: Topic Modeling, Rule-based: Adjusted dictionary",Structural Topic Models (STM),NA,Yes,"Two ""Preprocessing"" steps are compared to deal with multilingual texts for LDA",Yes,Yes,No,No,lukas.birkenmaier@outlook.de,"Machine_Maier,",10.1080/19312458.2021.1955845,Communication Methods and Measures,2022,3,c,"For the qualitative comparison, we first selected one best-fitting model per corpus and approach. Among all estimated topic models, we focused on those five models that offered the best fit based on the evaluation metrics offered by the STM package (K ¼ 10; 15; 20; 25; 30 f g for both corpora and both methods). Two of the authors jointly judged these models’ interpretability, inspecting their top associated words (MT approach) and concepts (MD approach). Based on this information, we selected for the News corpus those topic models obtained for K = 25 (independently for both MT and MD), and for the Twitter corpus those models obtained for K = 30, for further validation. For these selected models, all topics were then carefully labeled and validated by the same two researchers. To ascertain the validity of topics and their labels, we selected for every topic a random sample of ten documents with a topic probability above a threshold of t ¼ 0:3. Given that only few topics are prevalent in a single document, the threshold can be considered sufficiently high for a topic to be the most prevalent in the respective documents (see also Maier et al., 2018). Topics were confirmed as interpretable and labeled if, through a discursive process, both judges agreed on the same interpretation, which had to be supported both by the identified top words/concepts and by the inspected documents, read against our familiarity with the conflict and the referenced events.","Jugdement of models’ interpretability, inspecting their top associated words (MT approach) and concepts (MD approach)",Content Validation,No
2022-11-17T15:11:29Z,How Accurate Are Survey Responses on Social Media and Politics?,"Guess, Andrew; Munger, Kevin; Nagler, Jonathan; Tucker, Joshua","Yes, Continue with Coding",NA,Social Media: Facebook,English,NA,political topic,Topics,Supervised: Machine Learning,naive Bayes classifier,NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"How_Guess,",10.1080/10584609.2018.1504840,Political Communication,2019,1,a,"To validate our results, we took a random sample of 1,000 tweets and
1,000 Facebook posts from the entire population of posts, added 100 “gold standard” posts
of each type that contained an anchor term, and paid workers on Amazon’s Mechanical Turk
to code them as political or not. Each of these posts had already been labeled by our model
as political or not, and these codes allowed us to measure the performance of the model as
it would be expected to perform on the entire corpus.5
Our Facebook model performed extremely well: 97% of the machine labels and human
labels were the same. The Twitter model performed slightly less well, at 84% accuracy. This
was lower than we found acceptable, particularly because the errors were almost all in the
same direction: the machine labeled 34% of the posts as political, compared to only 20% of
the human-labeled posts.","hand-coding, but by Mechanical Turk",External: Human Annotated Scores,No
2022-11-17T15:33:02Z,Legislative Bellwethers: The Role of Committee Membership in Parliamentary Debate,"Fernandes, Jorge M.; Goplerud, Max; Won, Miguel","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,Others,Portuguese,political topic,Topics,Supervised: Machine Learning,SVM,NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Legislative_Fernandes,",10.1111/lsq.12226,Legislative Studies Quarterly,2019,3,a,"First, we confirm
that the SVM can do a good job of predicting the committee labels
of unseen bills that are not included when training the model. This
gives us confidence that our procedure is good at capturing the underlying words that differentiate bills. This implies that the committee jurisdictions are coherent enough that we can accurately
predict, say, unseen agriculture bills using the agriculture bills in
our original training data.",predicting comittee labels,External: Criterion data / Predictive validation,Yes
2022-11-17T15:33:02Z,Legislative Bellwethers: The Role of Committee Membership in Parliamentary Debate,"Fernandes, Jorge M.; Goplerud, Max; Won, Miguel","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,Others,Portuguese,political topic,Topics,Supervised: Machine Learning,SVM,NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Legislative_Fernandes,",10.1111/lsq.12226,Legislative Studies Quarterly,2019,3,b,"Second, we confirm that the SVM can
work on speeches. To do this, we hand-classify a moderate number of speeches and then examine the performance of our model. If
this shows good results, it confirms that there is enough similarity
across the political texts to justify using this procedure.",hand-coding,External: Human Annotated Scores,Yes
2022-11-17T15:33:02Z,Legislative Bellwethers: The Role of Committee Membership in Parliamentary Debate,"Fernandes, Jorge M.; Goplerud, Max; Won, Miguel","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,Others,Portuguese,political topic,Topics,Supervised: Machine Learning,SVM,NA,No,NA,Yes,No,NA,NA,lukas.birkenmaier@outlook.de,"Legislative_Fernandes,",10.1111/lsq.12226,Legislative Studies Quarterly,2019,3,c,"We conduct extensive further validation in Appendix E in
the online supporting information where we examine which types
of words drive performance; in short, we find that using only the
“technical” language has relatively poor performance, but the
joint usage of the “motivations” section (analogous to a political speech) and “technical” section outperforms “motivations”
alone. We further show that the top words selected by the SVM
for each category are nouns that relate to the core jurisdiction of
each committee, and thus we are classifying based on substantive
content rather than idiosyncratic words",examination of top words,Content Validation,No
2022-11-18T10:38:23Z,Exploring the Political Agenda of the European Parliament Using a Dynamic Topic Modeling Approach,"Greene, Derek; Cross, James P.","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,topic,Topics,"Unsupervised: Topic Modeling, Own Delevlopment",non-negative Matrix factorization,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Exploring_Greene,",10.1017/pan.2016.7,Political Analysis,2017,4,a,"Table 1 shows a partial example of a dynamic topic. We observe that, for the
four window topics, there is a common theme pertaining to climate change. The
evolution of the climate change topic can be seen in the emergence of the terms
‘Copenhagen’, ‘conference’ and ‘summit’ in 2009-Q4 and 2010-Q1, at exactly
the time when the Copenhagen climate change summit was underway. Detecting
the evolution of topics in this manner is one of the advantages of taking a dynamic approach. While the variation across the term lists reflects the evolution of
this dynamic topic over the time period (2008-Q4 to 2010-Q1), the considerable
number of terms shared between the lists underlines its semantic validity",examaning top 10 words semantic validty,Content Validation,Yes
2022-11-18T10:38:23Z,Exploring the Political Agenda of the European Parliament Using a Dynamic Topic Modeling Approach,"Greene, Derek; Cross, James P.","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,topic,Topics,"Unsupervised: Topic Modeling, Own Delevlopment",non-negative Matrix factorization,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Exploring_Greene,",10.1017/pan.2016.7,Political Analysis,2017,4,b,"To examine the intra-topic semantic validity of these dynamic topics, we examined the distribution of TC-W2V coherence values for all dynamic topics, when
evaluated in the word2vec space built from the complete speech corpus. These
coherence values correspond to the mean of the pairwise cosine similarities between the top-10 terms for each topic in the word2vec space (see Eqn. 1). As
evidenced by the coherence values reported in Table 2, the most coherent topics
often correspond to core EU competencies. Unsurprisingly, broad administrative
topics prove to be least coherent (e.g. ‘Commission questions’, ‘Council Presidency’, ‘Plenary administration’). Overall the mean topic coherence score of 0.36
is considerably higher than the lower bound for TC-W2V (i.e. minimum value =
-1), suggesting a high level of semantic validity",assessing the coherence of topics,Content Validation,Yes
2022-11-18T10:38:23Z,Exploring the Political Agenda of the European Parliament Using a Dynamic Topic Modeling Approach,"Greene, Derek; Cross, James P.","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,topic,Topics,"Unsupervised: Topic Modeling, Own Delevlopment",non-negative Matrix factorization,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Exploring_Greene,",10.1017/pan.2016.7,Political Analysis,2017,4,c,"To assess the inter-topic semantic validity of the results, we examine the extent to
which any meaningful higher-level grouping exists among the 57 dynamic topics.
To do this we apply average linkage agglomerative clustering to the topics. Using the approach described in Greene et al. (2008), we re-cluster the row vectors
from the second-layer NMF factor H using normalized Pearson correlation as a
similarity metric. Here the vectors correspond the weights of each dynamic topic
with respect to the 2,710 terms noted above. The dendrogram for the hierarchical clustering is shown in Fig. 2. Following the interpretation provided in Quinn
et al. (2010), the lower the height at which any two topics are connected in the
dendrogram, the more similar their term usage patterns in EP sessions.We observe a number of higher-level groupings of interest, which are highlighted in Fig. 2. These include groups related to transport, energy concerns, institutional interactions, education and research, trade relations, and EU enlargement.
The presence of these higher-level associations between topics provide semantic
validity for the results presented, where topics that one might expect to be related
are found to be correlated with respect to rows in their NMF factor H (i.e.similar
terms appear in the set of topic descriptors (words) that define them as topics).
",inter-topic validity,Content Validation,Yes
2022-11-18T10:38:23Z,Exploring the Political Agenda of the European Parliament Using a Dynamic Topic Modeling Approach,"Greene, Derek; Cross, James P.","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,English,NA,topic,Topics,"Unsupervised: Topic Modeling, Own Delevlopment",non-negative Matrix factorization,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Exploring_Greene,",10.1017/pan.2016.7,Political Analysis,2017,4,d,"To assess the extent to which the dynamic topics identified correspond to EU
policy areas, and thus provide evidence of construct validity, we compare the 57
dynamic topics to an existing taxonomy of subjects used by Europarl to classify
legislative procedures. The taxonomy retrieved from the EP website has several
different levels, ranging from broad top-level subjects (e.g. ‘3 Community policies’), to highly-specific low-level subjects (e.g. ‘3.10.06.05 Textile plants, cotton’). We compare our results to the second level of the taxonomy, containing 48
subjects (e.g. ‘3.10 Agricultural policy and economies’, ‘3.20 Transport policy in
general’). For each subject code, we create a “subject document” consisting of
the description of the subject and all lower-level subjects within that branch of the
taxonomy. We then identify the most similar dynamic topic by comparing the top
10 terms for that topic with subject documents, based on cosine similarity.
Table 3 shows the best matching subjects and topics identified using this approach. To give a couple of examples, the topic hand-coded as relating to ‘Tax’
from our topic model was correctly matched with the Europarl subject code ‘2.70
Taxation’ broadly defined at level-2 of the taxonomy, and with ‘2.70.01 Direct
taxation’ and ‘2.70.02 Indirect taxation’ defined separately at level-3 of the taxonomy. When looking at the topic manually labeled as relating to ‘Drugs’, cosine
similarity matches this with the level-2 subject ‘4.20 Public health’, which has a
level-3 sub-category relating to ‘4.20.04 Pharmaceutical products and industry’",predicting legislative labels,External: Criterion data / Predictive validation,No
2022-11-28T10:35:13Z,Cross-Domain Topic Classification for Political Texts,"Osnabrügge, Moritz; Ash, Elliott; Morelli, Massimo","Yes, Continue with Coding",NA,"Party Politics: Party Manifestos, Party Politics: Parliamentary Records",English,NA,topic,Topics,Supervised: Machine Learning,regularizedmultinomial logistic regressionmodel,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Cross-Domain_Osnabrügge,",10.1017/pan.2021.37,Political Analysis,2021,3,a,"The coder annotated a random sample of 4,165 parliamentary speeches in one manifesto
category. We only gave the coder the text of the speech and no metadata, such as the date or
speaker. We asked the coder to code based on the Manifesto Project. We annotated topics by
speech, as our downstream empirical analysis is at the speech level, which allowed us to obtain
muchmore data than sentence-level annotations. This approach is in linewith Barberá et al. (2021,
28–29), who find that breaking larger text units into sentences does not improve classification
performance. The annotations took a total of 52.5 hours.
We hired three additional coders to assess inter-coder reliability within the NewZealand target
corpus (Mikhaylov et al. 2012). Like the main coder, these coders also received training from
the Manifesto Project in English-language platforms; we asked them to code according to the
Manifesto Project methodology. The coders were not experts on New Zealand politics, however.
We drew a random sample of 250 speeches from the 4,165 speeches annotated by the first coder.
Each of the three secondary coders annotated the same subsample of 250 speeches, which gave
us four annotations in total.
Finally, to assess the method’s potential broader generalization, we also hand-annotated a
corpus of congressional speeches from the United States. We hired the Manifesto Project coder
for the United States and asked him to code a random sample of 150 speeches from the House of
Representatives. The sample was drawn from all speeches contained in the Congressional Record
for the period fromAugust 1987 to July 2002. All five coders assigned each speech to onemanifesto
category.",hand-coding,External: Human Annotated Scores,Yes
2022-11-28T10:35:13Z,Cross-Domain Topic Classification for Political Texts,"Osnabrügge, Moritz; Ash, Elliott; Morelli, Massimo","Yes, Continue with Coding",NA,"Party Politics: Party Manifestos, Party Politics: Parliamentary Records",English,NA,topic,Topics,Supervised: Machine Learning,regularizedmultinomial logistic regressionmodel,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Cross-Domain_Osnabrügge,",10.1017/pan.2021.37,Political Analysis,2021,3,b,"To increase our confidence that the model is properly identifying topics in the target corpus, we
undertook further analysis to interpret themodel predictions. First, we read the 10 parliamentary
speeches with the highest probability of belonging to each topic, using both the 44-topic and
8-topic specifications. In general, the speeches corresponded very well to the specified topics,
and we saw no evidence that they were driven by correlated features. Section B.6 in the Online
Appendix includes text snippets for each of these topics",error analysis by reading top classified sentences for each topic,Content Validation,Yes
2022-11-28T10:35:13Z,Cross-Domain Topic Classification for Political Texts,"Osnabrügge, Moritz; Ash, Elliott; Morelli, Massimo","Yes, Continue with Coding",NA,"Party Politics: Party Manifestos, Party Politics: Parliamentary Records",English,NA,topic,Topics,Supervised: Machine Learning,regularizedmultinomial logistic regressionmodel,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Cross-Domain_Osnabrügge,",10.1017/pan.2021.37,Political Analysis,2021,3,c,"Tomore systematically analyze the connection between text features and predicted topics, we
created a feature importance measure to identify which phrases are significantly correlated with
topics in the source and target corpora. We use a simple metric computed from ordinary least
squares (OLS) regressions. Formally, for each output topic class k and each N-gram feature j, we
run a bivariate OLS regressionmodel
pˆk
i = α +βj k xj
i
+ǫi , [j , k , (1)
where pˆk
i is the predicted probability that document i is about topic k and xj
i is the relative
frequency of N-gramj in document i. These regressions generate a dataset of coefficients βˆj k and
associated standard errorsds.e.j k , separately for the manifesto statements and the parliamentary
speeches.
To identify statistically significant features for each topic, we compute the t-statistic τˆj k =
βˆj k /ds.e.j k in both the source and target corpora. Important features have a high (absolute value)
t-statistic. To help further ensure that the features are interpretable, we constrain the vocabulary
to a set of idiomatic noun phrases. This procedure is described in detail in Section B.7 in the Online
Appendix, which also includes word clouds depicting the top-ranked phrases and t-statistics.
Figure 1 illustrates the results of this analysis using scatter plots. Each plot focuses on one of
the eight topics k. In each plot, the vertical axis indexes the t-statistic τˆS
j k for k in the manifesto
platformstatements, whereas the horizontal axis indexes τˆT
j k for k in the New Zealand parliamentary
speeches. Each dot on the plot corresponds to an N-gram j, printed as a marker label. The
vocabulary is filtered to the intersection of N-grams that are predictive for at least one topic in
either corpus. Looking at the plots by topic, we find that there is a strong relationship in general
between the t-stats in the source and target corpus. This is reassuring evidence that the content of
the topics is similar in themanifesto and the parliamentary speech data. Hence, the topics in both
data sources can be interpreted in a similar manner. The exception is no topic (Panel (h)), which
intuitively would be less well defined in terms of political language.",feature importance,Content Validation,No
2022-11-29T11:13:51Z,Mobilization vs. Demobilization Discourses on Social Media,"Kligler-Vilenchik, Neta; de Vries Kedem, Maya; Maier, Daniel; Stoltenberg, Daniela","Yes, Continue with Coding",NA,Social Media: Twitter,Others,"Hebrew, Arabic, English",topics,Topics,Unsupervised: Topic Modeling,STM,NA,No,NA,NA,Yes,Yes,No,lukas.birkenmaier@outlook.de,"Mobilization_Kligler-Vilenchik,",10.1080/10584609.2020.1820648,Political Communication,2021,1,a,"The topics were then validated by reading through the documents with high topic proportions (Maier et al., 2018).",manual evaluation of topics,Content Validation,No
2022-11-30T13:49:55Z,How dark corners collude: a study on an online Chinese alt-right community,"Yang, Tian; Fang, Kecheng","Yes, Continue with Coding",NA,Social Media: Weibo,Others,Mandarin,topic,Topics,Unsupervised: Topic Modeling,LDA,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"How_Yang,",10.1080/1369118X.2021.1954230,"Information, Communication & Society",2021,1,a,"To gain a deep understanding of the topics generated, for each topic, we picked 200
segments of post with the highest coeﬃcient associated with the topic, which could
best represent the content of one topic. By closely reading and interpreting these seg-
ments of post and posts that they belong to and checking top words of the topic (gener-
ated by LDA), we further synthesized the main themes for each topic.",top words,Content Validation,No
2022-11-30T14:57:52Z,Understanding the online relationship between politicians and citizens. A study on the user engagement of politicians’ Facebook posts in election and routine periods,"Peeters, Jeroen; Opgenhaffen, Michaël; Kreutz, Tim; Van Aelst, Peter","Yes, Continue with Coding",NA,Social Media: Facebook,Others,Dutch,topic type,Topics,Supervised: Machine Learning,"SVM, BERT, LiLaH",NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Understanding_Peeters,",10.1080/19331681.2022.2029791,Journal of Information Technology & Politics,2022,1,a,"n order to test the effec-
tiveness of this operationalization, we compared 
a random subset (N = 311) of posts that were 
manually coded with the label that was automati-
cally assigned to it using the aforementioned 
method. We ran correlations for each of the three 
categories. This resulted in a correlation of 0.96 for 
campaign posts, 0.97 for political posts and 0.93 for 
private posts. This shows the robustness of our 
operationalization.",hand-coding,External: Human Annotated Scores,No
2022-11-30T15:19:21Z,Topics to talk about. The effects of political topics and issue ownership on user engagement with politicians’ Facebook posts during the 2018 Hungarian general election,"Bene, Márton","Yes, Continue with Coding",NA,Social Media: Facebook,Others,Hungarian,topic,Topics,"Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary",really not named,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,"Topics_Bene,",10.1080/19331681.2021.1881015,Journal of Information Technology & Politics,2021,1,a,"Results yielded by automated content analysis 
methods are validated by comparing a sample of 
predictions to manually coded ‘gold standard’ out-
comes. For the evaluation of models’ performance 
precision, recall and F1 values are used. Precision 
indicates what share of automatically identified ele-
ments is appropriate, while recall shows the share 
of elements of the category under investigation in 
the population that the model detects. F1 values are 
calculated from both precision and recall, and their 
range is between 0 and 1 where 1 indicates a perfect 
fit to the ‘gold standard’ (maximum level of preci-
sion and recall).
A random sample of 1600 posts was manually 
coded, serving both as a gold standard dataset to 
evaluate the performance of the dictionary-based 
method and as a training set for the supervised 
machine learning method. A further manually 
coded sample of 400 posts was used to check the 
performance of the machine learning algorithm 
beyond the training set.5 The coder was not familiar 
with the dictionaries based on topics were automa-
tically identified therefore human coding is inde-
pendent from the automated approach",hand-coding,External: Human Annotated Scores,No
2022-11-30T15:27:01Z,How African countries respond to fake news and hate speech,"Garbe, Lisa; Selvik, Lisa-Marie; Lemaire, Pauline","Yes, Continue with Coding",NA,Newspaper,English,NA,topic,Topics,Unsupervised: Topic Modeling,STM,NA,No,NA,NA,Yes,No,Yes,lukas.birkenmaier@outlook.de,"How_Garbe,",10.1080/1369118X.2021.1994623,"Information, Communication & Society",2021,3,a,"Following suggestions by Debortoli et al. (2016; see also Fischer-Pressler et al., 2019), we use the most frequent terms and the 20 texts with the highest topic proportion per topic, to assign a label to each topic. Based on this qualitative inspection of the topics, we chose Topic 5 and Topic 31 as suitable indicators for ‘legal’ and ‘technological’ approaches to regulate fake news",top words etc.,Content Validation,Yes
2022-11-30T15:27:01Z,How African countries respond to fake news and hate speech,"Garbe, Lisa; Selvik, Lisa-Marie; Lemaire, Pauline","Yes, Continue with Coding",NA,Newspaper,English,NA,topic,Topics,Unsupervised: Topic Modeling,STM,NA,No,NA,NA,Yes,No,Yes,lukas.birkenmaier@outlook.de,"How_Garbe,",10.1080/1369118X.2021.1994623,"Information, Communication & Society",2021,3,b,"To validate our procedure, four human coders received a short description of the two selected topics, Topic 5 on ‘legal approaches’ and Topic 31 on ‘technological approaches’, plus a randomly chosen topic (Topic 17 on ‘fact-checking’). The coders were tasked to read 30 news articles with a high topic proportion, 10 for each topic, without knowing to which topic they belonged, and assign them to the topics based on the topic description. Two of the authors coded blind to the selection of texts, prepared by the third author. In addition, two further coders, fully blind to the study’s theoretical expectations and results, coded the texts as well. The results are reported in Table C2 below.",giving the text samples with specific topics to coders,Content Validation,Yes
2022-11-30T15:27:01Z,How African countries respond to fake news and hate speech,"Garbe, Lisa; Selvik, Lisa-Marie; Lemaire, Pauline","Yes, Continue with Coding",NA,Newspaper,English,NA,topic,Topics,Unsupervised: Topic Modeling,STM,NA,No,NA,NA,Yes,No,Yes,lukas.birkenmaier@outlook.de,"How_Garbe,",10.1080/1369118X.2021.1994623,"Information, Communication & Society",2021,3,c,"We further assess how well the identiﬁed topics for ‘legal’ and ‘technological’
approaches to content regulation in news coverage capture actual regulatory steps under-
taken by African governments (see Appendix F). Speciﬁcally, we compare our country-
year mean topic proportions with data from the Freedom on the Net reports (Freedom
House, 2015, 2016, 2017, 2018, 2019, 2020), ﬁrst through a t-test and then by investi-
gating four cases more qualitatively. According to the results, news reports provide a
fair indication of diﬀerent legal and technological regulations by African governments
For the remainder of this study, we use the expected proportion of each topic as depen-
dent variable.",comparing topics with external data,External: Criterion data / Predictive validation,No
2022-11-30T16:17:31Z,When populists become popular: comparing Facebook use by the right-wing movement Pegida and German political parties,"Stier, Sebastian; Posch, Lisa; Bleier, Arnim; Strohmaier, Markus","Yes, Continue with Coding",NA,Social Media: Facebook,Others,German,topic use,Topics,Unsupervised: Topic Modeling,LDA,NA,No,NA,NA,Yes,No,No,lukas.birkenmaier@outlook.de,"When_Stier,",10.1080/1369118X.2017.1328519,"Information, Communication & Society",2017,1,a,"To narrow down the scale to politically interpretable and thus substantively relevant
topics, three of the authors independently coded the model outputs as relevant topics
on policies or contemporary events, or of no substantive interest (with an inter-rater
reliability of Fleiss’ Kappa = 0.706, p < .000). This means that ‘stopword topics’ contain-
ing Facebook-specific language such as ‘like, follow, share’ or parliamentary procedural
topics containing ‘vote, debate, speaker’ were dropped, but also topics on constituency ser-
vice which, while being interpretable, cover procedural instead of substantive issues and
are therefore of no relevance here. We also excluded party-specific topics with predomi-
nantly organizational information and unique language only used by a particular group.
This procedure left us with 46 topics of substantive interest out of the original 100. The
authors independently assigned titles based on the top scoring words for each topic in
the appendix and decided on the few ambiguous cases consensually. The model identifies
a mix of generally relevant policy fields but also topics more specific to our research period
like the Euro or refugee crise",human  evaluation of topics -> content,Content Validation,No
2022-11-30T16:33:13Z,Reproducible extraction of cross-lingual topics (rectr),"Chan, Chung-Hong; Zeng, Jing; Wessler, Hartmut; Jungblut, Marc; Welbers, Kasper; Bajjalieh, Joseph W.; Van Atteveldt, Wouter; Althaus, Scott L.","Yes, Continue with Coding",NA,Newspaper,Others,"German, French, English",topic,Topics,Unsupervised: Topic Modeling,na,NA,No,NA,NA,Yes,Yes,No,lukas.birkenmaier@outlook.de,"Reproducible_Chan,",NA,Communication Methods and Measures,2020,2,a,"For a pair of articles in two different languages, two trilingual raters coded the topical 

similarity with a five-point Likert scale (1, very different, to 5, very similar). The candidate 
pairs of articles were selected based on the quartile difference in θt, for each topic and each 
language pair. Five articles were selected for each quartile pair and language pair. In total, 
240 pairs of articles were coded (3 language combinations × 4 quartiles 2 × 5 articles = 240 
pairs). The reason for choosing this quartile-based stratified sampling instead of a simple 

random sample is to maximize the variance of topical similarity in the sample, so that 

document pairs with low to high similarity are represented in the sample (Katki et al., 2012). 
The raters’ codings were averaged to represent the human judgment of topical similarity. 
 The human-coded similarity ratings of the document pairs were then correlated with the cosine similarity between the θt of the two documents in the pair. This correlation was 
used to evaluate the agreement between our topic models and human judgment.",comparison of text similiarity using human coders,External: Human Annotated Scores,Yes
2022-11-30T16:33:13Z,Reproducible extraction of cross-lingual topics (rectr),"Chan, Chung-Hong; Zeng, Jing; Wessler, Hartmut; Jungblut, Marc; Welbers, Kasper; Bajjalieh, Joseph W.; Van Atteveldt, Wouter; Althaus, Scott L.","Yes, Continue with Coding",NA,Newspaper,Others,"German, French, English",topic,Topics,Unsupervised: Topic Modeling,na,NA,No,NA,NA,Yes,Yes,No,lukas.birkenmaier@outlook.de,"Reproducible_Chan,",NA,Communication Methods and Measures,2020,2,b,"It is also easier for us to qualitatively label the topics from rectr than tdtm-stm or ft-stm by 
reading the articles with high θt (Appendix I & II). Figure 722 shows the correlations of human judgment in the cross-lingual topical similarity of our randomly selected 240 article 

pairs with topical similarities calculated with our four models. In general, all four charts show 

a positive trend, indicating that the topic similarity metrics from all four models have 

concurrent validity.",comparison of different topic models,External: Scores from other CATM,No
2022-11-30T17:11:50Z,Deliberative democracy in an unequal world: A text-as-data study of south India’s village assemblies,"Parthasarathy, Ramya; Rao, Vijayendra; Palaniswamy, Nethra","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,Others,unclear,topic,Topics,Unsupervised: Topic Modeling,dd,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Deliberative_Parthasarathy,",NA,American Political Science Review,2019,2,a,"First, as a test of predictive validity, we
examine whether the topics that capture proforma
features of the assembly are indeed more likely to be
discussed by ofﬁcials, rather than citizens. More spe-
ciﬁcally, the topic model identiﬁes a set of standard
remarks—such as the reading of resolutions, the for-
mal greetings and votes of thanks, and discussion of
government funding allocation—as distinct topics. If
these topics capture the rote features of assemblies as
they are conducted, these should be primarily spoken
by ofﬁcials, who are responsible for convening and
adjourning the meeting, as well as sharing information
about the recent public expenditures. Figure 2 plots the
difference between the expected proportion of these
proforma topics between citizens and ofﬁcials (both
elected and administrative) for the documents in the
corpus. As expected, these proforma speeches are all
signiﬁcantly more likely to be raised by ofﬁcials, sug-
gesting that the topics reﬂect our substantive in-
terpretation of their content",predicting agenda setting,External: Criterion data / Predictive validation,Yes
2022-11-30T17:11:50Z,Deliberative democracy in an unequal world: A text-as-data study of south India’s village assemblies,"Parthasarathy, Ramya; Rao, Vijayendra; Palaniswamy, Nethra","Yes, Continue with Coding",NA,Party Politics: Parliamentary Records,Others,unclear,topic,Topics,Unsupervised: Topic Modeling,dd,NA,No,NA,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Deliberative_Parthasarathy,",NA,American Political Science Review,2019,2,b,"Second, we also validate the topics against the survey
data collected by enumerators sent to each village. As
part of the data collection process, enumerators were
asked to record information on the types of issues raised
during the assemblies. We can coarsely examine
whether the type and frequency of issues counted in the
survey-collected data correspond to their counterparts
in transcript data. This comparison, while helpful, is
necessarily imperfect for two reasons: First, while the
survey-collected data merely count whether an issue was
raised within a village assembly, the transcript data
shares are calculated based on the proportion of
documents associated with that topic. As such, the
transcript data will overweight topics that are discussed
at length or by many speakers, relative to those that are
brieﬂy mentioned. Second, while many topics have clear
analogs across the datasets, others are coded differently
across the two sources. Given these discrepancies, we
ﬁnd the closest possible analogs, or aggregate where
necessary. There are also a handful of topics for which
clear analogs are not available. (See Appendix Table",comparison with survey data,External: Criterion data / Predictive validation,No
2022-11-29T15:06:17Z,The Distorting Prism of Social Media: How Self-Selection and Exposure to Incivility Fuel Online Comment Toxicity,"Kim, Jin Woo; Guess, Andrew; Nyhan, Brendan; Reifler, Jason","Yes, Continue with Coding",NA,"Newspaper, Social Media: Facebook",English,NA,toxic language,Toxiticy,Supervised: Machine Learning,toxicity machine learning model,NA,Yes,they use a pretrained model,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"The_Kim,",10.1093/joc/jqab034,Journal of Communication,2021,3,a,"We provide examples of comments at different levels of the toxicity measure in Table B1 in Online
Appendix B. Our reading of these examples, as well as other comments in the sample, suggests that the
Perspective toxicity score corresponds well—albeit not perfectly—to the textual contents of the comments.
In general, comments with low toxicity scores (< .2) lack uncivil or derogatory remarks, whereas those with
moderate scores (toxicity  .5) are often quite rude and inflammatory, and those with high scores (toxicity.8) tend to be extremely toxic and disrespectful.",evaluation of top sentences,Content Validation,Yes
2022-11-29T15:06:17Z,The Distorting Prism of Social Media: How Self-Selection and Exposure to Incivility Fuel Online Comment Toxicity,"Kim, Jin Woo; Guess, Andrew; Nyhan, Brendan; Reifler, Jason","Yes, Continue with Coding",NA,"Newspaper, Social Media: Facebook",English,NA,toxic language,Toxiticy,Supervised: Machine Learning,toxicity machine learning model,NA,Yes,they use a pretrained model,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"The_Kim,",10.1093/joc/jqab034,Journal of Communication,2021,3,b,"To validate the Perspective API more formally in our data, we created two alternative measures of toxicity:
using a crowd-sourced pairwise rating approach (Carlson & Montgomery 2017) and creating a dictionary of
uncivil words (Muddiman et al. 2019). The procedures and results are detailed in Appendix E. As shown
in Figure E1, alternative measures of toxicity — one human-labelled and another dictionary-based — are
strongly correlated with the Perspective score.",developing a dictionary,External: Scores from other CATM,Yes
2022-11-29T15:06:17Z,The Distorting Prism of Social Media: How Self-Selection and Exposure to Incivility Fuel Online Comment Toxicity,"Kim, Jin Woo; Guess, Andrew; Nyhan, Brendan; Reifler, Jason","Yes, Continue with Coding",NA,"Newspaper, Social Media: Facebook",English,NA,toxic language,Toxiticy,Supervised: Machine Learning,toxicity machine learning model,NA,Yes,they use a pretrained model,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"The_Kim,",10.1093/joc/jqab034,Journal of Communication,2021,3,c,"To validate the Perspective API more formally in our data, we created two alternative measures of toxicity:
using a crowd-sourced pairwise rating approach (Carlson & Montgomery 2017) and creating a dictionary of
uncivil words (Muddiman et al. 2019). The procedures and results are detailed in Appendix E. As shown
in Figure E1, alternative measures of toxicity — one human-labelled and another dictionary-based — are
strongly correlated with the Perspective score.",hand-coding,External: Human Annotated Scores,No
2022-11-30T17:18:09Z,After all this time? The impact of media and authoritarian history on political news coverage in twelve western countries,"de Leeuw, Sjifra E.; Azrout, Rachid; Rekker, Roderik SB; Van Spanje, Joost HP","Yes, Continue with Coding",NA,Newspaper,Others,multilingual,Pejoration,Toxiticy,Rule-based: Adjusted dictionary,na,NA,No,NA,NA,No,NA,NA,lukas.birkenmaier@outlook.de,After_de,NA,Journal of Communication,2020,1,a,"The automated content analysis returned 16,991 hits spread across the 27,830
articles in our dataset. To redress the chances of articles being incorrectly coded as positive, we asked our coders to validate each hit. We did so by presenting them
with short text fragments (snippets) in which the captured term and Trump’s name
were capitalized. Our coders were asked to evaluate whether the capitalized term
was indeed pejorative, as to identify incorrectly captured words. In Italian articles
for example, the search string “Nazi” incorrectly returned the word “nazionale” (na
tional). We then asked whether the term was linked to Trump through a label,
comparison, or a general association. In this phase, texts such as “Trump meets with
authoritarian leader Kim Jong Un” were recoded as negative. Finally, we asked all
coders to code the same subset of English snippets (N ¼320), which confirmed that
coders worked according to the same criteria (Krippendorff’s alpha ¼ 0.75)
Ultimately, these endeavors resulted in a dependent variable where “1” indicated
that an article contained pejorative language in relation to Donald Trump and “0
that it did not.",unclear if they coded everything?,External: Human Annotated Scores,No
2022-11-23T14:04:57Z,Brevity is the Soul of Twitter: The Constraint Affordance and Political Discussion,"Jaidka, Kokil; Zhou, Alvin; Lelkes, Yphtach","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,"Quality of political discussion (Uncivil behavior, Deliberative behavior)",Toxiticy,"Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary","uncivil word dictionary, Linguistic Inquiry and Word Count",NA,Yes,several concepts measured within the same study,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Brevity_Jaidka,",10.1093/joc/jqz023,Journal of Communication,2019,2,a,"For the incivility and the deliberative classifiers developed in this study, machine
learning methods were implemented to train logistic regression classifiers on the
language of 6,000 tweets, labeled by four annotators. The classifiers used a weighted
function of the normalized frequency distribution of words and phrases in a tweet to
assign a 0 or 1 label for the presence of incivility and each deliberative attribute. We
validated the classifiers against held-out, labeled data in 10-fold cross-validation and
obtained an average accuracy of 79% across all the categories. Details on the intercoder agreement and the predictive performance are provided in Table 4 and 5 of the
Supporting Information.",hand-coding,External: Human Annotated Scores,Yes
2022-11-23T14:04:57Z,Brevity is the Soul of Twitter: The Constraint Affordance and Political Discussion,"Jaidka, Kokil; Zhou, Alvin; Lelkes, Yphtach","Yes, Continue with Coding",NA,Social Media: Twitter,English,NA,"Quality of political discussion (Uncivil behavior, Deliberative behavior)",Toxiticy,"Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary","uncivil word dictionary, Linguistic Inquiry and Word Count",NA,Yes,several concepts measured within the same study,NA,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Brevity_Jaidka,",10.1093/joc/jqz023,Journal of Communication,2019,2,b,"For measuring politeness and offensiveness, we used supervised machine learning
models, trained on hand-annotated data developed by Danescu-Niculescu-Mizil
et al. (2013) and Davidson et al. (2017). The politeness API has also been extensively
used by the computer science research community (e.g., Althoff, Danescu-Niculescu-Mizil, & Jurafsky, 2014; Jongeling, Sarkar, Datta, & Serebrenik, 2017; Tan, Niculae,
Danescu-Niculescu-Mizil, & Lee, 2016) in linguistic analyses of social media text.
The classifier and the annotated data set on offensiveness have been validated
in subsequent studies (Almeida, Souza, Nakamura, & Nakamura, 2017; Olteanu,
Talamadupula, & Varshney, 2017).
For measuring swear words and informality, we used the dictionaries provided
by LIWC 2015 (Pennebaker, Boyd, Jordan, & Blackburn, 2015b), a computerized
program developed by psychologists to automatically categorize words in a text (Pennebaker, Booth, et al., 2015). Dictionaries of LIWC have been validated in subsequent
language analyses of social media posts.1 Further details about the supervised and
unsupervised methods used to mine these features are provided in the Supporting
Information.",reference to previous validation,Unsure,No
2022-11-14T14:02:16Z,Machine Learning Predictions as Regression Covariates,"Fong, Christian; Tyler, Matthew","Yes, Continue with Coding","method development, but application of use case",Social Media: Reddit,English,NA,uncivil replies,Toxiticy,Supervised: Machine Learning,SMV,NA,No,NA,Yes,Yes,Yes,Yes,lukas.birkenmaier@outlook.de,"Machine_Fong,",10.1017/pan.2020.38,Political Analysis,2021,1,a,"This SVM achieves a precision for the incivility
label of 0.36, a recall of 0.53, and an overall accuracy of 0.75. We use this fitted SVM to predict
whether each document in the validation and primary samples is uncivil.",hand-coding,External: Human Annotated Scores,No
