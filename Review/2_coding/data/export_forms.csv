Zeitstempel;This is the title of the Study;These are the Authors;"Please scan the text. 
Does the study fulfil the eligibility criteria?
- Empirical Study
- Field of Political Communication
- Using Text as Data Method to operationalise a construct of interest 

You may come back to this question if you have started to fill in some fields as well";If no: Why is the study out of scope?;What is the data source? ;What is the data language?;If others: Name all languages in the data? If there is more than one language assessed, please separate languages by a comma (,);"Please name the target construct as described by the authors)
Note: Please use the exact wording of the authors, such as ""affective polarization"", ""semantic complexity"".
If there is more than one construct studied, please separate constructs by a comma (,)";What Type(s) of method are initially applied? ;What is the name of the method? If there is more than one method applied, please separate methods by a comma (,);What is the outcome of the method?;Do you have any comments on the study design? ;If yes: What are your comments on the study design?;Are steps of validation reported? ;"In what paragraphs is the step of validation / robustness checks / […] reported? 
Please copy the paragraph/description into the open field, so that the whole validation procedure is copied! ";"What step of validation exercises is reported? 
Please report the brief description by the authors (e.g., in the abstract, in the introduction, or at the beginning of the method/validation section). This can also include just a term (e.g. face validity), but should not be longer than one sentence.
";"What type of Validation exercise does this refers to? 
For a detailed discussion on the different categories, please refer to the Coding Manual";Is there another validation step reported in the paper?;"In what paragraphs is the step of validation / robustness checks / […] reported? 
Please copy the paragraph/description into the open field!";"What step of validation exercises is reported? 
(As described by the authors)";"What type of Validation exercise does this refers to? 
For a detailed discussion on the different categories, please refer to the Coding Manual";Is there another validation step reported in the paper?;"In what paragraphs is the step of validation / robustness checks / […] reported? 
Please copy the paragraph/description into the open field!";"What step of validation exercises is reported? 
(As described by the authors)";"What type of Validation exercise does this refers to? 
For a detailed discussion on the different categories, please refer to the Coding Manual";Is there another validation step reported in the paper?;"In what paragraphs is the step of validation / robustness checks / […] reported? 
Please copy the paragraph/description into the open field!";"What step of validation exercises is reported? 
(As described by the authors)";"What type of Validation exercise does this refers to? 
For a detailed discussion on the different categories, please refer to the Coding Manual";Is there another validation step reported in the paper?;"In what paragraphs is the step of validation / robustness checks / […] reported? 
Please copy the paragraph/description into the open field!";"What step of validation exercises is reported? 
(As described by the authors)";What type of Validation exercise does this refers to? ;;Is there another validation step reported in the paper?;In what paragraphs are issues of measurement validity / robustness checks / […] reported? Please copy them into the open field! ;"What step of validation exercises is reported? 
(As described by the authors)";What type of Validation exercise does this refers to? ;Is there another validation step reported in the paper?;Is there a link to an Appendix/additional Materials/GitHub repository? ;"
Is the data for the analysis accessible? ";Is the code for the analysis accessible?;Any Comments on Open Science Standards?;E-Mail-Adresse
27.07.2022 11:13:56;Platforms for Incivility: Examining Perceptions Across Different Media Formats;Sydnor, Emily;No;experimental design;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
27.07.2022 11:21:40;Mapping Political Communities: A Statistical Analysis of Lobbying Networks in Legislative Politics;"Kim, In Song; Kunisky, Dmitriy";No;Only identification of texts using words, method is network analysis;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
27.07.2022 11:45:22;Presenting News on Social Media: Media logic in the communication style of newspapers on Facebook;"Welbers, Kasper; Opgenhaffen, Michaël";Yes, Continue with Coding;;Social Media: Facebook;Others;Dutch;Subjective Language (Subjectivity and Polarity);Rule-based: Off-the-shelf dictionary;FROG software, subjectivity lexicon for Dutch adjectives;Classification;Yes;Data comes from Newspaper Articles on Facebook;Yes;As illustration, Table 1 shows the results of the tool for several example sentences. The first two sentences describe a negative event, but the first sentence only states the facts, whereas the second sentence contains the adjective horrible, which is a negative subjective evaluation. Thus the first sentence is considered neutral, whereas the second is very subjective (1.00) and very negative (1.00). In the third sentence we see a combination of one positive and two negative words, which causes a moderate negative polarity (–0.66). Sentences four to eight show how different words have different intensities, and how negations (“not”) and intensifiers (“very”, exclamation marks) affect results.;Visual Inspection;Content Validation (e.g., Human Judgement);No;;;;;;;;;;;;;;;;;;;;;;No;;;;
27.07.2022 12:20:52;Measuring Polarization with Text Analysis: Evidence from the UK House of Commons, 1811–2015;Goet, Niels D.;Yes, Continue with Coding;;Politics: Parliamentary Records;English;;Polarization;Supervised: Machine Learning, Unsupervised: Text Scaling;wordfish, wordshoal, SGD (stochastic gradient descent);Classification, Scaling;Yes;Comparison of different methods. However, still empirical application ;Yes;An important quality of meaningful estimates from text-based measures of polarization is that they should correspond with our expectations of how the measure develops over time. As part of this face validity criterion, the general test (1.1) considers the stability of estimates within parliaments. One element of immediate concern is the implausibly high level of variability shown in the estimates within parliaments for the Wordshoal-based measure (Figure 2). The changes are dramatic, suggesting an almost random pattern of switches between high and low levels of polarization that are uncorrelated between sessions. By contrast, the measures derived from the classifier yield more stable results (Figure 3), with relatively high correspondence between sessions within a parliamentary term that appear to map onto a “stable political space”.;Visual Inspection of measures over time;Content Validation (e.g., Human Judgement);Yes;At a more granular level, the detailed test (1.2) similarly suggests that the machine classifier is able to produce estimates that correspond to our a priori expectations in a way that Wordshoal cannot. Figure 3 shows that this first measure corresponds well with important historically identifiable outliers in polarization (Table 2). Polarization grows in the wake of the Corn Laws (Table 2, id 1) and of the 1832 Reform Act (id 2), and is generally higher in the period aer 1880 (ids 3 & 4). The formation of the Liberal Unionist party in 1886 appears to mark the start of a period during which members did not fall consistently within their party label, which explains the rather dramatic drop in that year. Although they generally agreed with the Conservatives on Ireland, they were still classed as “Liberals” (at least for part of the time), which makes aggregate polarization look very low.;Comparison of measures with expected levels of polarization;Content Validation (e.g., Human Judgement);Yes;Our second set of tests focuses on the convergence of the estimates with an exogenous measure. First, to investigate the correspondence with session-level estimates (2.1), comparable data are only available for the period aer 1945, for which we can analyze the convergent validity of our estimates with the rile score (Laver and Budge 1992) of the parties based on the CMP data (Volkens et al. 2016).33 Figure 5 above presents a visual comparison.34 Here, the sessional score for the year preceding the election year is matched with the CMP scores.;Convergent validation with rile score of parties;External Validation: Other Scores / Measures;Yes;A more promising level for comparison is that of individual estimates (2.2), for which we can rely on data from the 1992 wave of the British Candidate Survey (BCS) (Norris and Lovenduski 1995).36 The BCS asks respondents to rank themselves on a seven-point ordinal le-to-right scale. I match these records from the 1992 wave—the availability of which is, of course, limited by response rates—with my own MP-level estimates (taking their maximum prediction value). I do so for the first session of the 1992–1997 parliament, as this is closest (in time) to when MPs responded to the survey (i.e. in 1991). The results (Figure 5) show that the estimates correlate most strongly with the classifier results (ρ = 0.43), but followed closely by Wordshoal (at ρ = 0.42). Naturally, we cannot extrapolate to the full sample, but these results are nevertheless encouraging. Specifically, they suggest that the machine classifier is, similar to Wordshoal, able to produce results that bear a close relationship to the position that legislators give themselves on a le-to-right scale;comparison with survey data;External Validation: Other Scores / Measures;Yes;Examining the stability of the estimates over time (between-session consistency test (3.1)), allows us to establish whether the estimated positions reflect long-held political views of legislators, or, alternatively, represent issue-specific divergences. Such stability is crucial when it is our intention to use a polarization measure in a substantive application, i.e. to test hypotheses that relate to political phenomena across extended time periods. It ensures that the measure is comparable over time, that is, that it relates to the same construct rather than to issue-specific divergences. To assess between-session consistency (or: stability), we consider the correlation from one year to the next for legislators in each parliament. Fi;Assessment of stability over time;Content Validation (e.g., Human Judgement);;Yes;Finally, the validation framework prescribes an investigation of the explanatory power of the party (3.3): do these text-based approaches capture anything beyond simple governmentopposition dynamics? To evaluate this question, we take a simple linear model for each session where we regress individual scores on their party’s mean position. These measures are plotted in Figure 9. It is clear that the SGD algorithm is not simply capturing party affiliation. There is a relatively strong correspondence between party position and label, but the levels of the R 2 show that some unexplained variation remains. It has a mean of 0.39 across the sample, and a minimum and maximum of 0.01 and 0.95 respectively.38 We obtain different results for the Wordshoal estimates. Here the range is [1.09e − 8, 0.55] with a mean of 0.06. These values are implausibly low and reaffirm our findings above that the division between parties in this approach less clearly reflects political affiliation.;Explanatory power in regression;External Validation: Other Scores / Measures;No;Yes;Yes;Yes;;
28.07.2022 16:14:52;The Mobilizing Effect of Parties' Moral Rhetoric;Jung, Jae‐Hee;Yes, Continue with Coding;;Politics: Party Manifestos;English;;moral rhetoric;Rule-based: Adjusted dictionary;Moral Foundations Dictionary (MFD);Classification;No;;Yes;Table SI3.1 shows randomly chosen examples of moral and non-moral quasi-sentences identified by the text analysis. The first column lists moral quasi-sentences, and the second column lists non-moral quasi-sentences. The examples show that quasi-sentences that are coded as moral involve either nuanced or overt appeals to morality. Meanwhile, quasi-sentences that are identified as non-moral are of various types: some are factual statements about policy and others are pragmatic, instrumental types of appeal. These examples suggest the face validity of Moral rhetoric.;Visual Inspection of Sentences;Content Validation (e.g., Human Judgement);Yes;I have checked the validity of Moral rhetoric in more systematic ways. First, I have checked that Moral rhetoric is consistent with conventional wisdom on the moral foundations of leftist voters and rightist voters. One of the most notable findings in the literature on the moral foundations theory is that the left and the right have different moral worlds (Graham, Haidt and Nosek 2009). That is, leftists tend to prioritize the care/harm and fairness/cheating foundations (i.e., individualizing foundations) over the authority/subversion, loyalty/betrayal, and sanctity/degradation foundations (i.e., binding foundations). Rightists, on the other hand, have a more even emphasis on individualizing and binding foundations. If so, my measure should pick up this pattern since parties are likely to use language that aligns with the moral worlds of their base. Therefore, I classified the ideology of the parties in my data using the ‘rile’ score from the Manifesto Project (Volkens et al. 2016). I coded parties with a score smaller than or equal to the mean value in the data as left-wing and parties with a score larger than the mean value as right-wing. Then, I checked whether left-wing parties have a higher percentage of quasi-sentences appealing to the care/harm and fairness/cheating foundations than the other three foundations. I find that left-wing parties indeed have higher proportions of quasisentences appealing to the individualizing foundations (mean difference = 0.02, p = 0.00 in a paired one-tailed t-test). I also checked whether right-wing parties have an even usage of individualizing and binding foundations. Results show that right-wing parties have no difference in the proportion of quasi-sentences appealing to the care/harm and fairness/cheating foundations and the proportion of quasi-sentences appealing to the loyalty/betrayal, authority/subversion, and sanctity/degradation foundations (mean difference = −0.01, p = 0.42 in a paired two-tailed t-test). The results are plotted in Figure SI3.1. These patterns remain the same when I code parties with a rile score of ‘0’ or below as left-wing and the rest as right-wing. I have also checked whether (1) left-wing parties appeal to individualizing foundations more than right-wing parties do and (2) right-wing parties appeal to binding foundations more than left-wing parties do. These are the expectations one might have based on Lipsitz (2018)’s finding that (1) liberal candidates (in the U.S. context) are more likely to emphasize the care and fairness foundations in their ads than conservative candidates do and (2) conservative candidates are more likely to emphasize the authority, loyalty, and sanctity foundations than liberal candidates do. In my data, I find that left-wing parties’ emphasis on individualizing foundations is one-percentage point higher than right-wing parties’ emphasis on individualizing foundations and that the difference is not statistically significant (p = 0.27 in a one-tailed t-test). On the other hand, I find that right-wing parties’ emphasis on bindings foundations is three-percentage points higher than left-wing parties’ emphasis on binding foundations and that the difference is statistically significant (p = 0.06 in a one-tailed t-test). The results are plotted in Figure SI3.2. Although the results are partially supportive of the expectations, I do not believe that I need to take great stock of the results because the difference in individualizing (binding) appeals between left-wing parties and right-wing parties does not necessarily have to play out in political texts. For example, suppose that a left-wing party A devotes 20% of its manifesto to moral appeal while a right-wing partyB devotes 60%. Party A uses 15% on individualizing appeals and 5% on binding appeals, while party B uses 30% on each type of foundation. In this case, right-wing party B appeals more to individualizing foundations than left-wing party A does, not because party B fundamentally prioritizes individualizing foundations more than party A does, but because party B is using more moral rhetoric overall. That could be why Lipsitz (2018) finds that (1) Democratic ads appeal significantly more to individualizing foundations than Republican ads do in presidential elections, but not necessarily in non-presidential elections and that (2) Republican ads contain significantly more binding appeals than Democratic ads do in House, Senate, and gubernatorial elections, but not necessarily in presidential elections.;correlation left-right dimension (rile score) of manifesto project;External Validation: Other Scores / Measures;Yes;Second, I have assessed both convergent and discriminant construct validity by looking at the relationship of my measure with a measure that is likely to be positively correlated but distinct: sociocultural issue emphasis. If my measure captures moral rhetoric well, it should have a positive correlation with the extent to which a party emphasizes social issues. Although moral rhetoric can emerge in a wide range of policy areas, intuition suggests that social issues like culture and family policy are more conducive to moral rhetoric than economic issues like trade and jobs because of the nature of the issue area. Hence, there should be a positive correlation between moral rhetoric and sociocultural issue emphasis. However, moral rhetoric is a different concept from sociocultural issue emphasis, so the positive correlation should not be high if my measure is measuring what it is supposed to measure. To test this, I calculated the proportion of sociocultural quasi-sentences for all the manifestos in my data. To identify sociocultural quasi-sentences, I followed the categorization developed in Tavits and Potter (2015). The correlation I find between Moral rhetoric and sociocultural issue emphasis is 0.21. The correlation is significant in a one-tailed test (p = 0.03). This provides evidence in support of the construct validity of Moral rhetoric.;Convergence with sociocultural issue emphasis;External Validation: Other Scores / Measures;Yes;"Third, I have checked whether moral rhetoric is used more in the sociocultural dimension than the economic dimension. This is again based on the intuition that moral rhetoric is more easily used in sociocultural issues than economic issues. I have classified the quasi-sentences into the two dimensions using the classification developed in Tavits and Potter (2015). I find that parties do have a higher proportion of moral quasi-sentences among quasi-sentences in the sociocultural dimension than among quasi-sentences in the economic dimension. The mean proportion in the sociocultural dimension is 0.43; the mean proportion in the economic dimension is 0.31 (mean difference = 0.12, p = 0.00 in a paired one-tailed t-test).";Predictive power on sociocultural dimension;External Validation: Other Scores / Measures;Yes;Fourth, I find that my measure of moral rhetoric does not have a systematic correlation with ideology, indicating that moral rhetoric is a distinct concept rather than a proxy for ideological extremism. The correlation between Moral rhetoric and the ‘rile’ score is 0.10 and not statistically significant (p = 0.38). Moreover, Figure SI3.3 shows that there is no Ushaped relationship. I also find that Moral rhetoric is not systematically different between mainstream parties and niche parties. The mean for mainstream parties is 0.29, and the mean for niche parties is 0.30 (p = 0.79 in a two-tailed t-test).2;discriminant validity;External Validation: Other Scores / Measures;;Yes;Lastly, I find some evidence that the level of moral rhetoric reflected in my manifestobased measure is played out in campaign speech data. With the dictionary I used to create Moral rhetoric, I created a speech-based measure of moral rhetoric at the sentence level for ten parties that I could find data on: the two major parties in the 2004, 2007, 2010, and 2013 Australian elections3 and the two major parties in the 2011 New Zealand general election.4The correlation between Moral rhetoric and the speech-based measure is 0.82 (p = 0.00).;convergent validation with speech-based data;External Validation: Other Scores / Measures;No;Yes;Yes;Yes;;
01.09.2022 18:18:17;Platforms for Incivility: Examining Perceptions Across Different Media Formats;Sydnor, Emily;No;There is not text analyzed (the text is just one of the treatment conditions/manipulations);;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
01.09.2022 19:23:55;Measuring Polarization with Text Analysis: Evidence from the UK House of Commons, 1811–2015;Goet, Niels D.;Yes, Continue with Coding;;Politics: Parliamentary Records;English;;polarization (or ideological standpoint);Supervised: Machine Learning, Unsupervised: Text Scaling;Text scaling: Wordfish, Text scaling: Wordshoal, Machine learning: New machine classification approach;;No;;Yes;"First, we consider face validity, which includes both a general and a detailed benchmark. The general test is a quick and simple impression of the distribution of the estimates
over time [...] However, as (Quinn et al. 2010, 216) rightly note, “[f]ace validity is inherently subjective, generally viewed as self-evident by authors and with practiced skepticism by readers”. This does not mean that face validity cannot be useful, if applied in a consistent way. A second, and more important face validity criterion of my framework therefore is that the measure must pass a detailed “historical test”:";General test: The level of variability between sessions within the same parliament should be at a reasonable level AND Detailed test: Outliers in our estimates should correspond with a priori expectations derived from authoritative (secondary) sources.;Others;Yes;Second, the framework considers how well our estimates converge with results obtained with supervised methods, at two levels. First, at the aggregate level (here defined as the yearly parliamentary session), we should expect that levels of polarization correlate with other exogenously defined measures:;Session-level test: The level of polarization in sessions should correspond well with exogenously defined measures of polarization. AND Individual-level test: The positions of MPs should correlate with their own left–right placement from an exogenous dataset.;Others;Yes;Third, the framework evaluates three measures of consistency relating to construct validity. First, we evaluate the variation in the position of MPs from session to session. We may expect the ideal point of legislators on a one-dimensional scale to vary somewhat because the agenda will include different items for each session. However, legislators should otherwise remain relatively consistent in their overall position across the issues discussed over the course of a parliamentary session.;Between-session consistency: The positions of MPs should correlate at a reasonable level between successive sessions. AND The Empirical Cumulative Distribution Function (ECDF) of individual-level estimates should show a reasonable separation of parties and key individuals should be placed as expected. AND Explanatory power of the party label: The variation in individual-level estimates should be a good predictor of the party label for each session.;Others;No;;;;;;;;;;;;;;Yes;No;No;;
01.09.2022 19:51:07;Twitter made me do it! Twitter's tonal platform incentive and its effect on online campaigning;"Mueller, Samuel David; Saeltzer, Marius";Yes, Continue with Coding;;Social Media: Twitter;English;;negative tone (negative incentive);Supervised: Machine Learning;Sentiment analysis;;No;;Yes;First, we test the accuracy of the dictionary on coded data;To validate our sentiment dictionary measurement, we had two coders code sentiment of a sample of 2000 tweets. Tweets were coded following a code book devel- oped by Mohammad (2016), separating them in five categories: positive, negative, neu- tral, sarcastic, and ambivalent. We then compared the sentiment score computed by the dictionary, coded down to the categorical level (negative tone < 0, positive tone > 0) to all tweets in the first three categories. We achieved a classification accuracy of 0.7 (see appendix for details).;External: Human Annotated Scores;Yes;second, we use machine learning to classify the rest of the tweets;"For testing our hypotheses, 2000 tweets are not nearly enough. More data is needed to estimate the negative binomial mixed models or having stable estimates on the individual level with more than 300 individuals. To extend the validation from coding to all obser- vations, we used a Naive Bayes classifier from the Quanteda package to classify tweets into three categories: neutral, negative, and positive. After training the classifier on the 2000 coded tweets, we used it to classify the remaining 95,909 tweets.
Since the measurement is now categorical and not continuous from −1 to 1, coeffi- cients are not directly comparable. To produce comparable output, we created a categ- orical measurement based on the sentiment analysis. Using the new categorical measurements of tone, we performed an identical analysis to our main analysis to see whether our previous results are supported by (1) the categorical measurement based on the sentiment analysis and (2) by the completely unrelated measurement produced by the Naive Bayes classifier.";External: Scores from other CATM;No;;;;;;;;;;;;;;;;;;Yes;No;No;;
04.10.2022 06:49:29;Mapping Political Communities: A Statistical Analysis of Lobbying Networks in Legislative Politics;"Kim, In Song; Kunisky, Dmitriy";No;"Seems that they don't really apply a computer-assisted text method; more so they seem to just count specific words in a Congress data set and then apply their so called ""Bipartite Link Community Model"" (biLCM).";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;david.gruening@gesis.org
05.09.2022 14:47:56;Mapping Political Communities: A Statistical Analysis of Lobbying Networks in Legislative Politics;"Kim, In Song; Kunisky, Dmitriy";No;only network analysis;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;lukas.birkenmaier@outlook.de
05.09.2022 15:26:00;Estimating Spatial Preferences from Votes and Text;"Kim, In Song; Londregan, John; Ratkovic, Marc";Yes, Continue with Coding;;Politics: Parliamentary Records;English;;Speech Dimension;New statistical model, most close to text scaling;Sparse Factor Analysis;;Yes;Combining text and votes for estimating spatial locations of MPs;Yes;"Before applying the method, let’s consider the applicability of SFA in this case. First, voting is not always sincere in the US Senate, as there are always motions to recommit, etc. We note, though, that ideal point estimates from the US Congress have been used extensively in other studies and possess high face validity. To be particularly careful, if a bill is voted on several times due to different motions, we only include the final vote in our analysis. Second, we consider the sincerity when speaking. Previous work has shown, albeit in the US House, that floor speeches are expressive rather than deliberative (Hill and Hurley, 2002; Maltzman and Sigelman, 1996). Many floor speeches aren’t even read verbally, but simply entered into the record, also suggesting that floor speeches are vehicles of expression rather than persuasion. For that reason, we feel more comfortable applying the method to floor speeches rather than, say, conference committee meetings";Considerations Applicability Data and Method;Content Validation;Yes;The left figure contains results from the 108th Senate, a Republican-led session during President George W. Bush’s tenure. The right figure contains results from the 112th Senate, a Democratic-led session during President Barack Obama’s time as President. We find a consistent pattern: for the majority party, the most extreme terms relate to parliamentary control words (consent committee, author meet, meet session). For the minority party, the first dimension identifies ideologically relevant terms. For the Democrats during the 108th Senate, these terms included administr, as the Democrats soured on the current Presidential administration, and health, a centerpiece of the Democratic policy agenda. In the 112th Senate, with the Democrats in the majority, parliamentary control terms switched their ideological polarity, aligning with the Democrats ( meet session, consent committee, author meet). The Republican end of this first dimension reflects that party’s programmatic fiscal concerns (budget, stimulus, debt, trillion).;Inspection of most relevant words;Unsure;Yes;Next, we look at the preferred outcomes of legislators from the 112th. Points in Figure 4 are shaded in proportion to their first dimensional DW-NOMINATE score, showing the agreement between SFA and DW-NOMINATE on the first dimension (p ρ « 0.93). The left plot labels party leaders, whips, and top chairmen, showing the close relationship between locations on the second dimension and leadership. The first dimension captures the political battle lines, reflecting legislators left vs right policy differences, while the second, vertical, dimension reflects differences in the terms selected by leaders versus the rank and file members.;Comparison with DW-Nominate Scores for vote choice (https://en.wikipedia.org/wiki/NOMINATE_(scaling_method));External: Criterion data / Predictive validation;Yes;"Scaling results informed by only words (α “ 1). We also apply SFA using only information from words. This is not our preferred model, as it ignores vote data, yet SFA still uncovers structure in the text data. [...] Figure 6 contains the top ten words at each of the first six dimensions of the 112th Senate. We note that the positive and negative level distinction along the y-axis is wholly arbitrary, as we only identify term levels up to a sign. Looking at the first column, we find that the first dimension starts with a set of non-controversial terms. These include parliamentary procedural terms (as opposed to parliamentary control terms) such as today wish, madam rise, and colleague support. Also on the non-controversial side are martial terms with universally positive affect during this Congress such as army, air forc, and deploy. On the other side are words that will be used in to differentiate issues in other dimensions, such as tax, vote, and peopl. The other dimensions have at their extremes words connoting some underlying dimension of policy. For example, the second dimension ranges from judiciary and women’s issues at one end to fiscal concerns at the other; the fourth goes from a broad set of social welfare concerns to the consideration of judicial nominees. These lower dimensions adapt to the issues of the day. Tobacco, for example is present in the 105th Senate; Iraq comes and goes as an issue, and health care goes from dealing with seniors and Medicare in the 107th Senate to dealing with students and families in the 112th. Even without including votes in";Visual inspection of word wheights in a scaling model;Content Validation;No;;;;;;;;;;Yes;Yes;Yes;;lukas.birkenmaier@outlook.de
05.09.2022 15:30:48;Campaigning in the fourth age of political communication. A multi-method study on the use of Facebook by German and Austrian parties in the 2013 national election campaigns;"Magin, Melanie; Podschuweit, Nicole; Haßler, Jörg; Russmann, Uta";No;"only descriptive statistics; texts are hand-coded and not analysed computationally";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;lukas.birkenmaier@outlook.de
05.09.2022 16:00:16;Policy Diffusion: The Issue‐Definition Stage;"Gilardi, Fabrizio; Shipan, Charles R.; Wüest, Bruno";Yes, Continue with Coding;;Newspaper;English;;Policy Framing;Unsupervised: Topic Modeling;Structural Topic Model;;No;;Yes;"Figure 1 shows how topic prevalence is distributed over time across all states; a detailed validation is discussed in SI Appendix C.3.";Visual Inspection of Topic prevalende (supplementary materials can not be found);Content Validation;No;;;;;;;;;;;;;;;;;;;;;;Yes;Yes;Yes;they claim that they provide data and code in the appendix, but this appendix can not be found on the internet;lukas.birkenmaier@outlook.de
06.09.2022 10:45:26;Right-Wing, Populist, Controlled by Foreign Powers? Topic Diversification and Partisanship in the Content Structures of German-Language Alternative Media;"Müller, Philipp; Freudenthaler, Rainer";Yes, Continue with Coding;;Newspaper;Others;German;Topic Diversification;Unsupervised: Topic Modeling;LDA;;No;;Yes;"In the last step, two researchers independently validated and labeled the topics of the 70-topic solution based on an in-depth reading of the documents, using both lists of the most common words in each topic (at k ¼ .6, see Sievert and Shirley 2014, 67) and full articles with the highest k under each topic and coherence indicator to guide their decisions; they discussed their independent assessments to arrive at the final set of topics. During this step, boilerplate topics (which occur in many articles and are not internally coherent, see DiMaggio, Nag, and Blei 2013, 568; Maier et al. 2018, 108) were removed and similar topics were grouped together, resulting in 14 groups of topics encompassing the 66 remaining topics, which are summarized in Table 1";Human Evaluation of Topics by two independent researchers;Content Validation;No;;;;;;;;;;;;;;;;;;;;;;No;;;;lukas.birkenmaier@outlook.de
06.09.2022 11:18:08;Connecting the (Far-)Right Dots: A Topic Modeling and Hyperlink Analysis of (Far-)Right Media Coverage during the US Elections 2016;"Kaiser, Jonas; Rauchfleisch, Adrian; Bourassa, Nikki";Yes, Continue with Coding;;Newspaper;English;;convergence of topics;Unsupervised: Topic Modeling;structural topic model analysis (STA);;No;;Yes;We randomly selected 100 articles and automatically assigned the topic with a probability above 0.49 to the article. Two coders without specific training coded the topics. The intercoder-reliability score between the two coders and the topic model was with a Krippendorff’s alpha of 0.67 acceptable for our case. When the more generic Republican topics are merged the alpha is even over 0.7. This is surprisingly high as most documents have a mix of topics (e.g., economy and republican primaries);Comparison with 100 hand coded articles;External: Human Annotated Scores;No;;;;;;;;;;;;;;;;;;;;;;No;;;;lukas.birkenmaier@outlook.de
06.09.2022 11:44:33;Testing the Validity of Automatic Speech Recognition for Political Text Analysis;"Proksch, Sven-Oliver; Wratil, Christopher; Wäckerle, Jens";No;The study assesses the accuracy of automatic speech transcription. However, no latent constructs are measured using CATM, so the study is out of scope for our analysis;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;lukas.birkenmaier@outlook.de
06.09.2022 15:04:07;Measuring Agenda Setting in Interactive Political Communication;Rossiter, Erin L.;Yes, Continue with Coding;;Politics: Others, Presidential Debates, online discussion (mturk), In-Person Deliberations;English;;Shift in Agenda Setting;Unsupervised: Topic Modeling, Topic Segmentation;LDA, parametric Speaker Identity for Topic Segmentation (SITS);;Yes;Topic Segmentation model builds up on LDA;Yes;To assess if SITS can accurately identify where latent shifts in topic occur, I classify a speaking turn as shifting topic if the posterior probability of a shift is greater than or equal to 0.50. I then compare where shifts were inferred by SITS to where topic changes were prompted by the researchers. SITS identifies 81.40% of the locations that begin a new prompted topic segment by the researcher.7 I check for an SITS-inferred topic shift within two speaking turns after a researcher-prompted shift because often after reading the prompt, the first few speaking turns would simply answer the prompt’s question by saying “yes,” “no,” or “do you want to go first?” Instead of classifying this as a topic shift, SITS would classify a subsequent speaking turn that actually began discussing the topic at hand as a shift. This exercise demonstrates that SITS can accurately identify the speaking turns that should be attributed as shifting the agenda.8 Moreover, SITS provides a more nuanced view of how topics ebbed and flowed in these discussions than if we considered the locations of researcher-prompted topic changes as ground truth.;Comparison latent shifts with gold-standard coding;External: Human Annotated Scores;Yes;"Influential work in computer science proposes that crowdsourced tasks are more useful than traditional metrics to assess if a topic model returns semantically meaningful and distinct topics (Chang et al. 2009). Therefore, I use the “topic intrusion” task proposed by Chang et al. (2009) to validate the topics from a SITS model estimated on 20 U.S. general election presidential debates held between 1992–2016.9
The topic intrusion task presents the human judge with a document, in this case a segment inferred by SITS. The judge is also presented with four word sets. Three of these word sets represent the three highest probability topics for the segment. The fourth word set is the intruder, drawn randomly from the segment’s low probability topics. Each word set contains the top eight frequent and exclusive (FREX) topwords for the topic (Roberts et al. 2014). I set up the topic intrusion task for 200 randomly drawn segments from the debates. Then, Amazon Mechanical Turk (MTurk) workers were asked to choose which word set was most unrelated to the passage. In line with recent work on validation procedures for topic models by Ying, Montgomery, and Stewart (2019), I ran two trials of the same 200 tasks. Figure 2 plots the results for each trial separately as well as the pooled result Workers competed 62% and 68% of the tasks correctly in Trial 1 and Trial 2, respectively. A difference of proportions test indicates that Trial 1 and Trial 2 are not significantly different (p = 0.25).Thisresultiscomparable to one, and better than three, of four models assessed by Ying, Montgomery, and Stewart (2019) using the topic intrusion task. In all, human coders and SITS largely agree about which topics are and are not associated with the inferred segments of the debates.
";topic intrusion from model topics;Content Validation;Yes;To do so, I follow a procedure proposed by Grimmer and King (2011) to evaluate “cluster quality,” which is the similarity of the documents (here, segments of the debates) estimated to belong to the same cluster (here, having similar topic distributions). Importantly, I evaluate SITS segments against segments derived from a hand-coding approach. Hand-coded data are from Boydstun, Glazier, and Pietryka (2013) for the 1992, 2004, and 2008 U.S. general election presidential debates. Boydstun, Glazier, and Pietryka hand code several variables from the debate transcripts, including the topic of each question posed to the candidates and the topic of each phrase in the candidates’ responses. Then, they deem a candidate as going “off-topic” and thus, engaging in agenda-setting behavior, if the phrase’s topic does not correspond to the question’s topic. Comparing the segments inferred by SITS to those derived from hand coding required five steps. First, I determined where topic changes occurred (and thus, formed segments of the debates) according to each method. Second, I determined the similarity of these segments according to each method’s topic assignments.10 Third, I set up the exercise outlined by Grimmer and King (2011). Separately with the segments from the SITS and the hand-coding approaches, I drew 25 random pairs of segments with the same most-assigned topic and 25 random pairs of segments with a different most-assigned topic.11 Fourth, four unique MTurk workers rated the similarity of the segments within each pair on a 3-point scale: (1) unrelated, (2) loosely related, or (3) closely related.12 Of interest is each method’s “cluster quality,” which is “the average similarity of pairs of documents from the same cluster minus the average similarity of pairs of documents from different clusters, as judged by human coders one pair at a time” (Grimmer and King 2011, p. 5). The fourth and final step is to use difference in means to compare the two methods. Figure 3 plots this point estimate along with the 80% (thick line) and 95% (thin line) confidence interval. The estimated difference between approaches is positive and significant. Therefore, the Grimmer and King (2011) evaluation suggests SITS can infer segments that are even more coherent than those derived from a hand-coding approach.;assessing the interrelated nature of where shifts in topic occur and the topics themselves by examining the resulting segments of an interaction. Using crowdsourced human judgments, I find that SITS segments are viewed as more coherent than segments derived from a hand-coding approach.;External: Human Annotated Scores;Yes;In particular, the results for 1992, 2004, and 2008 are in line with results from the hand-coded content analysis. Boydstun, Glazier, and Pietryka (2013) note that in 1992 and 2008, the economy was a salient issue, but in 2004, defense was uniquely more important to the public than the economy. These patterns hold in Figure 4, as we see the 2004 election discussed the economy less than any other election;Face Validation by drawing on knowledge on the salient political issues;Content Validation;No;;;;;;;;;;Yes;Yes;Yes;;lukas.birkenmaier@outlook.de
06.09.2022 15:43:20;A Pairwise Comparison Framework for Fast, Flexible, and Reliable Human Coding of Political Texts;"Carlson, David; Montgomery, Jacob M.";No;Introduction of a platform to conduct pairwise comparison;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;lukas.birkenmaier@outlook.de
06.09.2022 15:51:05;Twitter and News Gatekeeping: Interactivity, reciprocity, and promotion in news organizations’ tweets;Russell, Frank Michael;No;only hand coding for variables, quantitative analysis only includes descriptive statistcis, such as likes and retweets;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;lukas.birkenmaier@outlook.de
06.09.2022 17:04:50;Machine Translation Vs. Multilingual Dictionaries Assessing Two Strategies for the Topic Modeling of Multilingual Text Collections;"Maier, Daniel; Baden, Christian; Stoltenberg, Daniela; De Vries-Kedem, Maya; Waldherr, Annie";Yes, Continue with Coding;;Newspaper, Social Media: Twitter;Others;Hebrew, Arabic, English;Topics;Unsupervised: Topic Modeling, Rule-based: Adjusted dictionary;Structural Topic Models (STM);;Yes;"Two ""Preprocessing"" steps are compared to deal with multilingual texts for LDA";Yes;"The dictionary was repeatedly validated and improved until it achieved precision and recall scores well in excess of .75 in each language (see Baden & Stalpouskaya, 2015; Tenenboim-Weinblatt & Baden, 2021 for more detail). As the dictionary uses structural features of the documents (e.g., word distances, syntactic structure) for concept identification and disambiguation,8 the dictionary was applied to the unprocessed text";reference to previous validation excercise;Content Validation;Yes;"Following the modeling stage, the obtained models were validated and compared using both quantitative and qualitative procedures. For the quantitative comparison, we calculated the extent to which different topic models obtained from the respective methodological approaches break down the variation included in each corpus in similar ways. To do so, we use the Correlation Matrix Distance (CMD), a distance measure designed to determine the (dis)similarity between two quadratic correlation matrices (Herdin et al., 2005; Motta & Baden, 2013). Theoretically, the CMD ranges from one (no association between both matrices) to zero (both correlation matrices are identical, up to a scale factor). For this estimation, we depart from the compared topic models’ n � k-sized document-topic (θÞ matrices, wherein each of the n ¼ 1; . . . ; N rows correspond to a document in the respective corpus, and each of the k ¼ 1; . . . ; K columns to a topic in the respective topic model. In these matrices, the cell entries θnk represent the probability of document n to contain a given topic k – usually interpreted as the topic proportion that the respective document contains. Since the identity of the compared topics is uninformative for the comparison (i.e., one can permutate the order of topics (columns) in each matrix without changing the structural similarity), we transform the raw θ matrices into quadratic correlation matrices by calculating the dot product of θ with its transpose θT. As a result, we obtain, for each topic model, an n � n matrix C that reflects the extent to which different documents are composed of the same topics. The CMD then computes the distance between each pair of matrices C, which provides a direct measure of the extent to which the topics obtained by two compared topic models are distributed in (dis)similar ways over the same set of documents. Since the dimensionality of C depends solely on the number of the n documents, this transformation allows us to compare the θ’s of different topic models independently of their number of topics K. Formula 1 defines the CMD for the two correlation matrices CMTand CMD that result from the two methods under investigation, as denoted by their subscripts";calculated the extent to which different topic models obtained from the respective methodological approaches break down the variation included in each corpus in similar ways;Unsure;Yes;"For the qualitative comparison, we first selected one best-fitting model per corpus and approach. Among all estimated topic models, we focused on those five models that offered the best fit based on the evaluation metrics offered by the STM package (K ¼ 10; 15; 20; 25; 30 f g for both corpora and both methods). Two of the authors jointly judged these models’ interpretability, inspecting their top associated words (MT approach) and concepts (MD approach). Based on this information, we selected for the News corpus those topic models obtained for K = 25 (independently for both MT and MD), and for the Twitter corpus those models obtained for K = 30, for further validation. For these selected models, all topics were then carefully labeled and validated by the same two researchers. To ascertain the validity of topics and their labels, we selected for every topic a random sample of ten documents with a topic probability above a threshold of t ¼ 0:3. Given that only few topics are prevalent in a single document, the threshold can be considered sufficiently high for a topic to be the most prevalent in the respective documents (see also Maier et al., 2018). Topics were confirmed as interpretable and labeled if, through a discursive process, both judges agreed on the same interpretation, which had to be supported both by the identified top words/concepts and by the inspected documents, read against our familiarity with the conflict and the referenced events.";Jugdement of models’ interpretability, inspecting their top associated words (MT approach) and concepts (MD approach);Content Validation;No;;;;;;;;;;;;;;Yes;No;No;;lukas.birkenmaier@outlook.de
06.09.2022 17:41:22;Information Credibility under Authoritarian Rule: Evidence from China;Chang, Charles;No;only hand coding of texts;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;lukas.birkenmaier@outlook.de
07.09.2022 09:20:11;Predicting Partisan Responsiveness: A Probabilistic Text Mining Time-Series Approach;"Bustikova, Lenka; Siroky, David S.; Alashri, Saud; Alzahrani, Sultan";Yes, Continue with Coding;;Party Websites;Others;Slovak (slovakia);Partisan Responsiveness;Supervised: Machine Learning, Unsupervised: Topic Modeling;LDA Topic Model, sparse-learning classifier (SLEP);;Yes;"two step approach; First LDA, then Classification";Yes;Determining the number of topics can be done using various methods (e.g., elbow curves, AIC, BIC, etc.). Among these approaches, LDA tends to be most resilient when the number of topics, k , increases (Blei et al. 2010). However, larger k imposes additional computational costs and makes convergence of the posterior probability estimate more difficult. Finding the right k also requires qualitative validation by experts. Aer multiple trials, we determined that the most applicable k was 100. Later, we determined that the results are robust to minor changes to k (;"Topic Model: ""qualitative evaluation""";Content Validation;Yes; Table 1, in the Supplementary materials, shows the accuracy for predicting ethnic party spikes that lead to radical right party responses. The accuracy varies between 81% and 89% for different issues (F-measure). Table 2 shows the accuracy for predicting ethnic spikes that the radical right parties ignore, which varies between 78% and 84% (F-measure). The average F-measure for predicting outcomes of ethnic spikes is therefore 82.9%. Similarly, Table 3 shows the accuracy for predicting ethnic party responsiveness to radical right party spikes, which varies between 80% and 86% depending on the issue (F-measure). Table 4 shows the accuracy for predicting radical spikes that are ignored by the ethnic parties, which varies between 78% and 86% (F-measure). The average F-measure for predicting outcomes of radical spikes is about the same: 82.7%.;Accuracy/F1 Score of ML classifier on topic escalation on the basis of LDA topic (changes);External: Scores from other CATM;No;;;;;;;;;;;;;;;;;;Yes;Yes;Yes;;lukas.birkenmaier@outlook.de
07.09.2022 11:39:30;Computational Identification of Media Frames: Strengths, Weaknesses, and Opportunities;"Nicholls, Tom; Culpepper, Pepper D.";Yes, Continue with Coding;;Newspaper;English;;Frame Identification;Supervised: Machine Learning, Unsupervised: Topic Modeling, Unsupervised: K-Means Clustering, Unsupervised: Evolutionary Factor Analysis;K-Means, Evolutionary Factor Analysis, Structural Topic Model (STM);;Yes;comparison of different methods to identify media frames;Yes;We analyze two separate versions of this USA Today dataset. The first is the full collection of 4,653 news articles, covering all aspects of banking (and, indeed, containing some articles about non-bank related entities such as Red Bank (New Jersey), Tyra Banks, and the overflowing banks of various rivers). The second is a smaller 2,394 article subset produced by aggressively filtering out articles from the full dataset that match keywords associated with non-bank entities and subjects6. Thus, we have one narrow dataset (Australia Royal Commission) and one broad dataset (banking in USA Today), with the latter having both a filtered and unfiltered version. The value of using two contrasting kinds of data in this context is that they reflect two ends of a range of possibilities for the frame analyst. On the one hand, the Australian corpus is a fairly conventional tightly demarcated group of articles reflecting a single (extended) story. On the other, the USA Today corpus is much broader. It is the kind of dataset that would;Using two datasets to evaluate model performance;Content Validation;Yes;"We also carried out a manual analysis on the Australian dataset, for comparison with the computational methods. The basic structure of the analysis was based upon the procedure recommended by Chong & Druckman (2007, pp. 106–108). In the first stage, the authors specified the event for which frames would be sought; in this case, the Australian banking Royal Commission. In the second stage, the frame-defining attitudes were chosen to be attitudes to the banking scandals. For the third stage, two researchers (the first author and a research assistant well-versed in the Australian banking scandals) worked separately after being briefed on the event and attitudes, and also the Chong & Druckman definition of emphasis frames. Each was issued an identical 300-article randomly ordered random sample of the Australian data, and instructed to inductively identify a framing schema. We do not claim that the results of this manual analysis are “the” gold standard set of frames against which to measure the computational approaches. Given the challenges of operationalizing framing and the low reliability of human coding as a result of researcher effects (Matthes and Kohring, 2008), we offer them as one plausible set of frames, not as the only one. However, they do stand up as reasonably distinct statements of problem definition and causal interpretation of Australian banking scandal of 2018.";manual identification of overarching frames;External: Human Annotated Scores;Yes;"We therefore evaluate the different frame identification methods subjectively, according to the quality of the frames they have selected. There are no quantitative measures of each of these criteria; they are abstract theoretical qualities about which we make judgments. We use three criteria for quality, with the latter two drawing on Gerring (2001) and Roberts et al. (2014). First, the extent to which the frames identified by the methods fit with our understanding of what a media frame is – an interpretive lens that includes the definition of a problem and a diagnosis of its causes (Entman, 1993, p. 52). Second, the extent to which each identified frame is internally coherent. Third, the extent to which each identified frame is externally differentiable from other potential frames.";human evaluation of frames identified by different methods;Content Validation;No;;;;;;;;;;;;;;Yes;Yes;Yes;;lukas.birkenmaier@outlook.de
07.09.2022 13:36:44;Latent Semantic Scaling: A Semisupervised Text Analysis Technique for New Domains and Languages;Watanabe, Kohei;Yes, Continue with Coding;;Newspaper;Others;English, Japanese;sentiment;Unsupervised: Text Scaling;New semisupervised document scaling technique Latent Semantic Scaling (LSS;;Yes;development and application of a new method;Yes;"Following Young and Soroka (2012), I computed mean sentiment scores for articles in the sample to compare the results of manual and machine coding. In Figure 4, the mean scores correspond almost linearly with the manual coding in LSD, while the mean scores for moderately positive (+1) articles are lower than they should in LSS; the standard errors for very positive articles are also smaller in LSD than in LSS; both LSD and LSSscaled negative articles accurately. The overlapped 95% confidence intervals indicate that some of the mean differences are not statistically significant, but their standard errors will be much smaller when the sample is larger. Figure 5 shows the correlation between machine and manual coding in a longitudinal setting. Although the mean sentiment scores can be less accurate in both manual and machine coding as the articles are spread over 30 years, I can observe the strong correlation between them, especially humans and LSS until 1995. However, neither LSD nor LSS replicate negative overall shifts in manual coding from 1993; one of the largest discrepancies between humans and machines can be found in 2006 when the number of articles is the smallest. In this setting, the correlation between";comparison with hand-coded scores;External: Human Annotated Scores;Yes;"However, neither LSD nor LSS replicate negative overall shifts in manual coding from 1993; one of the largest discrepancies between humans and machines can be found in 2006 when the number of articles is the smallest. In this setting, the correlation between humans and machines is r = 0.62 in LSD and r = 0.70 in LSS (Figure 6), although the correlation between LSD and LSS is only r = 0.34.";comparing model estimates (LSS) with dictionary (LSD);External: Scores from other CATM;Yes;"In order to demonstrate how LSS can be used in real research projects, I applied the fitted model to the entire corpus of the English articles (Figure 7). I can confirm that the news articles were largely positive in the period of Reaganomics (1982–1989) but they became very negative after the savings and loan crisis; they became very positive after 1995, when America’s service sector enjoyed prosperity, but very negative after the occurrence of the economic crisis in Asia; the bursting of the dot-com bubble made their sentiment even more negative; the subprime mortgage crisis that triggered the global economic crisis also changed the sentiment dramatically. Figure 8 shows that mean sentiment scores by LSD and LSS are very strongly correlated with each other (r = 0.77) and with the changes in gross domestic product (GDP) of the United States (r = 0.65). Large discrepancies between the mean sentiment scores and the economic indicator are found in 1984 and 1997–1999, but the fall in sentiment in the latter period was caused mainly by the financial crisis in Asia";Face validation of scores on a long time frame;Content Validation;No;;;;;;;;;;;;;;Yes;Yes;Yes;;lukas.birkenmaier@outlook.de
07.09.2022 14:12:21;The Automatic Analysis of Emotion in Political Speech Based on Transcripts;"Cochrane, Christopher; Rheault, Ludovic; Godbout, Jean-François; Whyte, Tanya; Wong, Michael W.-C.; Borwein, Sophie";Yes, Continue with Coding;;Politics: Parliamentary Records;English;;emotional content;Supervised: Machine Learning, Rule-based: Off-the-shelf dictionary, Rule-based: Development dictionary;Lexicoder 3.0, Sentiwordnet 3.0, Hu-Lui Lexicon, Jockers-Ringers Lexicon, VADER;;Yes;comparison of different methods to analyse parliamentary data on the sentence level;Yes;"Figure 3 compares tools that we tested for predicting the sentiment scores of our human text coders, including the dictionary induced using word2vec embeddings described above. For each measure, the confusion matrix overlays a jitterplot to show the proportional reduction in error alongside the classification accuracy of the different tools. For the measure of accuracy, we consider a classification successful if the method produces a score in the positive range – past the midway point – while the human coders also coded a text as positive on average; the opposite must be true for negative predictions. This corresponds to the percent correctly predicted commonly used in binary classification tasks. Some of the dictionary tools performed well overall, but results were invariably mixed. Lexicoder was attuned very effectively to negative sentiment, but not as well to positive sentiment. Notably, many of the video transcripts contained no words in the Lexicoder, HuLiu, and Vader dictionaries. In total, 33% of the sentences were unclassified by Hu-Liu, 29% by Lexicoder, and 19% by Vader. Among the dictionaries, Sentiwordnet classified the largest proportion of the sentences, but it was also the least accurate.9 As we discussed earlier, there is no pre-classified or human annotated Hansard on which to train supervised learners, and manually annotating Hansard would be time-consuming, costly, and potentially predetermine the results to the same extent as human curated dictionaries. Nonetheless, it might be possible to leverage models trained on large annotated corpora from other domains. Thus, we also tested a number of supervised learners trained";comparison with hand-coded date;External: Human Annotated Scores;No;;;;;;;;;;;;;;;;;;;;;;Yes;Yes;Yes;;lukas.birkenmaier@outlook.de
12.09.2022 15:17:36;Role-based Association of Verbs, Actions, and Sentiments with Entities in Political Discourse;"Fogel-Dror, Yair; Shenhav, Shaul R.; Sheafer, Tamir; Van Atteveldt, Wouter";Yes, Continue with Coding;;Newspaper;English;;sentiment oriented sentiment analysis;Supervised: Part of Speech Tagging;Lexicoder Sentiment Dictionary, role-based association method;;No;;Yes;"We initially reviewed the results on a timeline by aggregating the sentiment scores (per entity, per article) by summing them per day. While reviewing the entity-oriented sentiment timeline separately for Israel and the PA, observations with high levels of negative sentiment scores for both entities were correlated with the two main violent events in this period —“Operation Protective Edge” in the Gaza strip (July-August 2014), and the Temple Mount/Al-Aqsa Mosque clashes (October 2015). Please refer to Figure 1 (the graph for the 10-word proximity method is practically identical to the graph created by the sentence proximity method). These results might seem promising at first, as negative sentiment is expected to be attributed to rival entities in violent events. However, when placed side by side at the daily level (Figure 1), the sentiment–entity timeline of Israel and the PA looked fairly symmetrical. Assigning a negative sentiment to both entities may be a reasonable finding, as both participated in the violent events; yet, the level of symmetry still raises a concern. Is it really the case that the media is so balanced in terms of negative sentiment toward both actors, or is this a problem of discriminant validity (Adcock & Collier, 2001)? If the latter is true, this means that instead of measuring the specific sentiment that was supposed to be associated with each entity, we measured the volume of sentiment expressed in the text in general. If this is true, we actually measured the textual sentiment surrounding the entities but not the sentiment attributed to them.";face validation;Content Validation;Yes;to test our concern, we measured the correlation between sentiment scores that were assigned to the two entities in each sentiment category—positive and negative. The results pointed at a strong correlation for negative sentiment (Pearson’s r > .74) and moderate correlations for positive sentiment (r > .50). To further test whether the scores measured the number of sentiment expressions instead of the sentiment assigned to each entity, we measured the correlations between the volume of sentiment expressions (positive/negative) for each entity and the total sentiment score (positive/negative). The results here pointed to the same validity-related concerns, as we found moderate-to-strong correlations for negative sentiments and positive sentiments for both entities (r > .76 and r > .42, respectively). All correlations were significant (p < .05). This means that these proximity methods of association do not clearly differentiate among the general sentiments expressed in the text and the sentiments intended to be associated with each entity.;comparison with Model estimates with two unassociated sentiment scores;External: Scores from other CATM;Yes;Two trained human coders classified each sentiment verb expression as associated with Israel, the PA, neither, or both (Krippendorf’s α = .8). Differences between coders were discussed to form a gold standard that was used to train and test the algorithm. At this point, for every verb, the dataset contained the gold standard and the four features. We split the dataset into a training set (60%), to be used by the learning version, and a test set (40%), and created two versions of the RBA method: rule-based and learning. The rule-based version was easier to implement and did not need the addition of manually coded examples to train the algorithm. It had two rules: (1) If the verb is active, associate the sentiment measured in the verb with entities found in the subject, and otherwise with entities found in the predicate. (2) If the verb is a direct referencing verb, reverse the association. To improve the accuracy of the method by fine-tuning the weights of the features inductively and allowing for further expansion of the method, we created a version of the method that replaces the two a priori rules with a learning algorithm. To this end, we used a Python implementation of a support vector machine with a linear kernel (Pedregosa et al., 2011). The algorithm was trained on the training set (N = 615 verbs).;Human annotated test set;External: Human Annotated Scores;Yes;"As a remedy, we focused the manual validation on measuring negative sentiment only, following researchers who have found negativity to be clearer to measure (Haselmayer & Jenny, 2017; Soroka et al., 2015). We also instructed human coders to follow the perspective and assumptions of the lexicon. For example, as the sentiment lexicon did not assign different values to different expressions, we only measured the quantity of sentiments, and not their quality (e.g., murdering a person and injuring a person had similar negative weights). Two human coders were trained to code articles for sentiments expressed toward each entity—Israel and the PA on a three-level scale: 0 (no negative sentiment), 1 (low level of negative sentiment), and 2 (high level of negative sentiment—see the full coding instructions in Section C in the online Appendix). As we were interested in the bias toward one entity compared with that toward the other, we calculated the value of delta between the negative sentiments associated with Israel and the PA. The result was a five-point scale of negativity measure that spanned from −2 (PA was represented more negatively) to + 2 (Israel was represented more negatively). Intercoder reliability was no lower than Krippendorf’s α = .71.

Moving to the validation of the association method, this analysis provides further support for our main concerns regarding proximity-based approaches and supports our alternative method. First, we found support for our claim regarding discriminant validation, as 39% of all articles were scored with a non-zero negativity score, which means that sentiments associated with both entities were asymmetrical. This finding was even more convincing when articles containing no negative sentiment toward any entity were eliminated. From the remaining articles, 72% received a non-zero negativity score. Second, results at the week level show that while the two proximity methods of association (sentence and 10-word) were not significantly correlated with the manual analysis (in fact, the correlation was negative), the RBA method was positively and significantly correlated with it (Spearman’s ρ = .26, p < .05). Although modest (maybe because of the RBA’s focus on associations of verbs), this correlation supports the validity of the RBA method compared with the other two proximity methods of association considered. These findings show that associations formed by the RBA method are considerably more accurate at the expression level and more valid at the document level than those made by proximity methods.";Manual Validation to test for discriminant validity (using hand-coded data, but the goal here is to evaluate discriminant validity between their method and other methods);Content Validation;No;;;;;;;;;;No;;;;lukas.birkenmaier@outlook.de
12.09.2022 15:47:27;A General Model of Author “Style” with Application to the UK House of Commons, 1935–2018;"Huang, Leslie; Perry, Patrick O.; Spirling, Arthur";Yes, Continue with Coding;;Politics: Parliamentary Records;English;;distinctiveness in speech;Unsupervised: Statistical Model based on Word Frequencies;not named;;No;;Yes;One basic requirement is that our method ought to label “intrusive” texts, i.e. ones that were not clearly produced by the parliamentary data generating process, as distinctive. To put this to the test, we took the set of backbenchers from a modern session (2015–2016) added to them the State of the Union speeches given by U.S. President Barack Obama. We randomly sampled n “speeches” of m sentences each from the SotU speeches, where n is the mean speeches per MP and m is the mean speech length in sentences for the 2015 session. Aer inserting Obama in the corpus, we select the vocabulary using our standard cross-validation procedure. We used Obama because although his works are approximately contemporaneous with our data, his style is distinctive relative to our MPs: they come from an American rather than British political system, and they are long oratories consumed by the general public rather than speeches directed primarily at other politicians. With that in mind, our model should identify Obama as easily the most distinctive author. As we see from Figure 1, this is indeed the case: Obama’s ƒt point estimate is by far the largest in the data and appears at the far top right. Its confidence interval does not overlap with any other MP in the distribution (note that we do not include every MP in the graphic due to limited space, but we do include the full range in terms of distinctiveness estimates). Of course, such a test might be cherry-picking, and there is no obvious baseline for performance (other than identifying the intruder).7 So we now turn to a domain-specific assessment.;Intrusion of outlier speeches;Content Validation;Yes;"For more substantive performance evaluation, we look at the “most distinctive” and “least distinctive” backbench MPs for the parliamentary sessions on either side of Blair’s election landslide in 1997 (that is, 1995–1996 and 1998–1999). This has the advantage of being a period in which (a) control of the Commons switched (from Conservative to Labour), meaning we have variation in the party of the backbenchers, and (b) there are academic accounts which help ground our understanding of MPs at this time (Cowley 2002; Spirling and Quinn 2010; Kellermann 2012). In terms of measurement, we use a convergent validity approach: we compare our measure to another (computed independently) and show that they are related as expected.

To see how we proceed in practice, note that for each MP t , in each session, we have an estimate of their distinctiveness in log-odds terms: our ƒ, above. For current purposes, however, we focus on something related but more concrete and directly interpretable: the proportion of their speeches which are correctly predicted as being from them relative to all other MPs (proportion of speeches correctly predicted, or “PCP,” in the tables below). We use fivefold cross-validation to fit a model to texts from a given session, predict the speakers of held-out texts using this model, and calculate each speaker’s rate of correct predictions; we report each speaker’s mean PCP. To validate these estimates, we consider their extrema—their minimums and maximums. In the subsection tables below, we list the twenty names of the MPs who were most distinct and least distinct by this measure (subject to having made a minimum of twenty speeches). We do this for the two sessions in question: one in 1995–1996 and one in 1998–1999. We also list the number of mentions of each MP in the Times newspaper archives (via Gale Group Digital Archive) for the same period, specifically the “Politics and Parliament” subsection of the “News.”";criterion validity (even tough they claim it to be convergent validity, but the comparison is with MP mentions in the TIME newspaper and thus an external criterion);External: Criterion data / Predictive validation;Yes;More substantively, we note the presence of several Labour “rebels” among the most distinct. These include Tony Benn, Diane Abbott, John McDonnell, Roger Berry, and Tam Dalyell, all of whom consistently voted against the Labour government’s plan to reform the welfare state.8 Peter Temple-Morris was a party switcher, and “interesting” for that reason—he was elected as a Tory MP in 1997, but then crossed the floor to Labour the same year. The most interesting MPs here include Stuart Bell, who was the Church Estates Commissioner, meaning that he was one of the managers of the Church of England’s property. David Hinchliffe, chairman of the Select Committee on Health, was subsequently extremely critical of the Blair government’s proposed reforms to the National;face validity of most extreme MPs;Content Validation;Yes;"Finally, with respect to a broader understanding of validity, we ask what exactly we are capturing as “distinctiveness” in our measure? As regards our comments at the opening of Section 2, is it mere “phrasing” (different ways of saying the same thing) or “substance” (saying something different)? Put more directly with respect to the extant literature, does our model “improve” (in fit terms at least) over the original Mosteller and Wallace (1963) approach and capture something more than function word usage?

To assess this, we conducted a simple experiment. We ran a special case of the estimation using only the seventy function words (i.e. stop words) from the original Mosteller and Wallace (1963) study. Our contention is that if our model is simply capturing idiosyncratic stylistic differences (in the narrow sense meant in that earlier literature), the restricted version should perform approximately as well as the more general one that uses all words in the vocabulary. Studying Figure 2, we see this is clearly false: there, the bottom line with triangle points is the mean prediction rate (for each speaker, with fivefold cross-validation) from the stop word model. The top line is the mean prediction rate from our model, which has no restrictions on stop words (as in the rest of this paper). It performs about three to five times as well as the pure phrasing model, on average. This implies that there is certainly something more than expressive manner going on: we happily refer to that residual variation as “substance.” This does not mean, of course, that the Mosteller and Wallace (1963) approach vocabulary is “wrong” (it is just a special case of ours), but it does suggest our model is doing something statistically useful in terms of capturing practical variation between contemporaneous MPs. Why do we see this performance difference? From inspection, we note that the fit improvement comes mostly from the middle of the distribution (that is, both our approach and the more simple one perform similarly for the most and least distinctive MPs but not for the median and mean—at least for the sessions we looked at in detail). We suspect this is because while almost everyone will have non-zero use of all of the Mosteller and Wallace (1963) words, our richer vocabulary has much higher variance in use. At the top of the distribution—MPs who are distinctive whatever the vocabulary—this makes no difference. Conversely, at the bottom of the distribution—MPs who use neither vocabulary very much—this also makes no difference. But for MPs in the middle, our much larger vocabulary offers more opportunities to distinguish oneself (for a fixed amount of speaking), and, thus, our model does better for these people. Before moving to the results, we note that readers may be qualitatively interested in the underlying tokens that affect distinctiveness of individuals in the model: in Appendix C, we discuss how these might be obtained and examined.";Evaluation of Model Outputs with small subset of data (only stopwords) to test model features;Content Validation;No;;;;;;;;;;Yes;No;No;;lukas.birkenmaier@outlook.de
13.09.2022 12:01:23;Strategy Framing in News Coverage and Electoral Success: An Analysis of Topic Model Networks Approach;"Walter, Dror; Ophir, Yotam";Yes, Continue with Coding;;Newspaper;English;;Strategy Framing;Unsupervised: Topic Modeling;Analysis of Topic Model Network (ANTMN);;Yes;"Strategy frames focus on candidates’ standing in polls (using horse race metaphors), their motivations, performance, character, and style. Issue frames focus on candidates’ record and opinions on policies. Media emphasis of strategy at the expense of issue frames (Aalberg et al., 2012; Patterson, 1994) was found to yield cynicism toward the political process (Cappella & Jamieson, 1997) and the media (Hopmann et al., 2015) and to decrease political knowledge (Cappella & Jamieson, 1997).";Yes;"The strategy and issue estimation per document were further validated using manual content analysis of 150 articles by two independent trained coders (not the authors; Krippendorff’s α = .85). As coders used the common five-level strategy issue scale (Aalberg et al., 2012) and our estimation was a continues 0–100 scale (the percentage of the language that related to strategyoriented topics), we used Pearson’s correlation to examine the similarity between the two scales at the article level. Results showed high agreement between the manual coders and the auto mated estimations (r = 0.81, p < .001; see Appendix 2 for detail).";Human annotation;External: Human Annotated Scores;No;;;;;;;;;;;;;;;;;;;;;;Yes;No;No;;lukas.birkenmaier@outlook.de
14.09.2022 15:25:26;Reading Between the Lines: Prediction of Political Violence Using Newspaper Text;"Mueller, Hannes; Rauh, Christopher";Yes, Continue with Coding;;Newspaper;English;;Topics;Unsupervised: Topic Modeling;LDA;;Yes;identifies topics and uses these in later analysis;No;;;;;;;;;;;;;;;;;;;;;;;;;;Yes;Yes;No;;lukas.birkenmaier@outlook.de
15.09.2022 10:03:50;Clickbait for climate change: comparing emotions in headlines and full-texts and their engagement;"Xu, Zhan; Laffidy, Mary; Ellis, Lauren";Yes, Continue with Coding;;Newspaper;English;;Emotions;Rule-based: Off-the-shelf dictionary;NRC Emotion Lexicon;;No;;Yes;Dictionary has demonstrated high validity when examining short texts such as tweets and SMS (Kiritchenko et al., 2014) as well as long texts such as novels (Mohammad, 2012). NRC NRC Emotion Lexicon identifies the intensity level that each word and phrase is associated with positively- or negatively- valenced emotions and eight discrete emotions including anger, anticipation, disgust, fear, joy, sadness, surprise, and trust (S. M. Mohammad & Turney, 2010);reference to previous validation evidence;Content Validation;No;;;;;;;;;;;;;;;;;;;;;;No;;;;lukas.birkenmaier@outlook.de
15.09.2022 10:33:55;Ideological Scaling of Social Media Users: A Dynamic Lexicon Approach;"Temporão, Mickael; Vande Kerckhove, Corentin; van der Linden, Clifton; Dufresne, Yannick; Hendrickx, Julien M.";Yes, Continue with Coding;;Social Media: Twitter;Others;English, French;Ideological Position;Unsupervised: Text Scaling;dynamic lexicon approach;;No;;Yes;"We begin with a focus on the face validity (Dinas and Gemenis 2010) of the estimates produced by the dynamic lexicon approach, drawing on an overview of the ideological landscapes in each of our three case studies. Our objective is to develop an intuitive rendering of the ideological positioning of the major parties in each case, so that we have a frame of reference by which to compare the estimates generated by the dynamic lexicon approach
";face validity;Content Validation;Yes;"We then test the convergent validity of the calibration process for the dynamic lexicon of political terms. Here we focus on the subset of political candidates, whose user-generated content defines the dynamic lexicon, by comparing the positions derived from this approach to an ideological scaling approach based on network information (Barberá 2015; Bond and Messing 2015)";comparing it with network analysis (so other method, but not CATM);Unsure;Yes;"Finally, we extend this approach to individual citizens in our sample and compare their estimated positions to a reference position derived from survey data collected from Vote Compass

""Having demonstrated that the dynamic lexicon approach can derive valid ideological dimensions from the content that political elites generate on social media, and that it can position those political elites in ideological space in ways that correlated highly with more established methods, we now examine its ability to classify the individual ideologies of the broader population of social media users. To do so, we compare the dynamic lexicon approach to a basic Wordfish approach, in which we ignore the precomputed values and instead simply apply the algorithm. Two different baseline strategies are proposed, depending on whether or not we consider the use of a lexicon of political terms to create the TDM for citizens. In the first strategy (Wordfish-all), users’ TDM is generated keeping all the N -grams (i.e., following the same building process as for elites’ TDM Z elit). The second strategy (Wordfish-political) considers the adapted TDM Z cit built from the dynamic lexicon of political terms. The reference ideological position for each users is derived from the survey data collected by Vote Compass.6 We use an expectation–maximization algorithm to solve the maximum likelihood problem that generates these estimates (Barber 2012). We consider a social media extraction to be effective if two social media users with similar ideologies according to the Vote Compass survey data are also situated in close ideological proximity to one another using the dynamic lexicon approach. The quality of estimates is then defined as the Pearson correlation coefficient (ρ) between the vector of textual estimates and the vector of reference ideologies.""";Criterion validity with survey scores;External: Criterion data / Predictive validation;No;;;;;;;;;;;;;;Yes;Yes;Yes;;lukas.birkenmaier@outlook.de
15.09.2022 10:39:51;Facebook affordances and citizen engagement during elections: European political parties and their benefit from online strategies?;"Koc-Michalska, Karolina; Lilleker, Darren G.; Michalski, Tomasz; Gibson, Rachel; Zajac, Jan M.";No;"only ""descriptive"" analysis of texts (word lenght, timing of posting etc.)";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;lukas.birkenmaier@outlook.de
20.09.2022 21:56:54;Policy Diffusion: The Issue-Definition Stage;"Gilardi, Fabrizio; Shipan Charles R.; Wüest, Bruno";Yes, Continue with Coding;;Newspaper;English;;policy diffusion, policy frames;Unsupervised: Topic Modeling;Structural topic model (STM);;No;;Yes;"We validate the output of the model by considering some correlations that help us to assess the plausibility of our results. First, we consider how topics correlate with the timing of smoking ban adoptions at the state level. Figure C5 below shows that the topic State legislation is much more prevalent during months in which state legislation was adopted than in other months, which, of course, is exactly what one would expect. Second, Figure C6 looks at the prevalence of topics before and after adoption. This figure shows peaks for several of our topics at the moments one would expect them to be most prominent: State legislation and Electoral politics during the month of adoption, and Enforcement in the first couple of years following policy adoption. 7 Third, we find greater attention to the electoral implication of adoptions in states where restrictions on smoking are more likely to be politically controversial. In particular, we would expect to find the Electoral politics topic to be more common in states where more people smoke, in more politically conservative states, and in states that are under Republican control. Figure C7 shows support for these expectations.
Finally, as we mention in the text, we also coded the sentiment of each topic—that is, whether the newspaper paragraph exhibited a “pro” smoking bans approach (i.e., a positive sentiment toward such bans and restrictions) or an “anti” smoking bans approach (i.e., a negative sentiment). Examining the sentiment for each topic allows us to further validate our measure. In particular, we would expect the Health topic to have the most positive sentiments, indicating that when this topic is discussed it is discussed in terms supportive of smoking restrictions. We also would expect that the more controversial topics such as Bars and restaurants and Casinos, which opponents of smoking bans have argued will be hurt by such bans (Warner, 2000), as well as Enforcement, to exhibit more negative sentiments, indicating that these are the most commonly raised arguments against smoking restrictions. And that is indeed what we find, with Figure C4 showing the Health topic exhibiting the most positive sentiments and Bars and restaurants, Casinos, and Enforcement exhibiting the most negative sentiments.";First, we consider how topics correlate with the timing of smoking ban adoptions at the state level. Figure C5 below shows that the topic State legislation is much more prevalent during months in which state legislation was adopted than in other months, which, of course, is exactly what one would expect.;External: Criterion data / Predictive validation;Yes;"We validate the output of the model by considering some correlations that help us to assess the plausibility of our results. First, we consider how topics correlate with the timing of smoking ban adoptions at the state level. Figure C5 below shows that the topic State legislation is much more prevalent during months in which state legislation was adopted than in other months, which, of course, is exactly what one would expect. Second, Figure C6 looks at the prevalence of topics before and after adoption. This figure shows peaks for several of our topics at the moments one would expect them to be most prominent: State legislation and Electoral politics during the month of adoption, and Enforcement in the first couple of years following policy adoption. Third, we find greater attention to the electoral implication of adoptions in states where restrictions on smoking are more likely to be politically controversial. In particular, we would expect to find the Electoral politics topic to be more common in states where more people smoke, in more politically conservative states, and in states that are under Republican control. Figure C7 shows support for these expectations.
Finally, as we mention in the text, we also coded the sentiment of each topic—that is, whether the newspaper paragraph exhibited a “pro” smoking bans approach (i.e., a positive sentiment toward such bans and restrictions) or an “anti” smoking bans approach (i.e., a negative sentiment). Examining the sentiment for each topic allows us to further validate our measure. In particular, we would expect the Health topic to have the most positive sentiments, indicating that when this topic is discussed it is discussed in terms supportive of smoking restrictions. We also would expect that the more controversial topics such as Bars and restaurants and Casinos, which opponents of smoking bans have argued will be hurt by such bans (Warner, 2000), as well as Enforcement, to exhibit more negative sentiments, indicating that these are the most commonly raised arguments against smoking restrictions. And that is indeed what we find, with Figure C4 showing the Health topic exhibiting the most positive sentiments and Bars and restaurants, Casinos, and Enforcement exhibiting the most negative sentiments.";Second, Figure C6 looks at the prevalence of topics before and after adoption. This figure shows peaks for several of our topics at the moments one would expect them to be most prominent: State legislation and Electoral politics during the month of adoption, and Enforcement in the first couple of years following policy adoption.;External: Criterion data / Predictive validation;Yes;"We validate the output of the model by considering some correlations that help us to assess the plausibility of our results. First, we consider how topics correlate with the timing of smoking ban adoptions at the state level. Figure C5 below shows that the topic State legislation is much more prevalent during months in which state legislation was adopted than in other months, which, of course, is exactly what one would expect. Second, Figure C6 looks at the prevalence of topics before and after adoption. This figure shows peaks for several of our topics at the moments one would expect them to be most prominent: State legislation and Electoral politics during the month of adoption, and Enforcement in the first couple of years following policy adoption. Third, we find greater attention to the electoral implication of adoptions in states where restrictions on smoking are more likely to be politically controversial. In particular, we would expect to find the Electoral politics topic to be more common in states where more people smoke, in more politically conservative states, and in states that are under Republican control. Figure C7 shows support for these expectations.
Finally, as we mention in the text, we also coded the sentiment of each topic—that is, whether the newspaper paragraph exhibited a “pro” smoking bans approach (i.e., a positive sentiment toward such bans and restrictions) or an “anti” smoking bans approach (i.e., a negative sentiment). Examining the sentiment for each topic allows us to further validate our measure. In particular, we would expect the Health topic to have the most positive sentiments, indicating that when this topic is discussed it is discussed in terms supportive of smoking restrictions. We also would expect that the more controversial topics such as Bars and restaurants and Casinos, which opponents of smoking bans have argued will be hurt by such bans (Warner, 2000), as well as Enforcement, to exhibit more negative sentiments, indicating that these are the most commonly raised arguments against smoking restrictions. And that is indeed what we find, with Figure C4 showing the Health topic exhibiting the most positive sentiments and Bars and restaurants, Casinos, and Enforcement exhibiting the most negative sentiments.";Third, we find greater attention to the electoral implication of adoptions in states where restrictions on smoking are more likely to be politically controversial. In particular, we would expect to find the Electoral politics topic to be more common in states where more people smoke, in more politically conservative states, and in states that are under Republican control. Figure C7 shows support for these expectations.;External: Criterion data / Predictive validation;Yes;"We validate the output of the model by considering some correlations that help us to assess the plausibility of our results. First, we consider how topics correlate with the timing of smoking ban adoptions at the state level. Figure C5 below shows that the topic State legislation is much more prevalent during months in which state legislation was adopted than in other months, which, of course, is exactly what one would expect. Second, Figure C6 looks at the prevalence of topics before and after adoption. This figure shows peaks for several of our topics at the moments one would expect them to be most prominent: State legislation and Electoral politics during the month of adoption, and Enforcement in the first couple of years following policy adoption. Third, we find greater attention to the electoral implication of adoptions in states where restrictions on smoking are more likely to be politically controversial. In particular, we would expect to find the Electoral politics topic to be more common in states where more people smoke, in more politically conservative states, and in states that are under Republican control. Figure C7 shows support for these expectations.
Finally, as we mention in the text, we also coded the sentiment of each topic—that is, whether the newspaper paragraph exhibited a “pro” smoking bans approach (i.e., a positive sentiment toward such bans and restrictions) or an “anti” smoking bans approach (i.e., a negative sentiment). Examining the sentiment for each topic allows us to further validate our measure. In particular, we would expect the Health topic to have the most positive sentiments, indicating that when this topic is discussed it is discussed in terms supportive of smoking restrictions. We also would expect that the more controversial topics such as Bars and restaurants and Casinos, which opponents of smoking bans have argued will be hurt by such bans (Warner, 2000), as well as Enforcement, to exhibit more negative sentiments, indicating that these are the most commonly raised arguments against smoking restrictions. And that is indeed what we find, with Figure C4 showing the Health topic exhibiting the most positive sentiments and Bars and restaurants, Casinos, and Enforcement exhibiting the most negative sentiments.";Finally, as we mention in the text, we also coded the sentiment of each topic—that is, whether the newspaper paragraph exhibited a “pro” smoking bans approach (i.e., a positive sentiment toward such bans and restrictions) or an “anti” smoking bans approach (i.e., a negative sentiment). Examining the sentiment for each topic allows us to further validate our measure. In particular, we would expect the Health topic to have the most positive sentiments, indicating that when this topic is discussed it is discussed in terms supportive of smoking restrictions. We also would expect that the more controversial topics such as Bars and restaurants and Casinos, which opponents of smoking bans have argued will be hurt by such bans (Warner, 2000), as well as Enforcement, to exhibit more negative sentiments, indicating that these are the most commonly raised arguments against smoking restrictions.;External: Human Annotated Scores;No;;;;;;;;;;Yes;Yes;Yes;;david.gruening@gesis.org
20.09.2022 22:15:02;Computational Identification of Media Frames: Strengths, Weaknesses, and Opportunities;"Nicholls, Tom; Culpepper, Pepper D.";Yes, Continue with Coding;;Newspaper;English;;Media frames;Supervised: Machine Learning, Unsupervised: Topic Modeling, (Evolutionary) Factor analysis;k-means clustering/NLP, evolutionary factor analysis (EFA), structural topic model (STM);;No;;Yes;"
We also carried out a manual analysis on the Australian dataset, for comparison with the computational methods. The basic structure of the analysis was based upon the procedure recommended by Chong and Druckman (2007, pp. 106–108). In the first stage, the authors specified the event for which frames would be sought; in this case, the Australian banking Royal Commission. In the second stage, the frame-defining attitudes were chosen to be attitudes to the banking scandals. For the third stage, two researchers (the first author and a research assistant well-versed in the Australian banking scandals) worked separately after being briefed on the event and attitudes, and also the Chong & Druckman definition of emphasis frames. Each was issued an identical 300-article ran- domly ordered random sample of the Australian data, and instructed to inductively identify a framing schema.
We do not claim that the results of this manual analysis are “the” gold standard set of frames against which to measure the computational approaches. Given the challenges of operationalizing framing and the low reliability of human coding as a result of researcher effects (Matthes & Kohring, 2008), we offer them as one plausible set of frames, not as the only one. However, they do stand up as reasonably distinct statements of problem definition and causal interpretation of Australian banking scandal of 2018.";We also carried out a manual analysis on the Australian dataset, for comparison with the computational methods.;External: Human Annotated Scores;Yes;"We therefore evaluate the different frame identification methods subjectively, according to the quality of the frames they have selected. There are no quantitative measures of each of these criteria; they are abstract theoretical qualities about which we make judgments. We use three criteria for quality, with the latter two drawing on Gerring (2001) and Roberts et al. (2014). First, the extent to which the frames identified by the methods fit with our understanding of what a media frame is – an interpretive lens that includes the definition of a problem and a diagnosis of its causes (Entman, 1993, p. 52). Second, the extent to which each identified frame is internally coherent. Third, the extent to which each identified frame is externally differentiable from other potential frames.";First, the extent to which the frames identified by the methods fit with our understanding of what a media frame is – an interpretive lens that includes the definition of a problem and a diagnosis of its causes (Entman, 1993, p. 52).;Content Validation;Yes;"We therefore evaluate the different frame identification methods subjectively, according to the quality of the frames they have selected. There are no quantitative measures of each of these criteria; they are abstract theoretical qualities about which we make judgments. We use three criteria for quality, with the latter two drawing on Gerring (2001) and Roberts et al. (2014). First, the extent to which the frames identified by the methods fit with our understanding of what a media frame is – an interpretive lens that includes the definition of a problem and a diagnosis of its causes (Entman, 1993, p. 52). Second, the extent to which each identified frame is internally coherent. Third, the extent to which each identified frame is externally differentiable from other potential frames.";Second, the extent to which each identified frame is internally coherent.;Content Validation;Yes;"We therefore evaluate the different frame identification methods subjectively, according to the quality of the frames they have selected. There are no quantitative measures of each of these criteria; they are abstract theoretical qualities about which we make judgments. We use three criteria for quality, with the latter two drawing on Gerring (2001) and Roberts et al. (2014). First, the extent to which the frames identified by the methods fit with our understanding of what a media frame is – an interpretive lens that includes the definition of a problem and a diagnosis of its causes (Entman, 1993, p. 52). Second, the extent to which each identified frame is internally coherent. Third, the extent to which each identified frame is externally differentiable from other potential frames.";Third, the extent to which each identified frame is externally differentiable from other potential frames.;Content Validation;No;;;;;;;;;;Yes;No;No;;david.gruening@gesis.org
20.09.2022 22:28:15;Connecting the (Far-)Right Dots: A Topic Modeling and Hyperlink Analysis of (Far-)Right Media Coverage during the US Elections 2016;"Kaiser, Jonas; Rauchfleisch, Adrian; Bourassa, Nikki";Yes, Continue with Coding;;Newspaper;English;;topic convergence, media topics, linking patterns;Unsupervised: Topic Modeling;Structural topic model (STM);;No;;No;;;;;;;;;;;;;;;;;;;;;;;;;;No;;;;david.gruening@gesis.org
04.10.2022 06:53:32;Campaigning in the fourth age of political communication. A multi-method study on the use of Facebook by German and Austrian parties in the 2013 national election campaigns;"Magin, Melanie; Podschuweit, Nicole; Haßler, Jörg; Russmann, Uta";No;There is no real computer-assisted text method applied. The authors' content analysis is mere counting of specific features of different Facebook-accounts.;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;david.gruening@gesis.org
04.10.2022 06:56:16;Testing the Validity of Automatic Speech Recognition for Political Text Analysis;"Proksch, Sven-Oliver; Wratil, Christopher; Wäckerle, Jens";No;"The paper only discusses/shows speech recognition methods for transcription; no text analysis beyond that is deployed.";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;david.gruening@gesis.org
04.10.2022 07:02:25;A Pairwise Comparison Framework for Fast, Flexible, and Reliable Human Coding of Political Texts;"Carlson, David; Montgomery, Jacob M.";No;Only development of a method paper;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;david.gruening@gesis.org
04.10.2022 07:06:30;Twitter and News Gatekeeping: Interactivity, reciprocity, and promotion in news organizations’ tweets;Russell, Frank Michael;No;Human coding as the core method of this paper;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;david.gruening@gesis.org
04.10.2022 07:30:11;Information Credibility under Authoritarian Rule: Evidence from China;Chang, Charles;No;The paper's text analysis is human coding;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;david.gruening@gesis.org
04.10.2022 07:57:22;Estimating Spatial Preferences from Votes and Text;"Kim, In Song; Londregan, John; Ratkovic, Marc";Yes, Continue with Coding;;Politics: Parliamentary Records;English;;Spatial preferences;Unsupervised: Text Scaling, Probabilistic principal component analysis;Sparse factor analysis (SFA);;No;;Yes;Before applying the method, let us consider the applicability of SFA in this case. First, voting is not always sincere in the US Senate, as there are always motions to recommit, etc. We note, though, that ideal point estimates from the US Congress have been used extensively in other studies and possess high face validity. To be particularly careful, if a bill is voted on several times due to different motions, we only include the final vote in our analysis.;We note, though, that ideal point estimates from the US Congress have been used extensively in other studies and possess high face validity.;Content Validation;Yes;In Section 3, we apply our estimator to Congress, and we implement several methods for assessing internal and external validity, methods that analysts might want to use as checks when using SFA.;In this section, we apply SFA to recent US Senate data. The analysis proceeds in three steps. First, we describe the data and discuss the viability of SFA in this context. Second, we apply the method to the contemporary US Senate. Third, we simulate a “strong-party” system where we use SFA to use text data to gain leverage on rank-and-file ideal points even in the presence of party line voting and heavy missingness in the vote data.;Content Validation;No;;;;;;;;;;;;;;;;;;Yes;Yes;No;;david.gruening@gesis.org
04.10.2022 08:04:48;Right-Wing, Populist, Controlled by Foreign Powers? Topic Diversification and Partisanship in the Content Structures of German-Language Alternative Media;"Müller, Philipp; Freudenthaler, Rainer";Yes, Continue with Coding;;Newspaper;English;;Topic diversification;Unsupervised: Topic Modeling;LDA topic modeling;;No;;Yes;"In the last step, two researchers independently validated and labeled the topics of the 70-topic solution based on an in-depth reading of the documents, using both lists of the most common words in each topic (at k 1⁄4 .6, see Sievert and Shirley 2014, 67) and full articles with the highest k under each topic and coherence indicator to guide their decisions; they discussed their independent assessments to arrive at the final set of topics. During this step, boilerplate topics (which occur in many articles and are not internally coherent, see DiMaggio, Nag, and Blei 2013, 568; Maier et al. 2018, 108) were removed and similar topics were grouped together, resulting in 14 groups of topics encompassing the 66 remaining topics, which are summarized in Table 1.";two researchers independently validated and labeled the topics of the 70-topic solution based on an in-depth reading of the documents;Content Validation;No;;;;;;;;;;;;;;;;;;;;;;Yes;No;No;;david.gruening@gesis.org
04.10.2022 08:17:58;Connecting the (Far-)Right Dots: A Topic Modeling and Hyperlink Analysis of (Far-)Right Media Coverage during the US Elections 2016;"Kaiser, Jonas; Rauchfleisch, Adrian; Bourassa, Nikki";Yes, Continue with Coding;;Newspaper;English;;(Far-)right media coverage of topics;Unsupervised: Topic Modeling;Structural topic model analysis (STM);;No;;Yes;We randomly selected 100 articles and automatically assigned the topic with a probability above 0.49 to the article. Two coders without specific training coded the topics. The intercoder-reliability score between the two coders and the topic model was with a Krippendorff’s alpha of 0.67 acceptable for our case. When the more generic Republican topics are merged the alpha is even over 0.7. This is surprisingly high as most documents have a mix of topics (e.g., economy and republican primaries).;The intercoder-reliability score between the two coders and the topic model was with a Krippendorff’s alpha of 0.67 acceptable for our case.;External: Human Annotated Scores;No;;;;;;;;;;;;;;;;;;;;;;No;;;;david.gruening@gesis.org
04.10.2022 10:03:25;Measuring Agenda Setting in Interactive Political Communication;Rossiter, Erin L.;Yes, Continue with Coding;;Politics: Parliamentary Records, in-person and online discussions;English;;Relative agenda-setting power of actors;Unsupervised: Topic Modeling;Parametric Speaker Identity for Topic Segmen- tation (SITS);;No;;Yes;I next explore agenda setting within three different inter- active political communication. First, with U.S. presiden- tial debates, I show that SITS can be used to test theories about the topics actors should favor in electoral debates. Second, using in-person deliberations, I shift emphasis from the topical agenda to who can set the agenda. I val- idate that participants who SITS identifies as setting the agenda indeed shift attention to their ideas, and do not do so by simply interrupting or out-talking others. And third, using a novel online discussion study, I show that agenda setting can shape the outcomes of political inter- actions.;Three empirical applications also validate the agenda-setting measure within different political settings: U.S. presidential debates, in-person deliberations, and online discussions.;Content Validation;Yes;To assess if SITS can accurately identify where latent shifts in topic occur, I classify a speaking turn as shift- ing topic if the posterior probability of a shift is greater than or equal to 0.50. I then compare where shifts were inferred by SITS to where topic changes were prompted by the researchers. SITS identifies 81.40% of the loca- tions that begin a new prompted topic segment by the re- searcher.7 I check for an SITS-inferred topic shift within two speaking turns after a researcher-prompted shift be- cause often after reading the prompt, the first few speak- ing turns would simply answer the prompt’s question by saying “yes,” “no,” or “do you want to go first?” Instead of classifying this as a topic shift, SITS would classify a subsequent speaking turn that actually began discussing the topic at hand as a shift.;The first exercise provides evi- dence that SITS can accurately identify where latent shifts in the agenda occur.;External: Human Annotated Scores;Yes;The topic intrusion task presents the human judge with a document, in this case a segment inferred by SITS. The judge is also presented with four word sets. Three of these word sets represent the three highest prob- ability topics for the segment. The fourth word set is the intruder, drawn randomly from the segment’s low probability topics. Each word set contains the top eight frequent and exclusive (FREX) topwords for the topic (Roberts et al. 2014). I set up the topic intrusion task for 200 randomly drawn segments from the debates. Then, Amazon Mechanical Turk (MTurk) workers were asked to choose which word set was most unrelated to the passage. In line with recent work on validation proce- dures for topic models by Ying, Montgomery, and Stew- art (2019), I ran two trials of the same 200 tasks. Fig- ure 2 plots the results for each trial separately as well as the pooled result.;Second, using crowdsourced hu- man judgments, I validate that SITS can infer semanti- cally meaningful latent topics.;External: Human Annotated Scores;Yes;Next, I validate that SITS can identify coherent segments of an interaction. To do so, I follow a procedure pro- posed by Grimmer and King (2011) to evaluate “clus- ter quality,” which is the similarity of the documents (here, segments of the debates) estimated to belong to the same cluster (here, having similar topic distributions). Importantly, I evaluate SITS segments against segments derived from a hand-coding approach. Hand-coded data are from Boydstun, Glazier, and Pietryka (2013) for the 1992, 2004, and 2008 U.S. general election presidential debates. Boydstun, Glazier, and Pietryka hand code sev- eral variables from the debate transcripts, including the topic of each question posed to the candidates and the topic of each phrase in the candidates’ responses. Then, they deem a candidate as going “off-topic” and thus, en- gaging in agenda-setting behavior, if the phrase’s topic does not correspond to the question’s topic.;Finally, I assess the interre- lated nature of where shifts in topic occur and the topics themselves by examining the resulting segments of an in- teraction.;External: Human Annotated Scores;No;;;;;;;;;;Yes;Yes;No;;david.gruening@gesis.org
04.10.2022 10:22:56;Machine Translation Vs. Multilingual Dictionaries Assessing Two Strategies for the Topic Modeling of Multilingual Text Collections;"Maier, Daniel; Baden, Christian; Stoltenberg, Daniela; De Vries-Kedem, Maya; Waldherr, Annie";Yes, Continue with Coding;;Newspaper, Social Media: Twitter;Others;English, Hebrew, Arabic;Conflict-related topics;Unsupervised: Topic Modeling, Rule-based: Off-the-shelf dictionary, Machine translation;Structural Topic Models (STM), INFOCORE, Google Translate API;;No;;Yes;"For the quantitative comparison, we calculated the extent to which different topic models obtained from the respective methodological approaches break down the variation included in each corpus in similar ways. To do so, we use the Correlation Matrix Distance (CMD), a distance measure designed to determine the (dis)similarity between two quadratic correlation matrices (Herdin et al., 2005; Motta & Baden, 2013). Theoretically, the CMD ranges from one (no association between both matrices) to zero (both correlation matrices are identical, up to a scale factor).
For this estimation, we depart from the compared topic models’ n x k-sized document-topic (θÞ matrices, wherein each of the n 1⁄4 1; . . . ; N rows correspond to a document in the respective corpus, and each of the k 1⁄4 1; . . . ; K columns to a topic in the respective topic model. In these matrices, the cell entries θnk represent the probability of document n to contain a given topic k – usually interpreted as the topic proportion that the respective document contains. Since the identity of the compared topics is uninformative for the comparison (i.e., one can permutate the order of topics (columns) in each matrix without changing the structural similarity), we transform the raw θ matrices into quadratic correlation matrices by calculating the dot product of θ with its transpose θT. As a result, we obtain, for each topic model, an n x n matrix C that reflects the extent to which different documents are composed of the same topics. The CMD then computes the distance between each pair of matrices C, which provides a direct measure of the extent to which the topics obtained by two compared topic models are distributed in (dis)similar ways over the same set of documents. Since the dimensionality of C depends solely on the number of the n documents, this transformation allows us to compare the θ’s of different topic models independently of their number of topics K. Formula 1 defines the CMD for the two correlation matrices CMTand CMD that result from the two methods under investigation, as denoted by their subscripts.";For the quantitative comparison, we calculated the extent to which different topic models obtained from the respective methodological approaches break down the variation included in each corpus in similar ways.;External: Scores from other CATM;Yes;"For the qualitative comparison, we first selected one best-fitting model per corpus and approach. Among all estimated topic models, we focused on those five models that offered the best fit based on the evaluation metrics offered by the STM package (K 1⁄4 f10; 15; 20; 25; 30g for both corpora and both methods). Two of the authors jointly judged these models’ interpretability, inspecting their top associated words (MT approach) and concepts (MD approach). Based on this information, we selected for the News corpus those topic models obtained for K = 25 (independently for both MT and MD), and for the Twitter corpus those models obtained for K = 30, for further validation. For these selected models, all topics were then carefully labeled and validated by the same two researchers. To ascertain the validity of topics and their labels, we selected for every topic a random sample of ten documents with a topic probability above a threshold of t 1⁄4 0:3. Given that only few topics are prevalent in a single document, the threshold can be considered sufficiently high for a topic to be the most prevalent in the respective documents (see also Maier et al., 2018). Topics were confirmed as interpretable and labeled if, through a discursive process, both judges agreed on the same interpretation, which had to be supported both by the identified top words/concepts and by the inspected documents, read against our familiarity with the conflict and the referenced events.";Topics were confirmed as interpretable and labeled if, through a discursive process, both judges agreed on the same interpretation, which had to be supported both by the identified top words/concepts and by the inspected documents, read against our familiarity with the conflict and the referenced events.;Content Validation;No;;;;;;;;;;;;;;;;;;Yes;No;No;;david.gruening@gesis.org
04.10.2022 11:00:23;Latent Semantic Scaling: A Semisupervised Text Analysis Technique for New Domains and Languages;Watanabe, Kohei;Yes, Continue with Coding;;Newspaper;Others;English, Japanese;Polarity score by semantic proximity between words;Supervised: Machine Learning;Latent Semantic Scaling (LSS);;No;;No;;;;;;;;;;;;;;;;;;;;;;;;;;No;;;;david.gruening@gesis.org
04.10.2022 11:56:02;A General Model of Author “Style” with Application to the UK House of Commons, 1935–2018;"Huang, Leslie; Perry, Patrick O.; Spirling, Arthur";Yes, Continue with Coding;;Politics: Parliamentary Records;English;;Distinctiveness;Multinomial naive Bayes approach;Multinomial naive Bayes approach;;No;;Yes;"One basic requirement is that our method ought to label “intrusive” texts, i.e. ones that were not clearly produced by the parliamentary data generating process, as distinctive. To put this to the test, we took the set of backbenchers from a modern session (2015–2016) added to them the State of the Union speeches given by U.S. President Barack Obama. We randomly sampled n “speeches” of m sentences each from the SotU speeches, where n is the mean speeches per MP and m is the mean speech length in sentences for the 2015 session. After inserting Obama in the corpus, we select the vocabulary using our standard cross-validation procedure. We used Obama because although his works are approximately contemporaneous with our data, his style is distinctive relative to our MPs: they come from an American rather than British political system, and they are long oratories consumed by the general public rather than speeches directed primarily at other politicians. With that in mind, our model should identify Obama as easily the most distinctive author. As we see from Figure 1, this is indeed the case: Obama’s 􏱉t point estimate is by far the largest in the data and appears at the far top right. Its confidence interval does not overlap with any other MP in the distribution (note that we do not include every MP in the graphic due to limited space, but we do include the full range in terms of distinctiveness estimates).
7
Of course, such a test might be cherry-picking, and there is no obvious baseline for performance (other than identifying the intruder).7 So we now turn to a domain-specific assessment.";One basic requirement is that our method ought to label “intrusive” texts, i.e. ones that were not clearly produced by the parliamentary data generating process, as distinctive.;Content Validation;Yes;"For more substantive performance evaluation, we look at the “most distinctive” and “least distinctive” backbench MPs for the parliamentary sessions on either side of Blair’s election landslide in 1997 (that is, 1995–1996 and 1998–1999). This has the advantage of being a period in which (a) control of the Commons switched (from Conservative to Labour), meaning we have variation in the party of the backbenchers, and (b) there are academic accounts which help ground our understanding of MPs at this time (Cowley 2002; Spirling and Quinn 2010; Kellermann 2012). In terms of measurement, we use a convergent validity approach: we compare our measure to another (computed independently) and show that they are related as expected.";In terms of measurement, we use a convergent validity approach: we compare our measure to another (computed independently) and show that they are related as expected.;External: Scores from other CATM;Yes;"Finally, with respect to a broader understanding of validity, we ask what exactly we are capturing as “distinctiveness” in our measure? As regards our comments at the opening of Section 2, is it mere “phrasing” (different ways of saying the same thing) or “substance” (saying something different)? Put more directly with respect to the extant literature, does our model “improve” (in fit terms at least) over the original Mosteller and Wallace (1963) approach and capture something more than function word usage?
To assess this, we conducted a simple experiment. We ran a special case of the estimation using only the seventy function words (i.e. stop words) from the original Mosteller and Wallace (1963) study. Our contention is that if our model is simply capturing idiosyncratic stylistic differences (in the narrow sense meant in that earlier literature), the restricted version should perform approximately as well as the more general one that uses all words in the vocabulary. Studying Figure 2, we see this is clearly false: there, the bottom line with triangle points is the mean prediction rate (for each speaker, with fivefold cross-validation) from the stop word model. The top line is the mean prediction rate from our model, which has no restrictions on stop words (as in the rest of this paper). It performs about three to five times as well as the pure phrasing model, on average. This implies that there is certainly something more than expressive manner going on: we happily refer to that residual variation as “substance.” This does not mean, of course, that the Mosteller and Wallace (1963) approach vocabulary is “wrong” (it is just a special case of ours), but it does suggest our model is doing something statistically useful in terms of capturing practical variation between contemporaneous MPs.";We ran a special case of the estimation using only the seventy function words (i.e. stop words) from the original Mosteller and Wallace (1963) https://doi.org/10.1017/pan.2019.49 Published online by Cambridge University Press study.;External: Scores from other CATM;No;;;;;;;;;;;;;;Yes;No;No;;david.gruening@gesis.org